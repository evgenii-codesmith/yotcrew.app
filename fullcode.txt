app/scrapers/__init__.py:
<code>
"""Pluggable scrapers package for yacht job sources"""
from .base import BaseScraper, ScrapingResult, UniversalJob
from .registry import ScraperRegistry, register_scraper

# Import all scrapers - they auto-register via decorators
from .yotspot import YotspotScraper
from .daywork123 import Daywork123Scraper

__all__ = [
    'BaseScraper',
    'ScrapingResult', 
    'UniversalJob',
    'ScraperRegistry',
    'register_scraper'
]
</code>

app/scrapers/registry.py:
<code>
"""Registry for managing pluggable scrapers"""
from typing import Dict, Type, List
from .base import BaseScraper

class ScraperRegistry:
    """Registry for managing pluggable scrapers"""
    
    _scrapers: Dict[str, Type[BaseScraper]] = {}
    
    @classmethod
    def register(cls, scraper_class: Type[BaseScraper]):
        """Register a new scraper"""
        instance = scraper_class()
        cls._scrapers[instance.source_name] = scraper_class
    
    @classmethod
    def get_scraper(cls, source_name: str) -> BaseScraper:
        """Get scraper instance by source name"""
        if source_name not in cls._scrapers:
            raise ValueError(f"Unknown scraper: {source_name}")
        return cls._scrapers[source_name]()
    
    @classmethod
    def list_scrapers(cls) -> List[str]:
        """List all registered scrapers"""
        return list(cls._scrapers.keys())
    
    @classmethod
    def get_all_scrapers(cls) -> List[BaseScraper]:
        """Get instances of all scrapers"""
        return [scraper_class() for scraper_class in cls._scrapers.values()]

# Auto-registration decorator
def register_scraper(cls):
    """Decorator to automatically register scrapers"""
    ScraperRegistry.register(cls)
    return cls
</code>

app/scrapers/yotspot.py:
<code>
"""Yotspot.com scraper refactored for pluggable architecture"""
import asyncio
import logging
import aiohttp
from typing import Dict, Any, Optional, List
from datetime import datetime
import re
from urllib.parse import urljoin

from .base import BaseScraper, UniversalJob, JobSource, EmploymentType, Department, VesselType
from .registry import register_scraper

logger = logging.getLogger(__name__)

@register_scraper
class YotspotScraper(BaseScraper):
    """Refactored Yotspot.com scraper implementing pluggable interface"""
    
    def __init__(self):
        self.config = {
            'max_retries': 3,
            'request_delay': 2.0,
            'timeout': 30,
            'headers': {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
            }
        }
    
    @property
    def source_name(self) -> str:
        return JobSource.YOTSPOT
    
    @property
    def base_url(self) -> str:
        return "https://www.yotspot.com"
    
    async def scrape_jobs(self, max_pages: int = 5, filters: Optional[Dict[str, Any]] = None):
        """Scrape jobs from Yotspot.com using aiohttp"""
        async with aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=self.config['timeout']),
            headers=self.config['headers']
        ) as session:
            
            for page in range(1, max_pages + 1):
                logger.info(f"Scraping Yotspot page {page}")
                
                try:
                    jobs = await self._scrape_page(session, page, filters)
                    for job in jobs:
                        yield self._normalize_job(job)
                        
                    # Add delay between pages
                    if page < max_pages:
                        await asyncio.sleep(self.config['request_delay'])
                        
                except Exception as e:
                    logger.error(f"Error scraping page {page}: {e}")
                    continue
    
    async def _scrape_page(self, session: aiohttp.ClientSession, page: int, filters: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """Scrape a single page of job listings"""
        url = f"{self.base_url}/job-search.html?page={page}"
        
        # Add filters to URL if provided
        if filters:
            params = []
            if filters.get('location'):
                params.append(f"location={filters['location']}")
            if filters.get('department'):
                params.append(f"department={filters['department']}")
            if params:
                url += "&" + "&".join(params)
        
        try:
            async with session.get(url) as response:
                if response.status != 200:
                    logger.error(f"HTTP {response.status} for {url}")
                    return []
                
                html = await response.text()
                return await self._parse_job_listings(html)
                
        except Exception as e:
            logger.error(f"Error fetching page {page}: {e}")
            return []
    
    async def _parse_job_listings(self, html: str) -> List[Dict[str, Any]]:
        """Parse job listings from HTML"""
        try:
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html, 'html.parser')
            
            jobs = []
            job_cards = soup.find_all('div', class_='job-item')
            
            if not job_cards:
                # Try alternative selectors
                job_cards = soup.find_all('div', class_=re.compile(r'job-listing|job-card'))
                if not job_cards:
                    job_cards = soup.find_all('article', class_=re.compile(r'job'))
                    if not job_cards:
                        job_cards = soup.find_all('div', attrs={'data-job-id': True})
            
            for card in job_cards:
                job_data = self._extract_job_data(card)
                if job_data:
                    jobs.append(job_data)
            
            return jobs
            
        except Exception as e:
            logger.error(f"Error parsing job listings: {e}")
            return []
    
    def _extract_job_data(self, card) -> Optional[Dict[str, Any]]:
        """Extract job data from a single job card"""
        try:
            job_data = {}
            
            # Job title and URL - look for the position link
            title_elem = card.find('div', class_='job-item__position')
            if title_elem:
                title_link = title_elem.find('a')
                if title_link:
                    job_data['title'] = title_link.get_text(strip=True)
                    job_data['url'] = urljoin(self.base_url, title_link.get('href', ''))
                else:
                    return None
            else:
                return None
            
            # Company name - default for yotspot
            job_data['company'] = "Yotspot"
            
            # Location - extract from job-item__info
            location = "Unknown"
            info_list = card.find('ul', class_='job-item__info')
            if info_list:
                info_items = info_list.find_all('li')
                for item in info_items:
                    item_text = item.get_text(strip=True)
                    # Look for location patterns
                    if any(loc in item_text.lower() for loc in ['miami', 'fort lauderdale', 'caribbean', 'mediterranean', 'europe']):
                        location = item_text
                        break
            
            job_data['location'] = location
            
            # Job type - extract from job-item__info
            job_type = None
            if info_list:
                for item in info_list.find_all('li'):
                    item_text = item.get_text(strip=True)
                    if any(type_word in item_text.lower() for type_word in ['permanent', 'temporary', 'contract', 'seasonal']):
                        job_type = item_text
                        break
            
            job_data['job_type'] = job_type
            
            # Posted date - extract from job-item__info
            posted_date = None
            if info_list:
                for item in info_list.find_all('li'):
                    item_text = item.get_text(strip=True)
                    if 'posted' in item_text.lower() or any(date_indicator in item_text.lower() for date_indicator in ['2024', '2025', 'jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']):
                        posted_date = self._parse_date(item_text)
                        break
            
            job_data['posted_date'] = posted_date
            
            # Salary - extract from job-item__info
            salary = None
            if info_list:
                for item in info_list.find_all('li'):
                    item_text = item.get_text(strip=True)
                    if any(currency in item_text.lower() for currency in ['eur', 'usd', 'gbp', '€', '$', '£']):
                        salary = item_text
                        break
            
            job_data['salary'] = salary
            
            # Description - use title as description for now
            job_data['description'] = job_data.get('title', '')
            
            # Extract job ID from URL
            job_data['external_id'] = self._extract_job_id(job_data.get('url', ''))
            
            return job_data
            
        except Exception as e:
            logger.error(f"Error extracting job data: {e}")
            return None
    
    def _extract_job_id(self, url: str) -> str:
        """Extract job ID from URL"""
        if not url:
            return ""
        match = re.search(r'/jobs/(\d+)', url)
        return match.group(1) if match else url
    
    def _parse_date(self, date_text: str) -> Optional[datetime]:
        """Parse posted date from text"""
        if not date_text:
            return None
        
        try:
            from dateparser import parse
            return parse(date_text, settings={'RETURN_AS_TIMEZONE_AWARE': False})
        except:
            return datetime.utcnow()
    
    def _normalize_job(self, raw_job: Dict[str, Any]) -> UniversalJob:
        """Convert raw job data to UniversalJob format"""
        # Parse employment type
        employment_type = self._detect_employment_type(raw_job.get('job_type', ''))
        
        # Parse department
        department = self._detect_department(raw_job.get('title', ''))
        
        # Parse vessel type
        vessel_type = self._detect_vessel_type(raw_job.get('description', ''))
        
        # Calculate quality score
        quality_score = self._calculate_quality_score(raw_job)
        
        return UniversalJob(
            external_id=raw_job.get('external_id', ''),
            title=raw_job.get('title', ''),
            company=raw_job.get('company', ''),
            source=self.source_name,
            source_url=raw_job.get('url', ''),
            location=raw_job.get('location', ''),
            vessel_type=vessel_type,
            employment_type=employment_type,
            department=department,
            salary_range=raw_job.get('salary'),
            description=raw_job.get('description', ''),
            posted_date=raw_job.get('posted_date'),
            quality_score=quality_score,
            raw_data=raw_job
        )
    
    def _detect_employment_type(self, job_type: str) -> Optional[EmploymentType]:
        """Detect employment type from job type text"""
        job_type_lower = job_type.lower() if job_type else ""
        
        if 'permanent' in job_type_lower:
            return EmploymentType.PERMANENT
        elif 'temporary' in job_type_lower:
            return EmploymentType.TEMPORARY
        elif 'rotational' in job_type_lower:
            return EmploymentType.ROTATIONAL
        elif 'seasonal' in job_type_lower:
            return EmploymentType.SEASONAL
        elif 'contract' in job_type_lower:
            return EmploymentType.CONTRACT
        else:
            return EmploymentType.PERMANENT
    
    def _detect_department(self, title: str) -> Optional[Department]:
        """Detect department from job title"""
        title_lower = title.lower() if title else ""
        
        deck_keywords = ['deckhand', 'bosun', 'mate', 'captain', 'officer', 'deck', 'skipper']
        interior_keywords = ['stewardess', 'steward', 'interior', 'housekeeping', 'butler', 'chief stewardess']
        engineering_keywords = ['engineer', 'mechanic', 'eto', 'technical', 'chief engineer']
        galley_keywords = ['chef', 'cook', 'galley', 'kitchen', 'sous chef', 'head chef']
        
        if any(keyword in title_lower for keyword in deck_keywords):
            return Department.DECK
        elif any(keyword in title_lower for keyword in interior_keywords):
            return Department.INTERIOR
        elif any(keyword in title_lower for keyword in engineering_keywords):
            return Department.ENGINEERING
        elif any(keyword in title_lower for keyword in galley_keywords):
            return Department.GALLEY
        else:
            return Department.OTHER
    
    def _detect_vessel_type(self, description: str) -> Optional[VesselType]:
        """Detect vessel type from description"""
        desc_lower = description.lower() if description else ""
        
        if 'sailing' in desc_lower or 'sail' in desc_lower:
            return VesselType.SAILING_YACHT
        elif 'catamaran' in desc_lower:
            return VesselType.CATAMARAN
        elif 'superyacht' in desc_lower or 'super yacht' in desc_lower:
            return VesselType.SUPER_YACHT
        elif 'expedition' in desc_lower:
            return VesselType.EXPEDITION
        else:
            return VesselType.MOTOR_YACHT
    
    def _calculate_quality_score(self, job: Dict[str, Any]) -> float:
        """Calculate data quality score (0-1)"""
        score = 0.0
        
        # Completeness (60%)
        required_fields = ['title', 'company', 'location', 'description']
        completeness = sum(1 for field in required_fields if job.get(field)) / len(required_fields)
        score += completeness * 0.6
        
        # URL validity (20%)
        if job.get('url') and job.get('external_id'):
            score += 0.2
        
        # Description length (20%)
        description = job.get('description', '')
        if len(description) > 200:
            score += 0.2
        elif len(description) > 100:
            score += 0.1
        
        return min(1.0, score)
    
    async def test_connection(self) -> bool:
        """Test Yotspot.com accessibility"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(self.base_url, timeout=10) as response:
                    return response.status == 200
        except Exception as e:
            logger.error(f"Connection test failed: {e}")
            return False
    
    def get_supported_filters(self) -> List[str]:
        """Return supported filter parameters"""
        return ["location", "department", "vessel_type", "salary_range", "experience_level"]
</code>

app/scrapers/daywork123.py:
<code>
"""Daywork123.com scraper with anti-detection measures"""
import asyncio
import logging
from typing import Dict, Any, Optional, List, AsyncIterator
from datetime import datetime
import re
from urllib.parse import urljoin, urlparse
from sqlalchemy.orm import Session

try:
    from playwright.async_api import async_playwright, Browser, Page
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False
    logging.warning("Playwright not available. Install with: pip install playwright")

from .base import BaseScraper, UniversalJob, JobSource, EmploymentType, Department, VesselType
from .registry import register_scraper
from ..database import SessionLocal
from ..models import Job

logger = logging.getLogger(__name__)

@register_scraper
class Daywork123Scraper(BaseScraper):
    """Production-grade Daywork123.com scraper with anti-detection"""
    
    def __init__(self):
        self.config = {
            'max_retries': 3,
            'request_delay': 2.5,
            'user_agents': [
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            ]
        }
    
    @property
    def source_name(self) -> str:
        return JobSource.DAYWORK123
    
    @property
    def base_url(self) -> str:
        return "https://www.daywork123.com"
    
    async def scrape_jobs(self, 
                         max_pages: int = 5,
                         filters: Optional[Dict[str, Any]] = None) -> AsyncIterator[UniversalJob]:
        """Scrape jobs from Daywork123.com"""
        logger.info(f"Starting Daywork123 scraper for {max_pages} pages")
        
        try:
            from playwright.async_api import async_playwright
            
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                context = await browser.new_context(
                    user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                )
                page = await context.new_page()
                
                # Navigate to jobs page
                await page.goto(f"{self.base_url}/JobAnnouncementList.aspx", wait_until='networkidle')
                
                for page_num in range(1, max_pages + 1):
                    logger.info(f"Scraping Daywork123 page {page_num}")
                    
                    if page_num > 1:
                        # Navigate to next page
                        next_url = f"{self.base_url}/JobAnnouncementList.aspx?page={page_num}"
                        await page.goto(next_url, wait_until='networkidle')
                    
                    # Wait for job listings to load (table structure)
                    await page.wait_for_selector('#ContentPlaceHolder1_RepJobAnnouncement', timeout=10000)
                    
                    # Extract job listings from table rows
                    job_elements = await page.query_selector_all('#ContentPlaceHolder1_RepJobAnnouncement tr:not(.head)')
                    
                    # Extract all job data immediately to avoid context issues
                    jobs_found = 0
                    for element in job_elements:
                        try:
                            universal_job = await self._extract_job_from_element(element, page)
                            if universal_job:
                                yield universal_job
                                jobs_found += 1
                        except Exception as e:
                            logger.error(f"Error extracting job element: {e}")
                            continue
                    
                    logger.info(f"Page {page_num}: Found {jobs_found} jobs")
                    
                    if jobs_found == 0:
                        logger.warning(f"No jobs found on page {page_num}, stopping pagination")
                        break
                    
                    # Add delay between pages
                    if page_num < max_pages:
                        await asyncio.sleep(2)
                
                await browser.close()
                
        except Exception as e:
            logger.error(f"Error in Daywork123 scraper: {e}")
            raise
    
    async def _extract_job_from_element(self, element, page) -> Optional[UniversalJob]:
        """Extract job data from a single job element (table row) and return UniversalJob"""
        try:
            # Extract all data immediately to avoid context issues
            cells = await element.query_selector_all('td')
            if len(cells) < 3:  # Need at least ID, title, and other info
                return None
            
            # Extract all text content immediately
            cell_texts = []
            for cell in cells:
                try:
                    text = await cell.text_content()
                    cell_texts.append(text.strip() if text else "")
                except Exception:
                    cell_texts.append("")
            
            if len(cell_texts) < 3:
                return None
            
            # Extract job ID from first cell
            job_id = cell_texts[0] if cell_texts else ""
            if not job_id:
                return None
            
            # Extract title and link from second cell
            title_cell = cells[1] if len(cells) > 1 else None
            if not title_cell:
                return None
            
            # Try to get the link from the title cell
            try:
                link_elem = await title_cell.query_selector('a')
                if link_elem:
                    href = await link_elem.get_attribute('href')
                    job_url = urljoin(self.base_url, href) if href else f"{self.base_url}/JobAnnouncementList.aspx"
                    title = cell_texts[1] if len(cell_texts) > 1 else ""
                else:
                    job_url = f"{self.base_url}/JobAnnouncementList.aspx"
                    title = cell_texts[1] if len(cell_texts) > 1 else ""
            except Exception:
                job_url = f"{self.base_url}/JobAnnouncementList.aspx"
                title = cell_texts[1] if len(cell_texts) > 1 else ""
            
            if not title or len(title) < 3:
                return None
            
            # Extract other information from remaining cells
            location = cell_texts[2] if len(cell_texts) > 2 else "Unknown"
            company = "Daywork123"  # Default company name
            date_posted_str = cell_texts[4] if len(cell_texts) > 4 else ""
            
            # Try to extract company name from the job description if available
            if len(cell_texts) > 3 and cell_texts[3]:
                description_text = cell_texts[3]
                # Extract first few words as potential company name
                words = description_text.split()[:3]  # Take first 3 words
                potential_company = " ".join(words)
                if len(potential_company) <= 100 and len(potential_company) > 2:
                    company = potential_company
                description = description_text
            else:
                description = f"Job ID: {job_id} - Position available via Daywork123.com"
            
            # Parse date
            posted_date = self._parse_date(date_posted_str) if date_posted_str else datetime.utcnow()
            
            # Parse employment type from title
            employment_type = self._detect_employment_type(title)
            
            # Parse department from title
            department = self._detect_department(title)
            
            # Parse vessel type (if any info available)
            vessel_type = self._detect_vessel_type(title + " " + description)
            
            # Calculate quality score
            quality_score = self._calculate_quality_score({
                'title': title,
                'company': company,
                'location': location,
                'description': description,
                'url': job_url,
                'external_id': job_id
            })
            
            # Create raw data for debugging
            raw_data = {
                'job_id': job_id,
                'cell_texts': cell_texts,
                'extraction_timestamp': datetime.utcnow().isoformat(),
                'page_url': page.url if page else None
            }
            
            # Create UniversalJob object
            universal_job = UniversalJob(
                external_id=f"dw123_{job_id}",
                title=title,
                company=company,
                source=JobSource.DAYWORK123,
                source_url=job_url,
                location=location,
                description=description,
                employment_type=employment_type,
                department=department,
                vessel_type=vessel_type,
                posted_date=posted_date,
                requirements=[],
                benefits=[],
                quality_score=quality_score,
                raw_data=raw_data
            )
            
            return universal_job
            
        except Exception as e:
            logger.error(f"Error extracting job element: {e}")
            return None
    
    async def _get_job_details(self, job_url: str, page) -> Dict[str, Any]:
        """Get detailed job information from job page"""
        try:
            await page.goto(job_url, wait_until='networkidle')
            
            # Extract detailed description
            desc_elem = await page.query_selector('.job-details, .full-description')
            full_description = await desc_elem.text_content() if desc_elem else ""
            
            # Extract requirements
            req_elem = await page.query_selector('.requirements, .job-requirements')
            requirements_text = await req_elem.text_content() if req_elem else ""
            requirements = self._parse_requirements(requirements_text)
            
            # Extract benefits
            benefits_elem = await page.query_selector('.benefits, .job-benefits')
            benefits_text = await benefits_elem.text_content() if benefits_elem else ""
            benefits = self._parse_benefits(benefits_text)
            
            # Extract vessel info
            vessel_elem = await page.query_selector('.vessel-info, .yacht-details')
            vessel_info = await vessel_elem.text_content() if vessel_elem else ""
            
            return {
                'full_description': full_description,
                'requirements': requirements,
                'benefits': benefits,
                'vessel_info': vessel_info
            }
            
        except Exception as e:
            logger.error(f"Error getting job details: {e}")
            return {}
    
    def _normalize_job(self, raw_job: Dict[str, Any]) -> UniversalJob:
        """Convert raw job data to UniversalJob format"""
        # Parse employment type
        employment_type = self._detect_employment_type(raw_job.get('title', ''))
        
        # Parse department
        department = self._detect_department(raw_job.get('title', ''))
        
        # Parse vessel type
        vessel_type = self._detect_vessel_type(raw_job.get('vessel_info', ''))
        
        # Calculate quality score
        quality_score = self._calculate_quality_score(raw_job)
        
        return UniversalJob(
            external_id=raw_job.get('external_id', ''),
            title=raw_job.get('title', ''),
            company=raw_job.get('company', ''),
            source=self.source_name,
            source_url=raw_job.get('url', ''),
            location=raw_job.get('location', ''),
            vessel_type=vessel_type,
            employment_type=employment_type,
            department=department,
            salary_range=raw_job.get('salary'),
            description=raw_job.get('full_description', raw_job.get('description', '')),
            requirements=raw_job.get('requirements', []),
            benefits=raw_job.get('benefits', []),
            posted_date=raw_job.get('posted_date'),
            quality_score=quality_score,
            raw_data=raw_job
        )
    
    def _extract_job_id(self, url: str) -> str:
        """Extract job ID from URL"""
        if not url:
            return ""
        match = re.search(r'/jobs/(\d+)', url)
        return match.group(1) if match else url
    
    def _parse_date(self, date_text: str) -> Optional[datetime]:
        """Parse posted date from text"""
        if not date_text:
            return None
        
        # Handle relative dates like "2 days ago", "1 week ago"
        import dateparser
        try:
            return dateparser.parse(date_text, settings={'RETURN_AS_TIMEZONE_AWARE': False})
        except:
            return datetime.utcnow()
    
    def _parse_requirements(self, text: str) -> List[str]:
        """Parse requirements from text"""
        if not text:
            return []
        
        # Split by common delimiters
        requirements = re.split(r'[•·\-\n]', text)
        return [req.strip() for req in requirements if req.strip()]
    
    def _parse_benefits(self, text: str) -> List[str]:
        """Parse benefits from text"""
        if not text:
            return []
        
        benefits = re.split(r'[•·\-\n]', text)
        return [benefit.strip() for benefit in benefits if benefit.strip()]
    
    def _detect_employment_type(self, title: str) -> Optional[EmploymentType]:
        """Detect employment type from job title"""
        title_lower = title.lower()
        
        if any(word in title_lower for word in ['daywork', 'day work', 'daily']):
            return EmploymentType.DAYWORK
        elif any(word in title_lower for word in ['rotational', 'rotation']):
            return EmploymentType.ROTATIONAL
        elif any(word in title_lower for word in ['seasonal', 'season']):
            return EmploymentType.SEASONAL
        elif any(word in title_lower for word in ['contract', 'temporary']):
            return EmploymentType.TEMPORARY
        else:
            return EmploymentType.PERMANENT
    
    def _detect_department(self, title: str) -> Optional[Department]:
        """Detect department from job title"""
        title_lower = title.lower()
        
        deck_keywords = ['deckhand', 'bosun', 'mate', 'captain', 'officer', 'deck']
        interior_keywords = ['stewardess', 'steward', 'interior', 'housekeeping', 'butler']
        engineering_keywords = ['engineer', 'mechanic', 'eto', 'technical']
        galley_keywords = ['chef', 'cook', 'galley', 'kitchen']
        
        if any(keyword in title_lower for keyword in deck_keywords):
            return Department.DECK
        elif any(keyword in title_lower for keyword in interior_keywords):
            return Department.INTERIOR
        elif any(keyword in title_lower for keyword in engineering_keywords):
            return Department.ENGINEERING
        elif any(keyword in title_lower for keyword in galley_keywords):
            return Department.GALLEY
        else:
            return Department.OTHER
    
    def _detect_vessel_type(self, text: str) -> Optional[VesselType]:
        """Detect vessel type from text"""
        text_lower = text.lower()
        
        if 'sailing' in text_lower or 'sail' in text_lower:
            return VesselType.SAILING_YACHT
        elif 'catamaran' in text_lower:
            return VesselType.CATAMARAN
        elif 'superyacht' in text_lower or 'super yacht' in text_lower:
            return VesselType.SUPER_YACHT
        elif 'expedition' in text_lower:
            return VesselType.EXPEDITION
        else:
            return VesselType.MOTOR_YACHT
    
    def _calculate_quality_score(self, job: Dict[str, Any]) -> float:
        """Calculate data quality score (0-1)"""
        score = 0.0
        
        # Completeness (60%)
        required_fields = ['title', 'company', 'location', 'description']
        completeness = sum(1 for field in required_fields if job.get(field)) / len(required_fields)
        score += completeness * 0.6
        
        # URL validity (20%)
        if job.get('url') and job.get('external_id'):
            score += 0.2
        
        # Description length (20%)
        description = job.get('description', '')
        if len(description) > 100:
            score += 0.2
        elif len(description) > 50:
            score += 0.1
        
        return min(1.0, score)
    
    async def test_connection(self) -> bool:
        """Test Daywork123.com accessibility"""
        if not PLAYWRIGHT_AVAILABLE:
            return False
        
        try:
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                context = await browser.new_context()
                page = await context.new_page()
                
                response = await page.goto(self.base_url, timeout=10000)
                await browser.close()
                
                return response.status == 200
        except Exception as e:
            logger.error(f"Connection test failed: {e}")
            return False
    
    def get_supported_filters(self) -> List[str]:
        """Return supported filter parameters"""
        return ["location", "date_range", "job_type", "vessel_size", "salary_range"]
    
    async def save_jobs_to_db(self, jobs: List[UniversalJob]) -> int:
        """Save scraped jobs to yachtjobs.db
        
        Args:
            jobs: List of UniversalJob objects to save
            
        Returns:
            Number of jobs successfully saved
        """
        if not jobs:
            return 0
            
        saved_count = 0
        
        with SessionLocal() as db:
            for job in jobs:
                try:
                    # Check if job already exists
                    existing_job = db.query(Job).filter(
                        Job.external_id == job.external_id,
                        Job.source == job.source
                    ).first()
                    
                    if existing_job:
                        # Update existing job
                        existing_job.title = job.title
                        existing_job.company = job.company
                        existing_job.location = job.location
                        existing_job.country = job.country
                        existing_job.region = job.region
                        existing_job.description = job.description
                        existing_job.salary_range = job.salary_range
                        existing_job.salary_currency = job.salary_currency
                        existing_job.salary_period = job.salary_period
                        existing_job.employment_type = job.employment_type.value if job.employment_type else None
                        existing_job.job_type = job.employment_type.value if job.employment_type else None
                        existing_job.department = job.department.value if job.department else None
                        existing_job.vessel_type = job.vessel_type.value if job.vessel_type else None
                        existing_job.vessel_size = job.vessel_size
                        existing_job.vessel_name = job.vessel_name
                        existing_job.position_level = job.position_level
                        existing_job.start_date = job.start_date
                        existing_job.requirements = job.requirements
                        existing_job.benefits = job.benefits
                        existing_job.posted_date = job.posted_date
                        existing_job.quality_score = job.quality_score
                        existing_job.raw_data = job.raw_data
                        existing_job.updated_at = datetime.utcnow()
                        
                        logger.debug(f"Updated existing job: {job.title}")
                    else:
                        # Create new job
                        db_job = Job(
                            external_id=job.external_id,
                            title=job.title,
                            company=job.company,
                            location=job.location,
                            country=job.country,
                            region=job.region,
                            description=job.description,
                            source=job.source,
                            source_url=str(job.source_url),
                            salary_range=job.salary_range,
                            salary_currency=job.salary_currency,
                            salary_period=job.salary_period,
                            employment_type=job.employment_type.value if job.employment_type else None,
                            job_type=job.employment_type.value if job.employment_type else None,
                            department=job.department.value if job.department else None,
                            vessel_type=job.vessel_type.value if job.vessel_type else None,
                            vessel_size=job.vessel_size,
                            vessel_name=job.vessel_name,
                            position_level=job.position_level,
                            start_date=job.start_date,
                            requirements=job.requirements,
                            benefits=job.benefits,
                            posted_date=job.posted_date,
                            posted_at=job.posted_date,
                            quality_score=job.quality_score,
                            raw_data=job.raw_data,
                            scraped_at=job.scraped_at,
                            created_at=datetime.utcnow(),
                            updated_at=datetime.utcnow()
                        )
                        db.add(db_job)
                        logger.debug(f"Added new job: {job.title}")
                    
                    saved_count += 1
                    
                except Exception as e:
                    logger.error(f"Error saving job {job.title}: {e}")
                    continue
            
            try:
                db.commit()
                logger.info(f"Successfully saved {saved_count} jobs to database")
            except Exception as e:
                logger.error(f"Error committing jobs to database: {e}")
                db.rollback()
                return 0
        
        return saved_count
    
    async def scrape_and_save_jobs(self, max_pages: int = 5, filters: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Scrape jobs and save them to database
        
        Args:
            max_pages: Maximum number of pages to scrape
            filters: Optional filters to apply
            
        Returns:
            Dictionary with scraping results
        """
        start_time = datetime.utcnow()
        jobs_list = []
        
        try:
            # Collect all jobs from scraping
            async for job in self.scrape_jobs(max_pages, filters):
                jobs_list.append(job)
            
            # Save to database
            saved_count = await self.save_jobs_to_db(jobs_list)
            
            duration = (datetime.utcnow() - start_time).total_seconds()
            
            result = {
                "source": self.source_name,
                "jobs_found": len(jobs_list),
                "jobs_saved": saved_count,
                "duration": duration,
                "timestamp": datetime.utcnow(),
                "success": True,
                "errors": []
            }
            
            logger.info(f"Daywork123 scraping completed: {len(jobs_list)} found, {saved_count} saved")
            return result
            
        except Exception as e:
            duration = (datetime.utcnow() - start_time).total_seconds()
            logger.error(f"Error in Daywork123 scrape_and_save_jobs: {e}")
            
            return {
                "source": self.source_name,
                "jobs_found": 0,
                "jobs_saved": 0,
                "duration": duration,
                "timestamp": datetime.utcnow(),
                "success": False,
                "errors": [str(e)]
            }
</code>

app/scrapers/base.py:
<code>
"""Base scraper interface for pluggable architecture"""
from abc import ABC, abstractmethod
from typing import List, Dict, Any, AsyncIterator, Optional
from datetime import datetime
from pydantic import BaseModel, Field, HttpUrl
from enum import Enum

class JobSource(str, Enum):
    """Supported job sources"""
    YOTSPOT = "yotspot"
    DAYWORK123 = "daywork123"
    MERIDIAN_GO = "meridian_go"

class EmploymentType(str, Enum):
    """Employment types across all sources"""
    PERMANENT = "permanent"
    TEMPORARY = "temporary"
    ROTATIONAL = "rotational"
    DAYWORK = "daywork"
    SEASONAL = "seasonal"
    CONTRACT = "contract"

class Department(str, Enum):
    """Yacht departments"""
    DECK = "deck"
    INTERIOR = "interior"
    ENGINEERING = "engineering"
    GALLEY = "galley"
    BRIDGE = "bridge"
    OTHER = "other"

class VesselType(str, Enum):
    """Types of vessels"""
    MOTOR_YACHT = "motor_yacht"
    SAILING_YACHT = "sailing_yacht"
    CATAMARAN = "catamaran"
    SUPER_YACHT = "super_yacht"
    EXPEDITION = "expedition"
    CHASE_BOAT = "chase_boat"

class UniversalJob(BaseModel):
    """Standardized job format across all sources"""
    
    # Required fields
    external_id: str = Field(..., description="Unique ID from source")
    title: str = Field(..., min_length=3, max_length=200)
    company: str = Field(..., max_length=100)
    source: JobSource
    source_url: HttpUrl
    
    # Location
    location: str = Field(..., max_length=100)
    country: Optional[str] = None
    region: Optional[str] = None
    
    # Vessel information
    vessel_type: Optional[VesselType] = None
    vessel_size: Optional[str] = None
    vessel_name: Optional[str] = None
    
    # Employment details
    employment_type: Optional[EmploymentType] = None
    department: Optional[Department] = None
    position_level: Optional[str] = None
    
    # Compensation
    salary_range: Optional[str] = None
    salary_currency: Optional[str] = None
    salary_period: Optional[str] = None
    
    # Timing
    start_date: Optional[str] = None
    posted_date: Optional[datetime] = None
    
    # Content
    description: str = Field(..., min_length=10)
    requirements: List[str] = Field(default_factory=list)
    benefits: List[str] = Field(default_factory=list)
    
    # Metadata
    scraped_at: datetime = Field(default_factory=datetime.utcnow)
    quality_score: float = Field(default=0.0, ge=0.0, le=1.0)
    raw_data: Optional[Dict[str, Any]] = None
    
    class Config:
        use_enum_values = True

class ScrapingResult(BaseModel):
    """Result from scraping operation"""
    source: str
    jobs_found: int
    new_jobs: int
    updated_jobs: int
    errors: List[str] = Field(default_factory=list)
    duration: float
    timestamp: datetime = Field(default_factory=datetime.utcnow)

class BaseScraper(ABC):
    """Abstract base class for all scrapers"""
    
    @property
    @abstractmethod
    def source_name(self) -> str:
        """Unique identifier for this scraper"""
        pass
    
    @property
    @abstractmethod
    def base_url(self) -> str:
        """Base URL of the source website"""
        pass
    
    @abstractmethod
    async def scrape_jobs(self, 
                         max_pages: int = 5,
                         filters: Optional[Dict[str, Any]] = None) -> AsyncIterator[UniversalJob]:
        """Scrape jobs and yield standardized job data"""
        pass
    
    @abstractmethod
    async def test_connection(self) -> bool:
        """Test if the source is accessible"""
        pass
    
    @abstractmethod
    def get_supported_filters(self) -> List[str]:
        """Return list of supported filter parameters"""
        pass
    
    async def health_check(self) -> Dict[str, Any]:
        """Health check for monitoring"""
        return {
            "source": self.source_name,
            "accessible": await self.test_connection(),
            "last_scrape": None,
            "status": "healthy" if await self.test_connection() else "unhealthy"
        }
</code>

app/services/scheduler_service.py:
<code>
"""
Scheduler service layer for managing scraping automation.

This module provides a high-level interface to the scheduler functionality,
making it easier to use from other parts of the application.
"""
import logging
import asyncio
from datetime import datetime
from typing import Dict, List, Optional

from ..daywork_scheduler import ScrapingScheduler, run_daywork123_scraping_job
from ..config import SchedulerConfig
from ..scrapers.registry import ScraperRegistry


logger = logging.getLogger(__name__)


class SchedulerService:
    """
    High-level service for managing the scraping scheduler.
    
    This service provides a simplified interface to the scheduler functionality
    and handles common operations like starting, stopping, and monitoring jobs.
    """
    
    def __init__(self, config: SchedulerConfig = None):
        """
        Initialize the scheduler service.
        
        Args:
            config: Configuration object, defaults to SchedulerConfig
        """
        self.config = config or SchedulerConfig()
        self.scheduler = ScrapingScheduler(self.config)
        self._running = False
        
    async def start(self):
        """Start the scheduler service."""
        try:
            if self._running:
                logger.warning("Scheduler service is already running")
                return
                
            await self.scheduler.start()
            self._running = True
            logger.info("Scheduler service started successfully")
            
        except Exception as e:
            logger.error(f"Error starting scheduler service: {e}")
            raise
            
    async def stop(self):
        """Stop the scheduler service."""
        try:
            if not self._running:
                logger.warning("Scheduler service is not running")
                return
                
            await self.scheduler.stop()
            self._running = False
            logger.info("Scheduler service stopped successfully")
            
        except Exception as e:
            logger.error(f"Error stopping scheduler service: {e}")
            raise
            
    def get_scheduler_status(self) -> Dict:
        """
        Get comprehensive scheduler status information.
        
        Returns:
            Dictionary with scheduler status, job information, and configuration
        """
        try:
            status = self.scheduler.get_scheduler_status()
            status.update({
                'service_running': self._running,
                'last_updated': datetime.now().isoformat()
            })
            return status
        except Exception as e:
            logger.error(f"Error getting scheduler status: {e}")
            return {
                'service_running': False,
                'error': str(e),
                'last_updated': datetime.now().isoformat()
            }
            
    async def run_daywork123_now(self, period: str = 'manual') -> Dict:
        """
        Execute the Daywork123 scraper immediately.
        
        Args:
            period: Period identifier for logging purposes
            
        Returns:
            Dictionary with execution results
        """
        try:
            start_time = datetime.now()
            logger.info(f"Running Daywork123 scraper manually ({period})")
            
            # Use the standalone function to run the scraper
            # We use current time for hour/minute since this is a manual run
            current_hour = start_time.hour
            current_minute = start_time.minute
            
            await run_daywork123_scraping_job(
                period=period,
                hour=current_hour,
                minute=current_minute,
                max_pages=self.config.DAYWORK123_MAX_PAGES
            )
            
            duration = (datetime.now() - start_time).total_seconds()
            
            result = {
                'success': True,
                'period': period,
                'started_at': start_time.isoformat(),
                'duration_seconds': duration,
                'max_pages': self.config.DAYWORK123_MAX_PAGES
            }
            
            logger.info(
                f"Manual Daywork123 scraping completed - "
                f"Duration: {duration:.2f}s"
            )
            
            return result
            
        except Exception as e:
            logger.error(f"Error running Daywork123 scraper manually: {e}")
            return {
                'success': False,
                'error': str(e),
                'period': period,
                'started_at': start_time.isoformat() if 'start_time' in locals() else None
            }
            
    async def update_daywork123_schedule(
        self,
        morning_hours: List[int] = None,
        morning_minutes: List[int] = None,
        day_hours: List[int] = None,
        day_minutes: List[int] = None,
        evening_hours: List[int] = None,
        evening_minutes: List[int] = None
    ) -> Dict:
        """
        Update the complete Daywork123 schedule with new time periods.
        
        Args:
            morning_hours: New morning hours (0-23)
            morning_minutes: New morning minutes (0-59)
            day_hours: New day hours (0-23)
            day_minutes: New day minutes (0-59)
            evening_hours: New evening hours (0-23)
            evening_minutes: New evening minutes (0-59)
            
        Returns:
            Dictionary with update results
        """
        try:
            # Update configuration if new values provided
            if morning_hours is not None:
                self.config.MORNING_HOURS = morning_hours
            if morning_minutes is not None:
                self.config.MORNING_MINUTES = morning_minutes
            if day_hours is not None:
                self.config.DAY_HOURS = day_hours
            if day_minutes is not None:
                self.config.DAY_MINUTES = day_minutes
            if evening_hours is not None:
                self.config.EVENING_HOURS = evening_hours
            if evening_minutes is not None:
                self.config.EVENING_MINUTES = evening_minutes
            
            # Validate new configuration
            if not self.config.validate_config():
                raise ValueError("Invalid schedule configuration")
            
            # Reschedule jobs
            await self.scheduler.schedule_daywork123_scraper()
            
            result = {
                'success': True,
                'updated_at': datetime.now().isoformat(),
                'new_schedule': {
                    'morning_hours': self.config.MORNING_HOURS,
                    'morning_minutes': self.config.MORNING_MINUTES,
                    'day_hours': self.config.DAY_HOURS,
                    'day_minutes': self.config.DAY_MINUTES,
                    'evening_hours': self.config.EVENING_HOURS,
                    'evening_minutes': self.config.EVENING_MINUTES
                },
                'total_daily_runs': self.config.get_total_daily_runs()
            }
            
            logger.info(f"Daywork123 schedule updated - {result['total_daily_runs']} runs per day")
            return result
            
        except Exception as e:
            logger.error(f"Error updating Daywork123 schedule: {e}")
            return {
                'success': False,
                'error': str(e),
                'updated_at': datetime.now().isoformat()
            }
            
    async def update_morning_schedule(
        self,
        hours: List[int],
        minutes: List[int]
    ) -> Dict:
        """
        Update only the morning scraping schedule.
        
        Args:
            hours: New morning hours (0-23)
            minutes: New morning minutes (0-59)
            
        Returns:
            Dictionary with update results
        """
        return await self.update_daywork123_schedule(
            morning_hours=hours,
            morning_minutes=minutes
        )
        
    async def update_day_schedule(
        self,
        hours: List[int],
        minutes: List[int]
    ) -> Dict:
        """
        Update only the daytime scraping schedule.
        
        Args:
            hours: New day hours (0-23)
            minutes: New day minutes (0-59)
            
        Returns:
            Dictionary with update results
        """
        return await self.update_daywork123_schedule(
            day_hours=hours,
            day_minutes=minutes
        )
        
    async def update_evening_schedule(
        self,
        hours: List[int],
        minutes: List[int]
    ) -> Dict:
        """
        Update only the evening scraping schedule.
        
        Args:
            hours: New evening hours (0-23)
            minutes: New evening minutes (0-59)
            
        Returns:
            Dictionary with update results
        """
        return await self.update_daywork123_schedule(
            evening_hours=hours,
            evening_minutes=minutes
        )
        
    def get_jobs_status(self) -> List[Dict]:
        """
        Get status information for all Daywork123 jobs.
        
        Returns:
            List of dictionaries with job information
        """
        try:
            return self.scheduler.get_all_jobs_status()
        except Exception as e:
            logger.error(f"Error getting jobs status: {e}")
            return []
            
    def pause_job(self, job_id: str) -> Dict:
        """
        Pause a specific job.
        
        Args:
            job_id: ID of the job to pause
            
        Returns:
            Dictionary with operation result
        """
        try:
            success = self.scheduler.pause_job(job_id)
            return {
                'success': success,
                'job_id': job_id,
                'action': 'pause',
                'timestamp': datetime.now().isoformat()
            }
        except Exception as e:
            logger.error(f"Error pausing job {job_id}: {e}")
            return {
                'success': False,
                'job_id': job_id,
                'action': 'pause',
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
            
    def resume_job(self, job_id: str) -> Dict:
        """
        Resume a specific job.
        
        Args:
            job_id: ID of the job to resume
            
        Returns:
            Dictionary with operation result
        """
        try:
            success = self.scheduler.resume_job(job_id)
            return {
                'success': success,
                'job_id': job_id,
                'action': 'resume',
                'timestamp': datetime.now().isoformat()
            }
        except Exception as e:
            logger.error(f"Error resuming job {job_id}: {e}")
            return {
                'success': False,
                'job_id': job_id,
                'action': 'resume',
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
            
    def remove_job(self, job_id: str) -> Dict:
        """
        Remove a specific job.
        
        Args:
            job_id: ID of the job to remove
            
        Returns:
            Dictionary with operation result
        """
        try:
            success = self.scheduler.remove_job(job_id)
            return {
                'success': success,
                'job_id': job_id,
                'action': 'remove',
                'timestamp': datetime.now().isoformat()
            }
        except Exception as e:
            logger.error(f"Error removing job {job_id}: {e}")
            return {
                'success': False,
                'job_id': job_id,
                'action': 'remove',
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
            
    def get_job_status(self, job_id: str) -> Optional[Dict]:
        """
        Get status information for a specific job.
        
        Args:
            job_id: ID of the job
            
        Returns:
            Dictionary with job information or None if not found
        """
        try:
            return self.scheduler.get_job_status(job_id)
        except Exception as e:
            logger.error(f"Error getting job status for {job_id}: {e}")
            return None
            
    def is_running(self) -> bool:
        """Check if the scheduler service is running."""
        return self._running and self.scheduler.is_running()
        
    async def restart(self):
        """Restart the scheduler service."""
        try:
            logger.info("Restarting scheduler service")
            await self.stop()
            await self.start()
            logger.info("Scheduler service restarted successfully")
        except Exception as e:
            logger.error(f"Error restarting scheduler service: {e}")
            raise
            
    def get_next_runs(self, limit: int = 10) -> List[Dict]:
        """
        Get information about the next scheduled runs.
        
        Args:
            limit: Maximum number of next runs to return
            
        Returns:
            List of dictionaries with next run information
        """
        try:
            jobs = self.scheduler.get_all_jobs_status()
            next_runs = []
            
            for job in jobs:
                if job.get('next_run_time'):
                    next_runs.append({
                        'job_id': job['id'],
                        'job_name': job['name'],
                        'next_run_time': job['next_run_time'],
                        'period': job.get('kwargs', {}).get('period', 'unknown')
                    })
            
            # Sort by next run time
            next_runs.sort(key=lambda x: x['next_run_time'])
            
            return next_runs[:limit]
            
        except Exception as e:
            logger.error(f"Error getting next runs: {e}")
            return []

</code>

app/services/scraping_service.py:
<code>
"""Unified scraping service for managing all scrapers"""
import asyncio
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime
from sqlalchemy.orm import Session

from app.scrapers.registry import ScraperRegistry
from app.scrapers.base import UniversalJob
from app.database import SessionLocal
from app.models import Job

logger = logging.getLogger(__name__)

class ScrapingService:
    """Unified service for managing all scrapers"""
    
    def __init__(self):
        self.registry = ScraperRegistry()
    
    async def scrape_source(self, source_name: str, max_pages: int = 5) -> Dict[str, Any]:
        """Scrape a specific source"""
        try:
            scraper = self.registry.get_scraper(source_name)
            
            start_time = datetime.utcnow()
            jobs_found = 0
            new_jobs = 0
            updated_jobs = 0
            errors = []
            
            # Collect all jobs
            jobs = []
            async for job in scraper.scrape_jobs(max_pages=max_pages):
                jobs.append(job)
                jobs_found += 1
            
            # Save to database
            if jobs:
                new_jobs, updated_jobs = await self._save_jobs(jobs)
            
            duration = (datetime.utcnow() - start_time).total_seconds()
            
            result = {
                "source": source_name,
                "jobs_found": jobs_found,
                "new_jobs": new_jobs,
                "updated_jobs": updated_jobs,
                "errors": errors,
                "duration": duration,
                "timestamp": datetime.utcnow()
            }
            
            logger.info(f"Scraped {source_name}: {jobs_found} found, {new_jobs} new, {updated_jobs} updated")
            return result
            
        except Exception as e:
            logger.error(f"Error scraping {source_name}: {e}")
            return {
                "source": source_name,
                "jobs_found": 0,
                "new_jobs": 0,
                "updated_jobs": 0,
                "errors": [str(e)],
                "duration": 0,
                "timestamp": datetime.utcnow()
            }
    
    async def scrape_all_sources(self, max_pages: int = 5) -> List[Dict[str, Any]]:
        """Scrape all registered sources"""
        results = []
        sources = self.registry.list_scrapers()
        
        for source_name in sources:
            result = await self.scrape_source(source_name, max_pages)
            results.append(result)
            
            # Add delay between sources to be respectful
            if source_name != sources[-1]:
                await asyncio.sleep(30)
        
        return results
    
    async def health_check_all(self) -> Dict[str, Any]:
        """Health check for all scrapers"""
        health_status = {}
        
        for scraper in self.registry.get_all_scrapers():
            try:
                health_status[scraper.source_name] = await scraper.health_check()
            except Exception as e:
                health_status[scraper.source_name] = {
                    "source": scraper.source_name,
                    "accessible": False,
                    "status": "error",
                    "error": str(e)
                }
        
        return health_status
    
    async def _save_jobs(self, jobs: List[UniversalJob]) -> tuple[int, int]:
        """Save jobs to database with deduplication"""
        new_jobs_count = 0
        updated_jobs_count = 0
        
        with SessionLocal() as db:
            for job in jobs:
                try:
                    # Check if job already exists
                    existing_job = db.query(Job).filter(
                        Job.external_id == job.external_id,
                        Job.source == job.source
                    ).first()
                    
                    if existing_job:
                        # Update existing job
                        existing_job.title = job.title
                        existing_job.company = job.company
                        existing_job.location = job.location
                        existing_job.country = job.country
                        existing_job.region = job.region
                        existing_job.description = job.description
                        existing_job.salary_range = job.salary_range
                        existing_job.salary_currency = job.salary_currency
                        existing_job.salary_period = job.salary_period
                        existing_job.employment_type = job.employment_type.value if job.employment_type else None
                        existing_job.job_type = job.employment_type.value if job.employment_type else None  # Keep compatibility
                        existing_job.department = job.department.value if job.department else None
                        existing_job.vessel_type = job.vessel_type.value if job.vessel_type else None
                        existing_job.vessel_size = job.vessel_size
                        existing_job.vessel_name = job.vessel_name
                        existing_job.position_level = job.position_level
                        existing_job.start_date = job.start_date
                        existing_job.requirements = job.requirements
                        existing_job.benefits = job.benefits
                        existing_job.posted_date = job.posted_date
                        existing_job.quality_score = job.quality_score
                        existing_job.raw_data = job.raw_data
                        existing_job.updated_at = datetime.utcnow()
                        
                        updated_jobs_count += 1
                    else:
                        # Create new job
                        db_job = Job(
                            external_id=job.external_id,
                            title=job.title,
                            company=job.company,
                            location=job.location,
                            country=job.country,
                            region=job.region,
                            description=job.description,
                            source=job.source,
                            source_url=str(job.source_url),
                            salary_range=job.salary_range,
                            salary_currency=job.salary_currency,
                            salary_period=job.salary_period,
                            employment_type=job.employment_type.value if job.employment_type else None,
                            job_type=job.employment_type.value if job.employment_type else None,  # Keep compatibility
                            department=job.department.value if job.department else None,
                            vessel_type=job.vessel_type.value if job.vessel_type else None,
                            vessel_size=job.vessel_size,
                            vessel_name=job.vessel_name,
                            position_level=job.position_level,
                            start_date=job.start_date,
                            requirements=job.requirements,
                            benefits=job.benefits,
                            posted_date=job.posted_date,
                            posted_at=job.posted_date,  # Keep compatibility
                            quality_score=job.quality_score,
                            raw_data=job.raw_data,
                            scraped_at=job.scraped_at,
                            created_at=datetime.utcnow(),
                            updated_at=datetime.utcnow()
                        )
                        db.add(db_job)
                        new_jobs_count += 1
                
                except Exception as e:
                    logger.error(f"Error saving job {job.title}: {e}")
                    continue
            
            try:
                db.commit()
            except Exception as e:
                logger.error(f"Error committing jobs to database: {e}")
                db.rollback()
                return 0, 0
        
        return new_jobs_count, updated_jobs_count
    
    def get_scraper_stats(self) -> Dict[str, Any]:
        """Get statistics for all scrapers"""
        stats = {
            "total_scrapers": len(self.registry.list_scrapers()),
            "available_scrapers": self.registry.list_scrapers(),
            "health_status": {}
        }
        
        # Get health status for each scraper
        for scraper in self.registry.get_all_scrapers():
            stats["health_status"][scraper.source_name] = {
                "supported_filters": scraper.get_supported_filters(),
                "base_url": scraper.base_url
            }
        
        return stats

# Convenience functions for backward compatibility
async def scrape_daywork123(max_pages: int = 5) -> Dict[str, Any]:
    """Convenience function to scrape Daywork123.com"""
    service = ScrapingService()
    return await service.scrape_source("daywork123", max_pages)

async def scrape_yotspot(max_pages: int = 5) -> Dict[str, Any]:
    """Convenience function to scrape Yotspot.com"""
    service = ScrapingService()
    return await service.scrape_source("yotspot", max_pages)

async def scrape_all_sources(max_pages: int = 5) -> List[Dict[str, Any]]:
    """Convenience function to scrape all sources"""
    service = ScrapingService()
    return await service.scrape_all_sources(max_pages)
</code>

app/database.py:
<code>
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
import os
from dotenv import load_dotenv

load_dotenv()

# Database URL from environment or default to SQLite
DATABASE_URL = os.getenv("DATABASE_URL", "sqlite:///./yacht_jobs.db")

# Create engine
engine = create_engine(DATABASE_URL, connect_args={"check_same_thread": False} if "sqlite" in DATABASE_URL else {})

# Session factory
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

# Base class for models
Base = declarative_base()

def get_db():
    """Database dependency for FastAPI"""
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close() 
</code>

app/scraper.py:
<code>
import requests
from bs4 import BeautifulSoup
import time
import random
import logging
from datetime import datetime
from typing import List, Dict, Any
import re
from urllib.parse import urljoin, urlparse

logger = logging.getLogger(__name__)

class YotspotScraper:
    def __init__(self):
        self.base_url = "https://www.yotspot.com"
        self.search_url = f"{self.base_url}/job-search.html"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
    def _respectful_delay(self, min_delay: float = 2.0, max_delay: float = 5.0):
        """Add random delay between requests to be respectful"""
        delay = random.uniform(min_delay, max_delay)
        time.sleep(delay)
    
    def _parse_job_card(self, job_element) -> Dict[str, Any]:
        """Parse individual job card from HTML"""
        try:
            job_data = {}
            
            # Extract job title
            title_elem = job_element.find('h3') or job_element.find('h4') or job_element.find('a')
            if title_elem:
                job_data['title'] = title_elem.get_text(strip=True)
                # Extract external ID from URL if present
                if title_elem.find('a'):
                    href = title_elem.find('a').get('href', '')
                    job_id_match = re.search(r'#(\d+)', href)
                    if job_id_match:
                        job_data['external_id'] = job_id_match.group(1)
            
            # Extract job details (usually in list items or divs)
            details = job_element.find_all(['li', 'div', 'span'])
            
            for detail in details:
                text = detail.get_text(strip=True)
                if not text:
                    continue
                
                # Parse different job attributes
                if 'Starting' in text and any(month in text for month in ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']):
                    job_data['start_date'] = text
                elif any(job_type in text for job_type in ['Permanent', 'Temporary', 'Rotational', 'Seasonal', 'Contract']):
                    job_data['job_type'] = text
                elif 'Motor Yacht' in text or 'Sailing Yacht' in text or 'Chase Boat' in text:
                    job_data['vessel_type'] = text
                elif 'm (' in text and 'ft)' in text:  # e.g., "36m (118ft)"
                    job_data['vessel_size'] = text
                elif any(currency in text for currency in ['USD', 'EUR', 'GBP', '$', '€', '£']):
                    job_data['salary_range'] = text
                    if 'Per Month' in text or 'per month' in text:
                        job_data['salary_per'] = 'month'
                    elif 'Per Day' in text or 'per day' in text:
                        job_data['salary_per'] = 'day'
                    elif 'Per Year' in text or 'per year' in text:
                        job_data['salary_per'] = 'year'
                elif any(location in text for location in ['United States', 'France', 'Italy', 'Spain', 'Greece', 'Monaco', 'Germany', 'United Kingdom']):
                    job_data['location'] = text
            
            # Extract "View Job" link for source URL
            view_job_link = job_element.find('a', text=re.compile(r'View Job', re.I))
            if view_job_link:
                job_data['source_url'] = urljoin(self.base_url, view_job_link.get('href', ''))
            
            # Set posted date to now if we don't have it
            job_data['posted_at'] = datetime.now()
            
            # Generate external_id if not found
            if 'external_id' not in job_data:
                job_data['external_id'] = f"yotspot_{hash(job_data.get('title', '') + str(job_data['posted_at']))}"
            
            return job_data
            
        except Exception as e:
            logger.error(f"Error parsing job card: {e}")
            return {}
    
    def _extract_job_details(self, job_url: str) -> Dict[str, Any]:
        """Extract detailed job information from job detail page"""
        try:
            self._respectful_delay()
            response = self.session.get(job_url)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            details = {}
            
            # Extract job description
            description_elem = soup.find('div', class_=re.compile(r'description|content|detail'))
            if description_elem:
                details['description'] = description_elem.get_text(strip=True)
            
            # Extract company name
            company_elem = soup.find('div', class_=re.compile(r'company|employer'))
            if company_elem:
                details['company'] = company_elem.get_text(strip=True)
            
            return details
            
        except Exception as e:
            logger.error(f"Error extracting job details from {job_url}: {e}")
            return {}
    
    async def scrape_jobs(self, max_pages: int = 5) -> List[Dict[str, Any]]:
        """Scrape jobs from Yotspot"""
        all_jobs = []
        
        try:
            logger.info("Starting Yotspot scraping...")
            
            for page in range(1, max_pages + 1):
                logger.info(f"Scraping page {page}")
                
                # Add delay between pages
                if page > 1:
                    self._respectful_delay(3.0, 6.0)
                
                # Get page
                params = {'page': page} if page > 1 else {}
                response = self.session.get(self.search_url, params=params)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Find job listings - these might be in various containers
                job_elements = []
                
                # Try different selectors that might contain job listings
                selectors = [
                    'div[class*="job"]',
                    'div[class*="position"]',
                    'div[class*="listing"]',
                    'article',
                    'li[class*="job"]',
                    '.job-card',
                    '.position-card'
                ]
                
                for selector in selectors:
                    elements = soup.select(selector)
                    if elements:
                        job_elements = elements
                        break
                
                # If no specific job containers found, look for elements with "View Job" links
                if not job_elements:
                    view_job_links = soup.find_all('a', text=re.compile(r'View Job', re.I))
                    job_elements = [link.find_parent() for link in view_job_links if link.find_parent()]
                
                logger.info(f"Found {len(job_elements)} job elements on page {page}")
                
                if not job_elements:
                    logger.warning(f"No job elements found on page {page}, stopping")
                    break
                
                # Parse each job
                page_jobs = []
                for job_elem in job_elements:
                    job_data = self._parse_job_card(job_elem)
                    if job_data and job_data.get('title'):
                        # Get additional details if we have a source URL
                        if job_data.get('source_url'):
                            additional_details = self._extract_job_details(job_data['source_url'])
                            job_data.update(additional_details)
                        
                        page_jobs.append(job_data)
                
                all_jobs.extend(page_jobs)
                logger.info(f"Scraped {len(page_jobs)} jobs from page {page}")
                
                # If no jobs found on this page, stop
                if not page_jobs:
                    break
            
            logger.info(f"Scraping completed. Total jobs found: {len(all_jobs)}")
            return all_jobs
            
        except Exception as e:
            logger.error(f"Error during scraping: {e}")
            return all_jobs
    
    def scrape_job_details(self, job_id: str) -> Dict[str, Any]:
        """Scrape detailed information for a specific job"""
        try:
            job_url = f"{self.base_url}/job-detail/{job_id}"
            return self._extract_job_details(job_url)
        except Exception as e:
            logger.error(f"Error scraping job details for {job_id}: {e}")
            return {}
    
    def test_scraping(self) -> Dict[str, Any]:
        """Test the scraper with a single page"""
        try:
            response = self.session.get(self.search_url)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            return {
                "status": "success",
                "url": self.search_url,
                "title": soup.title.get_text() if soup.title else "No title",
                "total_links": len(soup.find_all('a')),
                "view_job_links": len(soup.find_all('a', text=re.compile(r'View Job', re.I)))
            }
        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            } 
</code>

app/__init__.py:
<code>
# YotCrew.app Package 
</code>

app/cli.py:
<code>
"""
Command-line interface for scheduler management.

This module provides a convenient way to manually control the scheduler,
check status, run jobs immediately, and update schedules.
"""
import argparse
import asyncio
import json
import sys
from typing import List

from .services.scheduler_service import SchedulerService
from .config import SchedulerConfig


def parse_time_list(time_str: str) -> List[int]:
    """
    Parse a comma-separated string of integers.
    
    Args:
        time_str: Comma-separated string (e.g., "6,7,8,9")
        
    Returns:
        List of integers
    """
    try:
        return [int(x.strip()) for x in time_str.split(',')]
    except ValueError as e:
        raise argparse.ArgumentTypeError(f"Invalid time format: {e}")


async def cmd_status(args):
    """Check scheduler status."""
    service = SchedulerService()
    
    try:
        status = service.get_scheduler_status()
        
        if args.json:
            print(json.dumps(status, indent=2, default=str))
        else:
            print("=== Scheduler Status ===")
            print(f"Service Running: {status.get('service_running', False)}")
            print(f"Scheduler Running: {status.get('running', False)}")
            print(f"Total Jobs: {status.get('total_jobs', 0)}")
            print(f"Daywork123 Jobs: {status.get('daywork123_jobs', 0)}")
            
            config = status.get('config', {})
            print(f"Total Daily Runs: {config.get('total_daily_runs', 0)}")
            print(f"Max Pages per Run: {config.get('max_pages', 0)}")
            
            if 'error' in status:
                print(f"Error: {status['error']}")
                
    except Exception as e:
        error_msg = {"error": str(e)}
        if args.json:
            print(json.dumps(error_msg, indent=2))
        else:
            print(f"Error getting status: {e}")
        sys.exit(1)


async def cmd_run_now(args):
    """Run the Daywork123 scraper immediately."""
    service = SchedulerService()
    
    try:
        result = await service.run_daywork123_now(period=args.period)
        
        if args.json:
            print(json.dumps(result, indent=2, default=str))
        else:
            if result.get('success'):
                print("=== Scraping Completed Successfully ===")
                print(f"Period: {result.get('period', 'unknown')}")
                print(f"Jobs Found: {result.get('jobs_found', 0)}")
                print(f"Duration: {result.get('duration_seconds', 0):.2f} seconds")
                print(f"Max Pages: {result.get('max_pages', 0)}")
            else:
                print("=== Scraping Failed ===")
                print(f"Error: {result.get('error', 'Unknown error')}")
                print(f"Period: {result.get('period', 'unknown')}")
                
    except Exception as e:
        error_msg = {"error": str(e)}
        if args.json:
            print(json.dumps(error_msg, indent=2))
        else:
            print(f"Error running scraper: {e}")
        sys.exit(1)


async def cmd_update_schedule(args):
    """Update the complete scraping schedule."""
    service = SchedulerService()
    
    try:
        # Parse the time arguments
        kwargs = {}
        if args.morning_hours:
            kwargs['morning_hours'] = parse_time_list(args.morning_hours)
        if args.morning_minutes:
            kwargs['morning_minutes'] = parse_time_list(args.morning_minutes)
        if args.day_hours:
            kwargs['day_hours'] = parse_time_list(args.day_hours)
        if args.day_minutes:
            kwargs['day_minutes'] = parse_time_list(args.day_minutes)
        if args.evening_hours:
            kwargs['evening_hours'] = parse_time_list(args.evening_hours)
        if args.evening_minutes:
            kwargs['evening_minutes'] = parse_time_list(args.evening_minutes)
        
        result = await service.update_daywork123_schedule(**kwargs)
        
        if args.json:
            print(json.dumps(result, indent=2, default=str))
        else:
            if result.get('success'):
                print("=== Schedule Updated Successfully ===")
                schedule = result.get('new_schedule', {})
                print(f"Morning: {schedule.get('morning_hours')} at {schedule.get('morning_minutes')}")
                print(f"Day: {schedule.get('day_hours')} at {schedule.get('day_minutes')}")
                print(f"Evening: {schedule.get('evening_hours')} at {schedule.get('evening_minutes')}")
                print(f"Total Daily Runs: {result.get('total_daily_runs', 0)}")
            else:
                print("=== Schedule Update Failed ===")
                print(f"Error: {result.get('error', 'Unknown error')}")
                
    except Exception as e:
        error_msg = {"error": str(e)}
        if args.json:
            print(json.dumps(error_msg, indent=2))
        else:
            print(f"Error updating schedule: {e}")
        sys.exit(1)


async def cmd_update_morning(args):
    """Update the morning scraping schedule."""
    service = SchedulerService()
    
    try:
        hours = parse_time_list(args.hours)
        minutes = parse_time_list(args.minutes)
        
        result = await service.update_morning_schedule(hours, minutes)
        
        if args.json:
            print(json.dumps(result, indent=2, default=str))
        else:
            if result.get('success'):
                print("=== Morning Schedule Updated Successfully ===")
                schedule = result.get('new_schedule', {})
                print(f"Morning Hours: {schedule.get('morning_hours')}")
                print(f"Morning Minutes: {schedule.get('morning_minutes')}")
                print(f"Total Daily Runs: {result.get('total_daily_runs', 0)}")
            else:
                print("=== Morning Schedule Update Failed ===")
                print(f"Error: {result.get('error', 'Unknown error')}")
                
    except Exception as e:
        error_msg = {"error": str(e)}
        if args.json:
            print(json.dumps(error_msg, indent=2))
        else:
            print(f"Error updating morning schedule: {e}")
        sys.exit(1)


async def cmd_update_day(args):
    """Update the daytime scraping schedule."""
    service = SchedulerService()
    
    try:
        hours = parse_time_list(args.hours)
        minutes = parse_time_list(args.minutes)
        
        result = await service.update_day_schedule(hours, minutes)
        
        if args.json:
            print(json.dumps(result, indent=2, default=str))
        else:
            if result.get('success'):
                print("=== Day Schedule Updated Successfully ===")
                schedule = result.get('new_schedule', {})
                print(f"Day Hours: {schedule.get('day_hours')}")
                print(f"Day Minutes: {schedule.get('day_minutes')}")
                print(f"Total Daily Runs: {result.get('total_daily_runs', 0)}")
            else:
                print("=== Day Schedule Update Failed ===")
                print(f"Error: {result.get('error', 'Unknown error')}")
                
    except Exception as e:
        error_msg = {"error": str(e)}
        if args.json:
            print(json.dumps(error_msg, indent=2))
        else:
            print(f"Error updating day schedule: {e}")
        sys.exit(1)


async def cmd_update_evening(args):
    """Update the evening scraping schedule."""
    service = SchedulerService()
    
    try:
        hours = parse_time_list(args.hours)
        minutes = parse_time_list(args.minutes)
        
        result = await service.update_evening_schedule(hours, minutes)
        
        if args.json:
            print(json.dumps(result, indent=2, default=str))
        else:
            if result.get('success'):
                print("=== Evening Schedule Updated Successfully ===")
                schedule = result.get('new_schedule', {})
                print(f"Evening Hours: {schedule.get('evening_hours')}")
                print(f"Evening Minutes: {schedule.get('evening_minutes')}")
                print(f"Total Daily Runs: {result.get('total_daily_runs', 0)}")
            else:
                print("=== Evening Schedule Update Failed ===")
                print(f"Error: {result.get('error', 'Unknown error')}")
                
    except Exception as e:
        error_msg = {"error": str(e)}
        if args.json:
            print(json.dumps(error_msg, indent=2))
        else:
            print(f"Error updating evening schedule: {e}")
        sys.exit(1)


async def cmd_list_jobs(args):
    """List all scheduled jobs."""
    service = SchedulerService()
    
    try:
        jobs = service.get_jobs_status()
        
        if args.json:
            print(json.dumps(jobs, indent=2, default=str))
        else:
            print("=== Scheduled Jobs ===")
            if not jobs:
                print("No jobs scheduled")
            else:
                for job in jobs:
                    print(f"ID: {job.get('id')}")
                    print(f"  Name: {job.get('name')}")
                    print(f"  Next Run: {job.get('next_run_time')}")
                    print(f"  Trigger: {job.get('trigger')}")
                    kwargs = job.get('kwargs', {})
                    if kwargs:
                        print(f"  Period: {kwargs.get('period', 'unknown')}")
                    print()
                    
    except Exception as e:
        error_msg = {"error": str(e)}
        if args.json:
            print(json.dumps(error_msg, indent=2))
        else:
            print(f"Error listing jobs: {e}")
        sys.exit(1)


async def cmd_next_runs(args):
    """Show next scheduled runs."""
    service = SchedulerService()
    
    try:
        next_runs = service.get_next_runs(limit=args.limit)
        
        if args.json:
            print(json.dumps(next_runs, indent=2, default=str))
        else:
            print(f"=== Next {args.limit} Scheduled Runs ===")
            if not next_runs:
                print("No upcoming runs scheduled")
            else:
                for run in next_runs:
                    print(f"{run.get('next_run_time')} - {run.get('job_name')} ({run.get('period')})")
                    
    except Exception as e:
        error_msg = {"error": str(e)}
        if args.json:
            print(json.dumps(error_msg, indent=2))
        else:
            print(f"Error getting next runs: {e}")
        sys.exit(1)


def create_parser():
    """Create the argument parser."""
    parser = argparse.ArgumentParser(
        description="Scheduler CLI for Daywork123 scraper automation",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Check scheduler status
  python -m app.cli status

  # Run scraper immediately
  python -m app.cli run-now

  # Update complete schedule
  python -m app.cli update-schedule --morning-hours "6,7,8,9" --morning-minutes "0,30"

  # Update only morning schedule
  python -m app.cli update-morning --hours "6,7,8,9,10" --minutes "0,30"

  # List all jobs
  python -m app.cli list-jobs

  # Show next 5 runs
  python -m app.cli next-runs --limit 5

Time formats:
  Hours: 0-23 (comma-separated, e.g., "6,7,8,9")
  Minutes: 0-59 (comma-separated, e.g., "0,30")
        """
    )
    
    parser.add_argument(
        '--json',
        action='store_true',
        help='Output results in JSON format'
    )
    
    subparsers = parser.add_subparsers(dest='command', help='Available commands')
    
    # Status command
    status_parser = subparsers.add_parser('status', help='Check scheduler status')
    status_parser.set_defaults(func=cmd_status)
    
    # Run now command
    run_parser = subparsers.add_parser('run-now', help='Run scraper immediately')
    run_parser.add_argument(
        '--period',
        default='manual',
        help='Period identifier for logging (default: manual)'
    )
    run_parser.set_defaults(func=cmd_run_now)
    
    # Update complete schedule command
    update_parser = subparsers.add_parser('update-schedule', help='Update complete scraping schedule')
    update_parser.add_argument('--morning-hours', help='Morning hours (e.g., "6,7,8,9")')
    update_parser.add_argument('--morning-minutes', help='Morning minutes (e.g., "0,30")')
    update_parser.add_argument('--day-hours', help='Day hours (e.g., "12,15")')
    update_parser.add_argument('--day-minutes', help='Day minutes (e.g., "0")')
    update_parser.add_argument('--evening-hours', help='Evening hours (e.g., "18,19,20,21")')
    update_parser.add_argument('--evening-minutes', help='Evening minutes (e.g., "0,30")')
    update_parser.set_defaults(func=cmd_update_schedule)
    
    # Update morning schedule command
    morning_parser = subparsers.add_parser('update-morning', help='Update morning scraping schedule')
    morning_parser.add_argument('--hours', required=True, help='Morning hours (e.g., "6,7,8,9")')
    morning_parser.add_argument('--minutes', required=True, help='Morning minutes (e.g., "0,30")')
    morning_parser.set_defaults(func=cmd_update_morning)
    
    # Update day schedule command
    day_parser = subparsers.add_parser('update-day', help='Update daytime scraping schedule')
    day_parser.add_argument('--hours', required=True, help='Day hours (e.g., "12,15")')
    day_parser.add_argument('--minutes', required=True, help='Day minutes (e.g., "0")')
    day_parser.set_defaults(func=cmd_update_day)
    
    # Update evening schedule command
    evening_parser = subparsers.add_parser('update-evening', help='Update evening scraping schedule')
    evening_parser.add_argument('--hours', required=True, help='Evening hours (e.g., "18,19,20,21")')
    evening_parser.add_argument('--minutes', required=True, help='Evening minutes (e.g., "0,30")')
    evening_parser.set_defaults(func=cmd_update_evening)
    
    # List jobs command
    jobs_parser = subparsers.add_parser('list-jobs', help='List all scheduled jobs')
    jobs_parser.set_defaults(func=cmd_list_jobs)
    
    # Next runs command
    next_parser = subparsers.add_parser('next-runs', help='Show next scheduled runs')
    next_parser.add_argument(
        '--limit',
        type=int,
        default=10,
        help='Number of next runs to show (default: 10)'
    )
    next_parser.set_defaults(func=cmd_next_runs)
    
    return parser


async def main():
    """Main CLI entry point."""
    parser = create_parser()
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        sys.exit(1)
    
    try:
        await args.func(args)
    except KeyboardInterrupt:
        print("\nOperation cancelled by user")
        sys.exit(1)
    except Exception as e:
        error_msg = {"error": str(e)}
        if args.json:
            print(json.dumps(error_msg, indent=2))
        else:
            print(f"Unexpected error: {e}")
        sys.exit(1)


if __name__ == '__main__':
    asyncio.run(main())

</code>

app/daywork_scheduler.py:
<code>
"""
Advanced scheduler for Daywork123 scraper with time-based intervals.

This module implements a comprehensive scheduling system using APScheduler that
runs the Daywork123 scraper at different frequencies throughout the day.
"""
import logging
import asyncio
from datetime import datetime
from typing import Dict, List, Optional

from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.jobstores.sqlalchemy import SQLAlchemyJobStore
from apscheduler.executors.asyncio import AsyncIOExecutor
from apscheduler.events import EVENT_JOB_EXECUTED, EVENT_JOB_ERROR, EVENT_JOB_MISSED
from apscheduler.triggers.cron import CronTrigger

from .config import SchedulerConfig
from .scrapers.registry import ScraperRegistry
from .database import SessionLocal
from .models import ScrapingJob


async def run_daywork123_scraping_job(period: str, hour: int, minute: int, max_pages: int = 5):
    """
    Standalone function to execute the Daywork123 scraping task.
    
    This function is separate from the scheduler class to avoid serialization issues.
    
    Args:
        period: Time period ('morning', 'day', 'evening')
        hour: Hour when the job was scheduled
        minute: Minute when the job was scheduled
        max_pages: Maximum pages to scrape
    """
    start_time = datetime.now()
    db = SessionLocal()
    scraping_job = None
    
    try:
        logger.info(f"Starting Daywork123 scraping - {period} ({hour:02d}:{minute:02d})")
        
        # Create scraping job record
        scraping_job = ScrapingJob(
            status="started",
            started_at=start_time,
            scraper_type=f"daywork123_{period}",
            job_id=f"daywork123_{period}_{hour:02d}_{minute:02d}"
        )
        db.add(scraping_job)
        db.commit()
        
        # Get the Daywork123 scraper from registry
        scraper_registry = ScraperRegistry()
        scraper = scraper_registry.get_scraper('daywork123')
        
        if not scraper:
            raise ValueError("Daywork123 scraper not found in registry")
        
        # Run the scraper
        jobs_found = await scraper.scrape_and_save_jobs(max_pages=max_pages)
        
        # Update scraping job with results
        scraping_job.status = "completed"
        scraping_job.completed_at = datetime.now()
        scraping_job.jobs_found = len(jobs_found) if jobs_found else 0
        scraping_job.new_jobs = len(jobs_found) if jobs_found else 0  # This would need proper new job detection
        db.commit()
        
        duration = (datetime.now() - start_time).total_seconds()
        logger.info(
            f"Daywork123 scraping completed - {period} "
            f"({hour:02d}:{minute:02d}) - "
            f"Found {len(jobs_found) if jobs_found else 0} jobs in {duration:.2f}s"
        )
        
    except Exception as e:
        logger.error(f"Error in Daywork123 scraping - {period} ({hour:02d}:{minute:02d}): {e}")
        
        # Update scraping job with error
        if scraping_job:
            try:
                scraping_job.status = "failed"
                scraping_job.completed_at = datetime.now()
                scraping_job.error_message = str(e)
                db.commit()
            except Exception as db_error:
                logger.error(f"Error updating scraping job status: {db_error}")
                
    finally:
        db.close()


logger = logging.getLogger(__name__)


class ScrapingScheduler:
    """
    Advanced scheduler for automating scraping tasks with time-based intervals.
    
    This scheduler creates multiple cron jobs for different periods of the day:
    - Morning: High frequency scraping during morning hours
    - Day: Lower frequency scraping during daytime hours  
    - Evening: High frequency scraping during evening hours
    """
    
    def __init__(self, config: SchedulerConfig = None):
        """
        Initialize the scraping scheduler.
        
        Args:
            config: Configuration object, defaults to SchedulerConfig
        """
        self.config = config or SchedulerConfig()
        self.scheduler = None
        self._setup_scheduler()
        
    def _setup_scheduler(self):
        """Setup the APScheduler with appropriate configuration."""
        # Configure job store for persistence
        jobstores = {
            'default': SQLAlchemyJobStore(
                url=self.config.DB_URL,
                tablename=self.config.JOBSTORE_TABLE_NAME
            )
        }
        
        # Configure executor for async execution
        executors = {
            'default': AsyncIOExecutor()
        }
        
        # Configure job defaults
        job_defaults = {
            'coalesce': self.config.COALESCE,
            'max_instances': self.config.MAX_INSTANCES,
            'misfire_grace_time': self.config.MISFIRE_GRACE_TIME
        }
        
        # Create the scheduler
        self.scheduler = AsyncIOScheduler(
            jobstores=jobstores,
            executors=executors,
            job_defaults=job_defaults,
            timezone='UTC'
        )
        
        # Add event listeners
        self.scheduler.add_listener(self._job_executed_listener, EVENT_JOB_EXECUTED)
        self.scheduler.add_listener(self._job_error_listener, EVENT_JOB_ERROR)
        self.scheduler.add_listener(self._job_missed_listener, EVENT_JOB_MISSED)
        
    def _job_executed_listener(self, event):
        """Handle job execution events."""
        logger.info(f"Job {event.job_id} executed successfully")
        
    def _job_error_listener(self, event):
        """Handle job error events."""
        logger.error(f"Job {event.job_id} failed: {event.exception}")
        
    def _job_missed_listener(self, event):
        """Handle missed job events."""
        logger.warning(f"Job {event.job_id} was missed")
        
    async def start(self):
        """Start the scheduler and schedule default jobs."""
        try:
            # Validate configuration
            if not self.config.validate_config():
                raise ValueError("Invalid scheduler configuration")
            
            # Start the scheduler
            self.scheduler.start()
            logger.info("Scheduler started successfully")
            
            # Schedule the default Daywork123 scraper jobs
            await self.schedule_daywork123_scraper()
            
            # Print configuration summary
            self.config.print_schedule_summary()
            
        except Exception as e:
            logger.error(f"Error starting scheduler: {e}")
            raise
            
    async def stop(self):
        """Stop the scheduler gracefully."""
        try:
            if self.scheduler and self.scheduler.running:
                self.scheduler.shutdown(wait=True)
                logger.info("Scheduler stopped successfully")
        except Exception as e:
            logger.error(f"Error stopping scheduler: {e}")
            
    async def schedule_daywork123_scraper(self):
        """
        Schedule Daywork123 scraper jobs with time-based intervals.
        
        Creates three separate cron jobs:
        - Morning: High frequency during morning hours
        - Day: Lower frequency during daytime hours
        - Evening: High frequency during evening hours
        """
        try:
            # Remove existing Daywork123 jobs
            await self.remove_daywork123_jobs()
            
            # Schedule morning jobs
            for hour in self.config.MORNING_HOURS:
                for minute in self.config.MORNING_MINUTES:
                    job_id = f'daywork123_morning_{hour:02d}_{minute:02d}'
                    self.scheduler.add_job(
                        func=run_daywork123_scraping_job,
                        trigger=CronTrigger(hour=hour, minute=minute),
                        id=job_id,
                        name=f'Daywork123 Morning Scraping ({hour:02d}:{minute:02d})',
                        kwargs={
                            'period': 'morning', 
                            'hour': hour, 
                            'minute': minute,
                            'max_pages': self.config.DAYWORK123_MAX_PAGES
                        },
                        replace_existing=True
                    )
            
            # Schedule day jobs
            for hour in self.config.DAY_HOURS:
                for minute in self.config.DAY_MINUTES:
                    job_id = f'daywork123_day_{hour:02d}_{minute:02d}'
                    self.scheduler.add_job(
                        func=run_daywork123_scraping_job,
                        trigger=CronTrigger(hour=hour, minute=minute),
                        id=job_id,
                        name=f'Daywork123 Day Scraping ({hour:02d}:{minute:02d})',
                        kwargs={
                            'period': 'day', 
                            'hour': hour, 
                            'minute': minute,
                            'max_pages': self.config.DAYWORK123_MAX_PAGES
                        },
                        replace_existing=True
                    )
            
            # Schedule evening jobs
            for hour in self.config.EVENING_HOURS:
                for minute in self.config.EVENING_MINUTES:
                    job_id = f'daywork123_evening_{hour:02d}_{minute:02d}'
                    self.scheduler.add_job(
                        func=run_daywork123_scraping_job,
                        trigger=CronTrigger(hour=hour, minute=minute),
                        id=job_id,
                        name=f'Daywork123 Evening Scraping ({hour:02d}:{minute:02d})',
                        kwargs={
                            'period': 'evening', 
                            'hour': hour, 
                            'minute': minute,
                            'max_pages': self.config.DAYWORK123_MAX_PAGES
                        },
                        replace_existing=True
                    )
            
            total_jobs = self.config.get_total_daily_runs()
            logger.info(f"Scheduled {total_jobs} Daywork123 scraping jobs per day")
            
        except Exception as e:
            logger.error(f"Error scheduling Daywork123 scraper: {e}")
            raise
            

            
    async def remove_daywork123_jobs(self):
        """Remove all Daywork123 scraper jobs."""
        try:
            jobs_to_remove = []
            for job in self.scheduler.get_jobs():
                if job.id.startswith('daywork123_'):
                    jobs_to_remove.append(job.id)
            
            for job_id in jobs_to_remove:
                self.scheduler.remove_job(job_id)
                
            logger.info(f"Removed {len(jobs_to_remove)} existing Daywork123 jobs")
            
        except Exception as e:
            logger.error(f"Error removing Daywork123 jobs: {e}")
            
    def get_job_status(self, job_id: str) -> Optional[Dict]:
        """
        Get status information for a specific job.
        
        Args:
            job_id: ID of the job
            
        Returns:
            Dictionary with job information or None if not found
        """
        try:
            job = self.scheduler.get_job(job_id)
            if job:
                return {
                    'id': job.id,
                    'name': job.name,
                    'next_run_time': job.next_run_time,
                    'trigger': str(job.trigger),
                    'kwargs': job.kwargs
                }
        except Exception as e:
            logger.error(f"Error getting job status for {job_id}: {e}")
        return None
        
    def get_all_jobs_status(self) -> List[Dict]:
        """
        Get status information for all scheduled jobs.
        
        Returns:
            List of dictionaries with job information
        """
        jobs_status = []
        try:
            for job in self.scheduler.get_jobs():
                if job.id.startswith('daywork123_'):
                    jobs_status.append({
                        'id': job.id,
                        'name': job.name,
                        'next_run_time': job.next_run_time,
                        'trigger': str(job.trigger),
                        'kwargs': job.kwargs
                    })
        except Exception as e:
            logger.error(f"Error getting jobs status: {e}")
        return jobs_status
        
    def pause_job(self, job_id: str) -> bool:
        """
        Pause a specific job.
        
        Args:
            job_id: ID of the job to pause
            
        Returns:
            True if successful, False otherwise
        """
        try:
            self.scheduler.pause_job(job_id)
            logger.info(f"Job {job_id} paused")
            return True
        except Exception as e:
            logger.error(f"Error pausing job {job_id}: {e}")
            return False
            
    def resume_job(self, job_id: str) -> bool:
        """
        Resume a specific job.
        
        Args:
            job_id: ID of the job to resume
            
        Returns:
            True if successful, False otherwise
        """
        try:
            self.scheduler.resume_job(job_id)
            logger.info(f"Job {job_id} resumed")
            return True
        except Exception as e:
            logger.error(f"Error resuming job {job_id}: {e}")
            return False
            
    def remove_job(self, job_id: str) -> bool:
        """
        Remove a specific job.
        
        Args:
            job_id: ID of the job to remove
            
        Returns:
            True if successful, False otherwise
        """
        try:
            self.scheduler.remove_job(job_id)
            logger.info(f"Job {job_id} removed")
            return True
        except Exception as e:
            logger.error(f"Error removing job {job_id}: {e}")
            return False
            
    def is_running(self) -> bool:
        """Check if the scheduler is running."""
        return self.scheduler and self.scheduler.running
        
    def get_scheduler_status(self) -> Dict:
        """
        Get overall scheduler status.
        
        Returns:
            Dictionary with scheduler status information
        """
        try:
            return {
                'running': self.is_running(),
                'total_jobs': len(self.scheduler.get_jobs()) if self.scheduler else 0,
                'daywork123_jobs': len([j for j in self.scheduler.get_jobs() if j.id.startswith('daywork123_')]) if self.scheduler else 0,
                'config': {
                    'morning_hours': self.config.MORNING_HOURS,
                    'morning_minutes': self.config.MORNING_MINUTES,
                    'day_hours': self.config.DAY_HOURS,
                    'day_minutes': self.config.DAY_MINUTES,
                    'evening_hours': self.config.EVENING_HOURS,
                    'evening_minutes': self.config.EVENING_MINUTES,
                    'max_pages': self.config.DAYWORK123_MAX_PAGES,
                    'total_daily_runs': self.config.get_total_daily_runs()
                }
            }
        except Exception as e:
            logger.error(f"Error getting scheduler status: {e}")
            return {'running': False, 'error': str(e)}

</code>

app/models.py:
<code>
from sqlalchemy import Column, Integer, String, DateTime, Text, Boolean, Float, func, JSON
from datetime import datetime
from .database import Base
import uuid

class Job(Base):
    __tablename__ = "jobs"
    
    id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    external_id = Column(String, unique=True, index=True)  # ID from source website
    title = Column(String, nullable=False)
    company = Column(String)
    location = Column(String)
    
    # Vessel information
    vessel_type = Column(String)  # Motor Yacht, Sailing Yacht, etc.
    vessel_size = Column(String)  # 40m+, 50-74m, etc.
    vessel_name = Column(String)
    
    # Employment details
    job_type = Column(String)     # Permanent, Temporary, Rotational  
    employment_type = Column(String)  # For compatibility with UniversalJob
    department = Column(String)   # Deck, Interior, Engineering, etc.
    position_level = Column(String)
    
    # Compensation
    salary_range = Column(String)
    salary_currency = Column(String)
    salary_per = Column(String)   # per day, per month, per year
    salary_period = Column(String)  # For compatibility with UniversalJob
    
    # Timing
    start_date = Column(String)
    posted_at = Column(DateTime)
    posted_date = Column(DateTime)  # For compatibility with UniversalJob
    
    # Content
    description = Column(Text)
    requirements = Column(JSON)  # Store as JSON array
    benefits = Column(JSON)      # Store as JSON array
    
    # Location details
    country = Column(String)
    region = Column(String)
    
    # Metadata
    source_url = Column(String)
    source = Column(String, default="yotspot")
    is_featured = Column(Boolean, default=False)
    quality_score = Column(Float, default=0.0)
    raw_data = Column(JSON)  # Store raw scraping data
    scraped_at = Column(DateTime, default=datetime.utcnow)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    def to_dict(self):
        return {
            "id": self.id,
            "external_id": self.external_id,
            "title": self.title,
            "company": self.company,
            "location": self.location,
            "country": self.country,
            "region": self.region,
            "vessel_type": self.vessel_type,
            "vessel_size": self.vessel_size,
            "vessel_name": self.vessel_name,
            "job_type": self.job_type,
            "employment_type": self.employment_type,
            "department": self.department,
            "position_level": self.position_level,
            "salary_range": self.salary_range,
            "salary_currency": self.salary_currency,
            "salary_per": self.salary_per,
            "salary_period": self.salary_period,
            "start_date": self.start_date,
            "description": self.description,
            "requirements": self.requirements,
            "benefits": self.benefits,
            "posted_at": self.posted_at.isoformat() if self.posted_at else None,
            "posted_date": self.posted_date.isoformat() if self.posted_date else None,
            "source_url": self.source_url,
            "source": self.source,
            "is_featured": self.is_featured,
            "quality_score": self.quality_score,
            "scraped_at": self.scraped_at.isoformat() if self.scraped_at else None,
            "created_at": self.created_at.isoformat(),
            "updated_at": self.updated_at.isoformat()
        }

class ScrapingJob(Base):
    __tablename__ = "scraping_jobs"
    
    id = Column(Integer, primary_key=True, index=True)
    status = Column(String, default="pending")  # pending, started, completed, failed
    started_at = Column(DateTime)
    completed_at = Column(DateTime)
    jobs_found = Column(Integer, default=0)
    new_jobs = Column(Integer, default=0)
    error_message = Column(Text)
    scraper_type = Column(String, default="yotspot")
    
    def to_dict(self):
        return {
            "id": self.id,
            "status": self.status,
            "started_at": self.started_at.isoformat() if self.started_at else None,
            "completed_at": self.completed_at.isoformat() if self.completed_at else None,
            "jobs_found": self.jobs_found,
            "new_jobs": self.new_jobs,
            "error_message": self.error_message,
            "scraper_type": self.scraper_type
        } 
</code>

app/scheduler.py:
<code>
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.interval import IntervalTrigger
import logging
from datetime import datetime
from .database import SessionLocal
from .models import Job, ScrapingJob
from .scraper import YotspotScraper

logger = logging.getLogger(__name__)

scheduler = BackgroundScheduler()
scraper = YotspotScraper()

def scheduled_scrape_job():
    """Scheduled function to scrape jobs"""
    db = SessionLocal()
    try:
        logger.info("Starting scheduled job scraping...")
        
        # Create scraping job record
        scraping_job = ScrapingJob(
            status="started",
            started_at=datetime.now(),
            scraper_type="yotspot_scheduled"
        )
        db.add(scraping_job)
        db.commit()
        
        # Run scraper
        jobs_found = []
        try:
            # Note: We need to make this async-compatible or run in sync mode
            import asyncio
            try:
                loop = asyncio.get_event_loop()
            except RuntimeError:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
            
            jobs_found = loop.run_until_complete(scraper.scrape_jobs(max_pages=3))
        except Exception as e:
            logger.error(f"Error in scheduled scraping: {e}")
            jobs_found = []
        
        # Save jobs to database
        new_jobs = 0
        for job_data in jobs_found:
            try:
                existing_job = db.query(Job).filter(Job.external_id == job_data["external_id"]).first()
                if not existing_job:
                    job = Job(**job_data)
                    db.add(job)
                    new_jobs += 1
            except Exception as e:
                logger.error(f"Error saving job: {e}")
                continue
        
        db.commit()
        
        # Update scraping job
        scraping_job.status = "completed"
        scraping_job.completed_at = datetime.now()
        scraping_job.jobs_found = len(jobs_found)
        scraping_job.new_jobs = new_jobs
        db.commit()
        
        logger.info(f"Scheduled scraping completed. Found {len(jobs_found)} jobs, {new_jobs} new")
        
    except Exception as e:
        logger.error(f"Error in scheduled scrape: {e}")
        # Update scraping job with error
        try:
            scraping_job = db.query(ScrapingJob).filter(ScrapingJob.id == scraping_job.id).first()
            if scraping_job:
                scraping_job.status = "failed"
                scraping_job.completed_at = datetime.now()
                scraping_job.error_message = str(e)
                db.commit()
        except:
            pass
    finally:
        db.close()

def start_scheduler():
    """Start the background scheduler"""
    try:
        # Schedule job scraping every 45 minutes
        scheduler.add_job(
            func=scheduled_scrape_job,
            trigger=IntervalTrigger(minutes=45),
            id='scrape_jobs',
                            name='Scrape jobs for YotCrew.app',
            replace_existing=True
        )
        
        scheduler.start()
        logger.info("Scheduler started - jobs will be scraped every 45 minutes")
        
    except Exception as e:
        logger.error(f"Error starting scheduler: {e}")

def stop_scheduler():
    """Stop the background scheduler"""
    try:
        scheduler.shutdown()
        logger.info("Scheduler stopped")
    except Exception as e:
        logger.error(f"Error stopping scheduler: {e}") 
</code>

app/config.py:
<code>
"""
Configuration module for the scheduler functionality.

This module defines all configuration parameters for the APScheduler integration,
including time-based intervals for different periods of the day.
"""
import os
from typing import List


class SchedulerConfig:
    """Configuration class for scheduler settings with time-based intervals."""
    
    # Database Configuration
    DB_URL: str = os.getenv('SCHEDULER_DB_URL', 'sqlite:///./yachtjobs.db')
    
    # Logging Configuration
    LOG_LEVEL: str = os.getenv('LOG_LEVEL', 'INFO')
    
    # Daywork123 Scraper Configuration
    DAYWORK123_MAX_PAGES: int = int(os.getenv('DAYWORK123_MAX_PAGES', '5'))
    
    # Time-based interval definitions
    # Morning hours: High activity period (6 AM - 9 AM)
    MORNING_HOURS: List[int] = [
        int(h) for h in os.getenv('DAYWORK123_MORNING_HOURS', '6,7,8,9').split(',')
    ]
    MORNING_MINUTES: List[int] = [
        int(m) for m in os.getenv('DAYWORK123_MORNING_MINUTES', '0,30').split(',')
    ]
    
    # Day hours: Lower activity period (12 PM - 3 PM)
    DAY_HOURS: List[int] = [
        int(h) for h in os.getenv('DAYWORK123_DAY_HOURS', '12,15').split(',')
    ]
    DAY_MINUTES: List[int] = [
        int(m) for m in os.getenv('DAYWORK123_DAY_MINUTES', '0').split(',')
    ]
    
    # Evening hours: High activity period (6 PM - 9 PM)
    EVENING_HOURS: List[int] = [
        int(h) for h in os.getenv('DAYWORK123_EVENING_HOURS', '18,19,20,21').split(',')
    ]
    EVENING_MINUTES: List[int] = [
        int(m) for m in os.getenv('DAYWORK123_EVENING_MINUTES', '0,30').split(',')
    ]
    
    # Scheduler Job Configuration
    COALESCE: bool = os.getenv('SCHEDULER_COALESCE', 'true').lower() == 'true'
    MAX_INSTANCES: int = int(os.getenv('SCHEDULER_MAX_INSTANCES', '1'))
    MISFIRE_GRACE_TIME: int = int(os.getenv('SCHEDULER_MISFIRE_GRACE_TIME', '300'))  # 5 minutes
    
    # Job Store Configuration
    JOBSTORE_TABLE_NAME: str = os.getenv('SCHEDULER_JOBSTORE_TABLE', 'apscheduler_jobs')
    
    @classmethod
    def get_cron_schedule_string(cls, hours: List[int], minutes: List[int]) -> str:
        """
        Generate a cron schedule string from hours and minutes lists.
        
        Args:
            hours: List of hours (0-23)
            minutes: List of minutes (0-59)
            
        Returns:
            Cron schedule string for use with APScheduler
        """
        hours_str = ','.join(map(str, hours))
        minutes_str = ','.join(map(str, minutes))
        return f"{minutes_str} {hours_str} * * *"
    
    @classmethod
    def get_morning_schedule(cls) -> str:
        """Get the cron schedule string for morning scraping."""
        return cls.get_cron_schedule_string(cls.MORNING_HOURS, cls.MORNING_MINUTES)
    
    @classmethod
    def get_day_schedule(cls) -> str:
        """Get the cron schedule string for daytime scraping."""
        return cls.get_cron_schedule_string(cls.DAY_HOURS, cls.DAY_MINUTES)
    
    @classmethod
    def get_evening_schedule(cls) -> str:
        """Get the cron schedule string for evening scraping."""
        return cls.get_cron_schedule_string(cls.EVENING_HOURS, cls.EVENING_MINUTES)
    
    @classmethod
    def get_all_schedules(cls) -> dict:
        """Get all schedule strings as a dictionary."""
        return {
            'morning': cls.get_morning_schedule(),
            'day': cls.get_day_schedule(),
            'evening': cls.get_evening_schedule()
        }
    
    @classmethod
    def get_total_daily_runs(cls) -> int:
        """Calculate the total number of scraping runs per day."""
        morning_runs = len(cls.MORNING_HOURS) * len(cls.MORNING_MINUTES)
        day_runs = len(cls.DAY_HOURS) * len(cls.DAY_MINUTES)
        evening_runs = len(cls.EVENING_HOURS) * len(cls.EVENING_MINUTES)
        return morning_runs + day_runs + evening_runs
    
    @classmethod
    def validate_config(cls) -> bool:
        """
        Validate the configuration parameters.
        
        Returns:
            True if configuration is valid, False otherwise
        """
        # Validate hours (0-23)
        all_hours = cls.MORNING_HOURS + cls.DAY_HOURS + cls.EVENING_HOURS
        if not all(0 <= h <= 23 for h in all_hours):
            return False
        
        # Validate minutes (0-59)
        all_minutes = cls.MORNING_MINUTES + cls.DAY_MINUTES + cls.EVENING_MINUTES
        if not all(0 <= m <= 59 for m in all_minutes):
            return False
        
        # Validate max pages
        if cls.DAYWORK123_MAX_PAGES <= 0:
            return False
        
        # Validate max instances
        if cls.MAX_INSTANCES <= 0:
            return False
        
        return True
    
    @classmethod
    def print_schedule_summary(cls) -> None:
        """Print a summary of the current schedule configuration."""
        print("=== Scheduler Configuration Summary ===")
        print(f"Database URL: {cls.DB_URL}")
        print(f"Max Pages per Run: {cls.DAYWORK123_MAX_PAGES}")
        print(f"Total Daily Runs: {cls.get_total_daily_runs()}")
        print()
        
        print("Morning Schedule:")
        print(f"  Hours: {cls.MORNING_HOURS}")
        print(f"  Minutes: {cls.MORNING_MINUTES}")
        print(f"  Runs: {len(cls.MORNING_HOURS) * len(cls.MORNING_MINUTES)}")
        print()
        
        print("Day Schedule:")
        print(f"  Hours: {cls.DAY_HOURS}")
        print(f"  Minutes: {cls.DAY_MINUTES}")
        print(f"  Runs: {len(cls.DAY_HOURS) * len(cls.DAY_MINUTES)}")
        print()
        
        print("Evening Schedule:")
        print(f"  Hours: {cls.EVENING_HOURS}")
        print(f"  Minutes: {cls.EVENING_MINUTES}")
        print(f"  Runs: {len(cls.EVENING_HOURS) * len(cls.EVENING_MINUTES)}")
        print()
        
        print("Cron Schedules:")
        schedules = cls.get_all_schedules()
        for period, schedule in schedules.items():
            print(f"  {period.capitalize()}: {schedule}")

</code>

templates/partials/jobs_table.html:
<code>
<!-- Enhanced Job Cards Grid with Alpine.js -->
<div class="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 xl:grid-cols-4 gap-6 p-4" 
     x-data="jobGrid()">
    {% for job in jobs %}
    <!-- Interactive Job Card -->
    <div class="card bg-base-100 w-full shadow-sm hover:shadow-lg transition-all duration-300"
         x-data="jobCard('{{ job.id }}', '{{ job.title }}', `{{ job.description|replace('`', '') if job.description else '' }}`)"
         :class="{ 'ring-2 ring-primary': selected, 'bg-base-200': saved }">
        
        <!-- Card Header with Select Checkbox and Source Badge -->
        <div class="absolute top-2 right-2 z-10 flex gap-2">
            <!-- Source Badge -->
            <div class="badge badge-sm {% if job.source == 'daywork123' %}badge-accent{% elif job.source == 'yotspot' %}badge-primary{% else %}badge-secondary{% endif %}">
                {{ job.source|title }}
            </div>
            
            <input type="checkbox" 
                   x-model="selected"
                   @change="updateSelection('{{ job.id }}', $event.target.checked)"
                   class="checkbox checkbox-primary checkbox-sm bg-white shadow-md"
                   title="Select for comparison">
            
            <button @click="toggleSaved()" 
                    :class="saved ? 'text-red-500' : 'text-gray-400'"
                    class="btn btn-circle btn-xs bg-white shadow-md hover:bg-gray-100"
                    :title="saved ? 'Remove from saved' : 'Save job'">
                <span x-text="saved ? '❤️' : '🤍'"></span>
            </button>
        </div>
        
        <!-- Quality Score Indicator -->
        {% if job.quality_score %}
        <div class="absolute top-2 left-2 z-10">
            <div class="badge badge-sm {% if job.quality_score >= 0.8 %}badge-success{% elif job.quality_score >= 0.6 %}badge-warning{% else %}badge-error{% endif %}">
                {{ "★%.1f"|format(job.quality_score) }}
            </div>
        </div>
        {% endif %}
        
        <figure @click="toggleExpanded()" class="cursor-pointer relative overflow-hidden">
            <img :src="cardYachtImage.url || 'https://images.unsplash.com/photo-1544551763-46a013bb70d5?w=400&h=200&fit=crop&crop=center'" 
                 :alt="cardYachtImage.alt || 'Luxury Yacht'"
                 class="w-full h-48 object-cover transition-all duration-300"
                 :class="{ 'scale-105': expanded }"
                 @error="handleImageError()"
                 loading="lazy" />
            <!-- Yacht image refresh button -->
            <button @click.stop="refreshYachtImage()" 
                    class="absolute bottom-2 right-2 btn btn-circle btn-xs bg-white/80 hover:bg-white opacity-70 hover:opacity-100 transition-all"
                    title="New yacht image">
                🎲
            </button>
            <!-- Image overlay with yacht name -->
            <div class="absolute bottom-2 left-2 bg-black/50 text-white text-xs px-2 py-1 rounded backdrop-blur-sm">
                <span x-text="cardYachtImage.alt || 'Luxury Yacht'"></span>
            </div>
        </figure>
        
        <div class="card-body p-4">
            <h2 class="card-title text-lg font-bold line-clamp-2 cursor-pointer"
                @click="toggleExpanded()">
                {{ job.title }}
                <span x-show="expanded" class="text-primary">📖</span>
            </h2>
            
            <!-- Job ID Badge for Daywork123 -->
            {% if job.source == 'daywork123' and job.external_id %}
            <div class="text-xs font-mono text-base-content/60 mt-1">
                <span class="badge badge-ghost badge-xs">
                    ID: {{ job.external_id.split('_')[1] if '_' in job.external_id else job.external_id }}
                </span>
            </div>
            {% endif %}
            
            <!-- Job metadata badges -->
            <div class="flex flex-wrap gap-1 mt-2">
                {% if job.employment_type or job.job_type %}
                <div class="badge badge-ghost badge-sm">
                    {{ job.employment_type or job.job_type }}
                </div>
                {% endif %}
                {% if job.department %}
                <div class="badge badge-outline badge-sm">
                    {{ job.department }}
                </div>
                {% endif %}
                {% if job.vessel_type %}
                <div class="badge badge-neutral badge-sm">
                    {{ job.vessel_type|replace('_', ' ')|title }}
                </div>
                {% endif %}
            </div>
            
            {% if job.company %}
            <p class="text-sm text-base-content/70 font-medium">{{ job.company }}</p>
            {% endif %}
            
            <!-- Expandable Description -->
            <div>
                {% if job.description %}
                <p class="text-sm text-base-content/80 my-2"
                   :class="expanded ? '' : 'line-clamp-3'">
                    {% if job.description|length > 100 %}
                        <span x-show="!expanded">{{ job.description[:100] }}...</span>
                        <span x-show="expanded">{{ job.description }}</span>
                    {% else %}
                        {{ job.description }}
                    {% endif %}
                </p>
                
                {% if job.description|length > 100 %}
                <button @click="toggleExpanded()" 
                        class="text-primary text-xs hover:underline">
                    <span x-text="expanded ? 'Show Less' : 'Show More'"></span>
                </button>
                {% endif %}
                {% endif %}
            </div>
            
            <!-- Enhanced Job Information -->
            <div class="space-y-2">
                <!-- Location and Vessel Info -->
                <div class="flex flex-wrap gap-1">
                    {% if job.location %}
                    <div class="badge badge-outline badge-sm">
                        📍 {{ job.location }}
                    </div>
                    {% endif %}
                    
                    {% if job.vessel_size %}
                    <div class="badge badge-ghost badge-sm">
                        🛥️ {{ job.vessel_size }}
                    </div>
                    {% endif %}
                </div>
                
                <!-- Salary Information -->
                {% if job.salary_range or job.salary %}
                <div class="flex items-center gap-2 text-sm">
                    <span class="text-green-600 font-semibold">💰 {{ job.salary_range or job.salary }}</span>
                    {% if job.salary_currency and job.salary_currency != 'USD' %}
                    <span class="text-xs text-base-content/60">({{ job.salary_currency }})</span>
                    {% endif %}
                    {% if job.salary_period %}
                    <span class="text-xs text-base-content/60">per {{ job.salary_period }}</span>
                    {% endif %}
                </div>
                {% endif %}
                
                <!-- Requirements (expandable) -->
                {% if job.requirements and job.requirements|length > 0 %}
                <div x-show="expanded" class="mt-3">
                    <h4 class="text-sm font-semibold text-base-content/80 mb-1">✅ Requirements:</h4>
                    <ul class="text-xs text-base-content/70 space-y-1">
                        {% for req in job.requirements[:3] %}
                        <li class="flex items-start gap-1">
                            <span class="text-primary">•</span>
                            <span>{{ req }}</span>
                        </li>
                        {% endfor %}
                        {% if job.requirements|length > 3 %}
                        <li class="text-base-content/50">... and {{ job.requirements|length - 3 }} more</li>
                        {% endif %}
                    </ul>
                </div>
                {% endif %}
                
                <!-- Benefits (expandable) -->
                {% if job.benefits and job.benefits|length > 0 %}
                <div x-show="expanded" class="mt-3">
                    <h4 class="text-sm font-semibold text-base-content/80 mb-1">🎁 Benefits:</h4>
                    <ul class="text-xs text-base-content/70 space-y-1">
                        {% for benefit in job.benefits[:3] %}
                        <li class="flex items-start gap-1">
                            <span class="text-success">•</span>
                            <span>{{ benefit }}</span>
                        </li>
                        {% endfor %}
                        {% if job.benefits|length > 3 %}
                        <li class="text-base-content/50">... and {{ job.benefits|length - 3 }} more</li>
                        {% endif %}
                    </ul>
                </div>
                {% endif %}
                
                <!-- Source badge -->
                {% if job.source %}
                <div class="badge badge-secondary badge-sm">
                    {{ job.source }}
                </div>
                {% endif %}
            </div>
            
            <!-- Actions -->
            <div class="card-actions justify-between mt-4">
                <div class="flex gap-2">
                    {% if job.source_url or job.url %}
                    <a href="{{ job.source_url or job.url }}" target="_blank" 
                       class="btn btn-primary btn-sm"
                       @click.stop>
                        Apply
                    </a>
                    {% endif %}
                    <button class="btn btn-outline btn-sm" 
                            hx-get="/htmx/job-card/{{ job.id }}" 
                            hx-target="#job-modal-content"
                            @click.stop>
                        Details
                    </button>
                </div>
                
                <!-- Quick Actions -->
                <div class="flex gap-1">
                    <button @click.stop="copyJobLink()" 
                            class="btn btn-ghost btn-xs"
                            title="Copy job link">
                        📋
                    </button>
                    <button @click.stop="shareJob()" 
                            class="btn btn-ghost btn-xs"
                            title="Share job">
                        📤
                    </button>
                </div>
            </div>
        </div>
    </div>
    {% endfor %}
    
    <!-- Floating Comparison Panel -->
    <div x-show="selectedJobs.length > 0" 
         x-transition:enter="transition ease-out duration-300"
         x-transition:enter-start="transform translate-y-full"
         x-transition:enter-end="transform translate-y-0"
         class="fixed bottom-4 right-4 z-50">
        <div class="bg-primary text-primary-content p-4 rounded-lg shadow-xl">
            <div class="flex items-center gap-3">
                <span x-text="`${selectedJobs.length} jobs selected`" class="font-medium"></span>
                
                <button @click="compareJobs()" 
                        x-show="selectedJobs.length >= 2"
                        class="btn btn-secondary btn-sm">
                    Compare Jobs
                </button>
                
                <button @click="clearSelection()" 
                        class="btn btn-ghost btn-sm">
                    Clear
                </button>
            </div>
        </div>
    </div>
</div>

{% if not jobs %}
<!-- No Jobs State -->
<div class="flex flex-col items-center justify-center py-20">
    <div class="text-6xl mb-4">🛥️</div>
    <h3 class="text-xl font-bold mb-2">No Jobs Found</h3>
    <p class="text-base-content/70">Try refreshing or check back later</p>
</div>
{% endif %}

<!-- Job Detail Modal -->
<dialog id="job-modal" class="modal">
    <div class="modal-box max-w-4xl">
        <div id="job-modal-content">
            <!-- Job details will be loaded here -->
        </div>
        <div class="modal-action">
            <form method="dialog">
                <button class="btn">Close</button>
            </form>
        </div>
    </div>
</dialog> 
</code>

templates/partials/job_card.html:
<code>
<!-- Job Detail Modal Content -->
<div class="space-y-6">
    <!-- Job Header -->
    <div class="text-center border-b pb-4">
        <h1 class="text-2xl font-bold mb-2">{{ job.title }}</h1>
        {% if job.company %}
        <p class="text-lg text-base-content/70">{{ job.company }}</p>
        {% endif %}
        {% if job.location %}
        <p class="text-base-content/60">📍 {{ job.location }}</p>
        {% endif %}
    </div>

    <!-- Job Description -->
    {% if job.description %}
    <div class="card bg-base-200">
        <div class="card-body">
            <h3 class="card-title">Description</h3>
            <p class="text-base-content/80 leading-relaxed">{{ job.description }}</p>
        </div>
    </div>
    {% endif %}

    <!-- Job Details Grid -->
    <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
        {% if job.salary_range %}
        <div class="card bg-base-200">
            <div class="card-body">
                <h4 class="card-title text-lg">💰 Salary</h4>
                <p class="text-xl font-semibold text-success">{{ job.salary_range }}</p>
                {% if job.salary_per %}
                <p class="text-sm text-base-content/70">per {{ job.salary_per }}</p>
                {% endif %}
            </div>
        </div>
        {% endif %}

        {% if job.job_type or job.vessel_type or job.vessel_size %}
        <div class="card bg-base-200">
            <div class="card-body">
                <h4 class="card-title text-lg">🚢 Position Details</h4>
                <div class="space-y-2">
                    {% if job.job_type %}
                    <p><span class="font-medium">Type:</span> {{ job.job_type }}</p>
                    {% endif %}
                    {% if job.vessel_type %}
                    <p><span class="font-medium">Vessel:</span> {{ job.vessel_type }}</p>
                    {% endif %}
                    {% if job.vessel_size %}
                    <p><span class="font-medium">Size:</span> {{ job.vessel_size }}</p>
                    {% endif %}
                </div>
            </div>
        </div>
        {% endif %}

        {% if job.department %}
        <div class="card bg-base-200">
            <div class="card-body">
                <h4 class="card-title text-lg">👥 Department</h4>
                <p class="text-lg">{{ job.department }}</p>
            </div>
        </div>
        {% endif %}

        {% if job.source %}
        <div class="card bg-base-200">
            <div class="card-body">
                <h4 class="card-title text-lg">📋 Source</h4>
                <p class="text-lg">{{ job.source.title() }}</p>
                {% if job.external_id %}
                <p class="text-sm text-base-content/70">ID: {{ job.external_id }}</p>
                {% endif %}
            </div>
        </div>
        {% endif %}
    </div>

    <!-- Apply Button -->
    {% if job.source_url %}
    <div class="text-center pt-4 border-t">
        <a href="{{ job.source_url }}" 
           target="_blank" 
           class="btn btn-primary btn-lg gap-2">
            Apply for this Position
            <svg class="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14"></path>
            </svg>
        </a>
    </div>
    {% endif %}
</div> 
</code>

templates/partials/dashboard_stats.html:
<code>
<!-- Dashboard Statistics -->
<div class="stats shadow-xl w-full grid grid-cols-1 md:grid-cols-2 lg:grid-cols-4 bg-base-100" id="dashboard-stats">
    <div class="stat place-items-center">
        <div class="stat-figure text-primary">
            <i data-lucide="briefcase" class="h-8 w-8"></i>
        </div>
        <div class="stat-title">Total Opportunities</div>
        <div class="stat-value text-primary">{{ total_jobs }}</div>
        <div class="stat-desc">Premium yacht positions</div>
    </div>

    <div class="stat place-items-center">
        <div class="stat-figure text-secondary">
            <i data-lucide="trending-up" class="h-8 w-8"></i>
        </div>
        <div class="stat-title">New Today</div>
        <div class="stat-value text-secondary">{{ today_jobs }}</div>
        <div class="stat-desc">
            {% if today_jobs > 0 %}
            <span class="text-success flex items-center gap-1">
                <i data-lucide="arrow-up" class="h-3 w-3"></i>
                Fresh postings
            </span>
            {% else %}
            <span class="text-base-content/50">Check back tomorrow</span>
            {% endif %}
        </div>
    </div>

    <div class="stat place-items-center">
        <div class="stat-figure text-accent">
            <i data-lucide="calendar-days" class="h-8 w-8"></i>
        </div>
        <div class="stat-title">This Week</div>
        <div class="stat-value text-accent">{{ week_jobs }}</div>
        <div class="stat-desc">Past 7 days</div>
    </div>

    <div class="stat place-items-center">
        <div class="stat-figure text-success">
            <i data-lucide="activity" class="h-8 w-8"></i>
        </div>
        <div class="stat-title">System Status</div>
        {% if latest_scrape %}
            {% if latest_scrape.status == 'completed' %}
            <div class="stat-value text-sm text-success flex items-center gap-1">
                <i data-lucide="check-circle" class="h-4 w-4"></i>
                Online
            </div>
            <div class="stat-desc text-center">
                <div class="text-xs">
                    Last updated: {{ latest_scrape.completed_at.strftime('%H:%M') if latest_scrape.completed_at else 'Unknown' }}
                </div>
                <div class="text-xs font-medium text-success">
                    {{ latest_scrape.new_jobs or 0 }} new positions found
                </div>
            </div>
            {% elif latest_scrape.status == 'started' %}
            <div class="stat-value text-sm text-warning flex items-center gap-1">
                <span class="loading loading-spinner loading-xs"></span>
                Scanning
            </div>
            <div class="stat-desc">Searching for new positions...</div>
            {% elif latest_scrape.status == 'failed' %}
            <div class="stat-value text-sm text-error flex items-center gap-1">
                <i data-lucide="alert-triangle" class="h-4 w-4"></i>
                Error
            </div>
            <div class="stat-desc">System maintenance needed</div>
            {% else %}
            <div class="stat-value text-sm text-info flex items-center gap-1">
                <i data-lucide="pause" class="h-4 w-4"></i>
                {{ latest_scrape.status.title() }}
            </div>
            <div class="stat-desc">Status monitoring</div>
            {% endif %}
        {% else %}
        <div class="stat-value text-sm text-warning flex items-center gap-1">
            <i data-lucide="settings" class="h-4 w-4"></i>
            Setup
        </div>
        <div class="stat-desc">Initializing job scanner</div>
        {% endif %}
    </div>
</div>

<script>
    // Initialize Lucide icons for this stats component
    if (typeof lucide !== 'undefined') {
        lucide.createIcons();
    }
</script> 
</code>

templates/partials/job_stats.html:
<code>
<!-- Job Statistics Display -->
<div class="flex items-center gap-4 text-xs">
    <!-- Total Jobs -->
    <div class="flex items-center gap-1">
        <span class="font-semibold">{{ total }}</span>
        <span class="text-base-content/60">jobs</span>
    </div>
    
    <!-- Source Breakdown -->
    {% if sources %}
    <div class="flex items-center gap-2">
        {% for source, count in sources.items() %}
        <div class="flex items-center gap-1">
            <!-- Source-specific badges -->
            {% if source == 'daywork123' %}
            <div class="badge badge-accent badge-xs">{{ count }}</div>
            <span class="text-base-content/60">DW123</span>
            {% elif source == 'yotspot' %}
            <div class="badge badge-primary badge-xs">{{ count }}</div>
            <span class="text-base-content/60">Yotspot</span>
            {% else %}
            <div class="badge badge-secondary badge-xs">{{ count }}</div>
            <span class="text-base-content/60">{{ source|title }}</span>
            {% endif %}
        </div>
        {% endfor %}
    </div>
    {% endif %}
    
    <!-- Recent Activity -->
    {% if recent_week %}
    <div class="flex items-center gap-1">
        <div class="badge badge-success badge-xs">+{{ recent_week }}</div>
        <span class="text-base-content/60">this week</span>
    </div>
    {% endif %}
</div>


</code>

templates/base.html:
<code>
<!DOCTYPE html>
<html lang="en" data-theme="cupcake">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate" />
    <meta http-equiv="Pragma" content="no-cache" />
    <meta http-equiv="Expires" content="0" />
    <meta name="cache-buster" content="boats-only-v4-20250128" />
    <title>{% block title %}YotCrew.app{% endblock %}</title>
    
    <!-- Favicon -->
    <link rel="icon" type="image/svg+xml" href="/static/favicon.svg">
    <link rel="icon" type="image/x-icon" href="/static/favicon.svg">
    
    <!-- Tailwind CSS -->
    <script src="https://cdn.tailwindcss.com"></script>
    
    <!-- DaisyUI 4.x for better CDN compatibility -->
    <link href="https://cdn.jsdelivr.net/npm/daisyui@4.12.10/dist/full.min.css" rel="stylesheet" type="text/css" />
    
    <!-- HTMX -->
    <script src="https://unpkg.com/htmx.org@1.9.9"></script>
    
    <!-- Alpine.js -->
    <script defer src="https://unpkg.com/alpinejs@3.x.x/dist/cdn.min.js?v=20250128"></script>
    
    <!-- Lucide Icons -->
    <script src="https://unpkg.com/lucide@latest/dist/umd/lucide.js"></script>
    
    <style>
        .line-clamp-2 {
            display: -webkit-box;
            -webkit-line-clamp: 2;
            -webkit-box-orient: vertical;
            overflow: hidden;
        }
        .line-clamp-3 {
            display: -webkit-box;
            -webkit-line-clamp: 3;
            -webkit-box-orient: vertical;
            overflow: hidden;
        }
    </style>

    <!-- COMPLETE CACHE DESTRUCTION - BOATS ONLY v4.0 -->
    <script>
        // NUCLEAR CACHE-BUSTING: Destroy all browser cache for Alpine components
        window.YOTCREW_VERSION = '4.0-BOATS-ONLY-' + Date.now();
        console.log('🚀🛥️ NUCLEAR CACHE-BUSTING - YotCrew v' + window.YOTCREW_VERSION);
        
        // Force Alpine to reload by creating completely new namespace
        window.addEventListener('load', function() {
            console.log('🔥 Page loaded - Forcing Alpine component refresh');
            
            // Clear any cached data
            try {
                localStorage.removeItem('alpine-components');
                sessionStorage.clear();
                console.log('🧹 Cleared browser storage');
            } catch(e) {
                console.log('Storage already clear');
            }
        });
        
        // Global Alpine.js components available to all templates and partials
        document.addEventListener('alpine:init', () => {
            console.log('🎿 Alpine.js initializing BOATS-ONLY components v' + window.YOTCREW_VERSION);
            
            // BOATS ONLY - Job Grid Component
            Alpine.data('jobGrid', () => {
                console.log('🛥️ Creating BOATS-ONLY jobGrid component');
                return {
                    selectedJobs: [],
                    
                    // Yacht images collection for job cards
                    yachtImages: [
                        {
                            id: 1,
                            url: "https://images.unsplash.com/photo-1544551763-46a013bb70d5?w=400&h=200&fit=crop&crop=center",
                            alt: "White Superyacht Side Profile"
                        },
                        {
                            id: 2,
                            url: "https://images.unsplash.com/photo-1506905925346-21bda4d32df4?w=400&h=200&fit=crop&crop=center",
                            alt: "Luxury Motor Yacht Aerial"
                        },
                        {
                            id: 3,
                            url: "https://images.unsplash.com/photo-1517101216274-0a676e44e4bd?w=400&h=200&fit=crop&crop=center",
                            alt: "Superyacht Bow Angle"
                        },
                        {
                            id: 4,
                            url: "https://images.unsplash.com/photo-1469474968028-56623f02e42e?w=400&h=200&fit=crop&crop=center",
                            alt: "Marina Superyacht Docked"
                        },
                        {
                            id: 5,
                            url: "https://images.unsplash.com/photo-1615781222256-cd404c71eb9e?w=400&h=200&fit=crop&crop=center",
                            alt: "Sunset Superyacht"
                        },
                        {
                            id: 6,
                            url: "https://images.unsplash.com/photo-1566576912321-d58ddd7a6088?w=400&h=200&fit=crop&crop=center",
                            alt: "Classic Motor Yacht"
                        },
                        {
                            id: 7,
                            url: "https://images.unsplash.com/photo-1600298881974-6be191ceeda1?w=400&h=200&fit=crop&crop=center",
                            alt: "Harbor Yacht View"
                        },
                        {
                            id: 8,
                            url: "https://images.unsplash.com/photo-1600298882047-7a4db16b37bb?w=400&h=200&fit=crop&crop=center",
                            alt: "White Yacht Starboard"
                        },
                        {
                            id: 9,
                            url: "https://images.unsplash.com/photo-1571019613454-1cb2f99b2d8b?w=400&h=200&fit=crop&crop=center",
                            alt: "Superyacht Low Profile"
                        },
                        {
                            id: 10,
                            url: "https://images.unsplash.com/photo-1558618047-3c8c76ca7d13?w=400&h=200&fit=crop&crop=center",
                            alt: "Modern Yacht Design"
                        },
                        {
                            id: 11,
                            url: "https://images.unsplash.com/photo-1567899378494-47b22a2ae96a?w=400&h=200&fit=crop&crop=center",
                            alt: "Sleek Motor Yacht"
                        },
                        {
                            id: 12,
                            url: "https://images.unsplash.com/photo-1589810635657-84caf286b8ea?w=400&h=200&fit=crop&crop=center",
                            alt: "Yacht Bridge Detail"
                        },
                        {
                            id: 13,
                            url: "https://images.unsplash.com/photo-1600195077909-3b42abfcddef?w=400&h=200&fit=crop&crop=center",
                            alt: "Superyacht High View"
                        },
                        {
                            id: 14,
                            url: "https://images.unsplash.com/photo-1600195078299-a7de86e7dee8?w=400&h=200&fit=crop&crop=center",
                            alt: "Yacht Port Side"
                        },
                        {
                            id: 15,
                            url: "https://images.unsplash.com/photo-1626897530052-0a28ad2f1237?w=400&h=200&fit=crop&crop=center",
                            alt: "Motor Yacht Quarter"
                        },
                        {
                            id: 16,
                            url: "https://images.unsplash.com/photo-1600298881428-5c0f2a0b5173?w=400&h=200&fit=crop&crop=center",
                            alt: "Luxury Yacht Stern"
                        },
                        {
                            id: 17,
                            url: "https://images.unsplash.com/photo-1597149379073-e9a3c9fd5f65?w=400&h=200&fit=crop&crop=center",
                            alt: "Ocean Superyacht"
                        },
                        {
                            id: 18,
                            url: "https://images.unsplash.com/photo-1507003211169-0a1dd7228f2d?w=400&h=200&fit=crop&crop=center",
                            alt: "Mega Yacht Profile"
                        },
                        {
                            id: 19,
                            url: "https://images.unsplash.com/photo-1544551763-46a013bb70d5?w=400&h=200&fit=crop&crop=focalpoint&fp-x=0.3&fp-y=0.6",
                            alt: "Superyacht Detail View"
                        },
                        {
                            id: 20,
                            url: "https://images.unsplash.com/photo-1506905925346-21bda4d32df4?w=400&h=200&fit=crop&crop=focalpoint&fp-x=0.7&fp-y=0.4",
                            alt: "Yacht Aerial Close-up"
                        },
                        {
                            id: 21,
                            url: "https://images.unsplash.com/photo-1517101216274-0a676e44e4bd?w=400&h=200&fit=crop&crop=focalpoint&fp-x=0.5&fp-y=0.3",
                            alt: "Superyacht Front Detail"
                        },
                        {
                            id: 22,
                            url: "https://images.unsplash.com/photo-1469474968028-56623f02e42e?w=400&h=200&fit=crop&crop=focalpoint&fp-x=0.6&fp-y=0.7",
                            alt: "Marina Yacht Perspective"
                        },
                        {
                            id: 23,
                            url: "https://images.unsplash.com/photo-1615781222256-cd404c71eb9e?w=400&h=200&fit=crop&crop=focalpoint&fp-x=0.4&fp-y=0.5",
                            alt: "Sunset Yacht Silhouette"
                        },
                        {
                            id: 24,
                            url: "https://images.unsplash.com/photo-1566576912321-d58ddd7a6088?w=400&h=200&fit=crop&crop=focalpoint&fp-x=0.2&fp-y=0.6",
                            alt: "Classic Yacht Side"
                        },
                        {
                            id: 25,
                            url: "https://images.unsplash.com/photo-1600298881974-6be191ceeda1?w=400&h=200&fit=crop&crop=focalpoint&fp-x=0.8&fp-y=0.4",
                            alt: "Harbor Yacht Wide"
                        },
                        {
                            id: 26,
                            url: "https://images.unsplash.com/photo-1571019613454-1cb2f99b2d8b?w=400&h=200&fit=crop&crop=focalpoint&fp-x=0.3&fp-y=0.7",
                            alt: "Superyacht Low Angle"
                        },
                        {
                            id: 27,
                            url: "https://images.unsplash.com/photo-1558618047-3c8c76ca7d13?w=400&h=200&fit=crop&crop=focalpoint&fp-x=0.6&fp-y=0.3",
                            alt: "Modern Yacht Close"
                        },
                        {
                            id: 28,
                            url: "https://images.unsplash.com/photo-1567899378494-47b22a2ae96a?w=400&h=200&fit=crop&crop=focalpoint&fp-x=0.4&fp-y=0.8",
                            alt: "Sleek Yacht Profile"
                        },
                        {
                            id: 29,
                            url: "https://images.unsplash.com/photo-1589810635657-84caf286b8ea?w=400&h=200&fit=crop&crop=focalpoint&fp-x=0.7&fp-y=0.2",
                            alt: "Yacht Bridge Angle"
                        },
                        {
                            id: 30,
                            url: "https://images.unsplash.com/photo-1600195077909-3b42abfcddef?w=400&h=200&fit=crop&crop=focalpoint&fp-x=0.5&fp-y=0.5",
                            alt: "Superyacht Top View"
                        }
                    ],
                    
                    getRandomYachtImage() {
                        const randomIndex = Math.floor(Math.random() * this.yachtImages.length);
                        const randomImage = this.yachtImages[randomIndex];
                        console.log('🎲🛥️ BOATS-ONLY Random yacht image selected:', randomImage);
                        console.log('🔍 Total yacht images available:', this.yachtImages.length);
                        console.log('📸 First image alt text:', this.yachtImages[0].alt);
                        return randomImage;
                    },
                    
                    updateSelection(jobId, isSelected) {
                        if (isSelected) {
                            if (!this.selectedJobs.includes(jobId)) {
                                this.selectedJobs.push(jobId);
                            }
                        } else {
                            this.selectedJobs = this.selectedJobs.filter(id => id !== jobId);
                        }
                    },
                    
                    clearSelection() {
                        this.selectedJobs = [];
                        // Clear all checkboxes
                        document.querySelectorAll('input[type="checkbox"]').forEach(cb => cb.checked = false);
                    },
                    
                    compareJobs() {
                        alert(`Comparing ${this.selectedJobs.length} jobs: ${this.selectedJobs.join(', ')}`);
                    }
                };
            });
            
            // Individual Job Card Component
            Alpine.data('jobCard', (jobId, jobTitle, jobDescription) => {
                console.log('🃏 Creating jobCard component for:', jobId);
                return {
                    id: jobId,
                    title: jobTitle,
                    description: jobDescription,
                    expanded: false,
                    selected: false,
                    saved: false,
                    cardYachtImage: {
                        url: 'https://images.unsplash.com/photo-1544551763-46a013bb70d5?w=400&h=200&fit=crop&crop=center',
                        alt: 'White Superyacht Side Profile'
                    },
                    
                    init() {
                        // Get random yacht image for this card
                        console.log('🚀 Initializing job card:', this.id);
                        this.refreshYachtImage();
                    },
                    
                    refreshYachtImage() {
                        // Get random yacht image from parent component
                        const parent = this.$parent;
                        console.log('🔄 Refreshing yacht image for job:', this.id, 'Parent:', parent);
                        if (parent && parent.getRandomYachtImage) {
                            this.cardYachtImage = parent.getRandomYachtImage();
                            console.log('✅ New image assigned:', this.cardYachtImage);
                        } else {
                            console.warn('⚠️ Parent or getRandomYachtImage not available, using fallback');
                            // Fallback: get random image directly - BOATS ONLY
                            const images = [
                                {
                                    url: "https://images.unsplash.com/photo-1544551763-46a013bb70d5?w=400&h=200&fit=crop&crop=center",
                                    alt: "White Superyacht Side Profile"
                                },
                                {
                                    url: "https://images.unsplash.com/photo-1506905925346-21bda4d32df4?w=400&h=200&fit=crop&crop=center",
                                    alt: "Luxury Motor Yacht Aerial"
                                },
                                {
                                    url: "https://images.unsplash.com/photo-1517101216274-0a676e44e4bd?w=400&h=200&fit=crop&crop=center",
                                    alt: "Superyacht Bow Angle"
                                },
                                {
                                    url: "https://images.unsplash.com/photo-1469474968028-56623f02e42e?w=400&h=200&fit=crop&crop=center",
                                    alt: "Marina Superyacht Docked"
                                },
                                {
                                    url: "https://images.unsplash.com/photo-1615781222256-cd404c71eb9e?w=400&h=200&fit=crop&crop=center",
                                    alt: "Sunset Superyacht"
                                }
                            ];
                            const randomIndex = Math.floor(Math.random() * images.length);
                            this.cardYachtImage = images[randomIndex];
                            console.log('🎯 Fallback image assigned:', this.cardYachtImage);
                        }
                    },
                    
                    toggleExpanded() {
                        this.expanded = !this.expanded;
                    },
                    
                    toggleSaved() {
                        this.saved = !this.saved;
                        console.log(`Job ${this.id} ${this.saved ? 'saved' : 'unsaved'}`);
                    },
                    
                    copyJobLink() {
                        const url = `${window.location.origin}/jobs/${this.id}`;
                        navigator.clipboard.writeText(url).then(() => {
                            console.log('Job link copied!');
                        });
                    },
                    
                    shareJob() {
                        if (navigator.share) {
                            navigator.share({
                                title: this.title,
                                text: this.description.substring(0, 100) + '...',
                                url: `${window.location.origin}/jobs/${this.id}`
                            });
                        } else {
                            this.copyJobLink();
                        }
                    },

                    handleImageError() {
                        console.warn(`❌ Image failed to load for job ${this.id}. Using fallback.`);
                        this.refreshYachtImage();
                    }
                };
            });
            
            console.log('✅ Alpine.js global components registered successfully');
        });
    </script>
</head>

<body class="min-h-screen bg-base-200">

    <!-- Main Content -->
    <main>
        {% block content %}{% endblock %}
    </main>

    <!-- Additional page-specific JavaScript -->
    {% block scripts %}{% endblock %}

    <script>
         // Set cupcake theme properly
         document.addEventListener('DOMContentLoaded', function() {
             document.documentElement.setAttribute('data-theme', 'cupcake');
             console.log('Cupcake theme set via DaisyUI data-theme attribute');
         });

    </script>
</body>
</html> 
</code>

templates/dashboard.html:
<code>
{% extends "base.html" %}

{% block title %}YotCrew.app{% endblock %}

{% block content %}
<!-- Dashboard with Logo -->
<div class="container mx-auto p-4">
    <!-- Navigation Header with Logo -->
    <nav class="navbar bg-base-100 shadow-lg rounded-lg mb-6 min-h-28 py-4">
        <div class="navbar-start">
            <a href="/" class="btn btn-ghost normal-case text-xl p-2">
                <img src="/static/logo.png" alt="YotCrew.app" class="h-24 w-24 rounded-full object-cover">
            </a>
        </div>
        <div class="navbar-center hidden lg:flex">
            <p class="text-base-content/70 text-sm">Latest yacht crew opportunities</p>
        </div>
        <div class="navbar-end">
            <a href="/" class="btn btn-primary btn-sm">
                🔍 Go to Interactive Jobs Page
            </a>
        </div>
    </nav>
    
    <!-- Simple Jobs Grid - Cards Only -->
    <div id="jobs-container" 
         hx-get="/htmx/jobs-table?limit=50" 
         hx-trigger="load"
         hx-target="this"
         hx-swap="innerHTML">
        <div class="flex justify-center items-center h-64">
            <div class="loading loading-spinner loading-lg"></div>
        </div>
    </div>
</div>
{% endblock %}

{% block scripts %}
<script>
    // Auto-refresh dashboard stats
    setInterval(function() {
        const statsElement = document.getElementById('dashboard-stats');
        if (statsElement) {
            htmx.trigger(statsElement, 'refresh');
        }
    }, 60000); // Every minute
    
    // Initialize icons after content loads
    document.addEventListener('htmx:afterSwap', function() {
        lucide.createIcons();
    });
</script>
{% endblock %} 
</code>

templates/jobs.html:
<code>
{% extends "base.html" %}

{% block title %}YotCrew.app{% endblock %}

{% block content %}
<div class="space-y-6">
    <!-- Unified Header with Navigation and Filters -->
    <div class="bg-base-100 shadow-xl rounded-lg overflow-hidden" x-data="{ ...jobFilters(), ...yachtGallery() }">
        <!-- Top Navigation Bar -->
        <nav class="navbar bg-base-200/50 px-6 py-4">
            <div class="navbar-start">
                <a href="/" class="btn btn-ghost normal-case text-xl p-2">
                    <img src="/static/logo.png" alt="YotCrew.app" class="h-16 w-16 rounded-full object-cover">
                    <span class="ml-2 font-bold text-lg hidden sm:block">YotCrew.app</span>
                </a>
            </div>
            <div class="navbar-center hidden lg:flex">
                <div class="text-center">
                    <p class="text-base-content/70 text-sm">Find your perfect yacht position</p>
                    <div class="text-xs text-base-content/50" x-show="searchResults.total > 0">
                        <span x-text="searchResults.total || '0'"></span> opportunities available
                    </div>
                </div>
            </div>
            <div class="navbar-end flex items-center gap-3">
                <!-- Random Yacht Image Display -->
                <div class="hidden md:flex items-center gap-2">
                    <div class="avatar">
                        <div class="w-10 h-10 rounded-full ring ring-primary ring-offset-base-100 ring-offset-2">
                            <img :src="currentYachtImage.url" 
                                 :alt="currentYachtImage.alt"
                                 class="object-cover w-full h-full transition-all duration-500"
                                 x-transition:enter="transition ease-out duration-300"
                                 x-transition:enter-start="opacity-0 scale-95"
                                 x-transition:enter-end="opacity-100 scale-100">
                        </div>
                    </div>
                    <button @click="randomizeYachtImage()" 
                            class="btn btn-ghost btn-xs text-xs opacity-70 hover:opacity-100"
                            title="New yacht image">
                        🎲
                    </button>
                </div>

                                        <!-- Source Statistics -->
                <div class="hidden lg:flex items-center gap-2 text-sm">
                    <div id="source-stats" 
                         hx-get="/api/jobs/stats" 
                         hx-trigger="load, every 30s"
                         hx-target="this" 
                         hx-swap="innerHTML">
                        Loading stats...
                    </div>
                </div>
                
                        <!-- Quick Actions -->
                <div class="flex gap-2">
                    <button class="btn btn-primary btn-sm" 
                            hx-post="/api/scrape" 
                            hx-confirm="Refresh job listings?"
                            hx-target="#refresh-status">
                        🔄 Refresh
                    </button>
                    
                    <!-- Source Filter Dropdown -->
                    <div class="dropdown dropdown-end">
                        <div tabindex="0" role="button" class="btn btn-outline btn-sm">
                            📊 Sources
                        </div>
                        <ul tabindex="0" class="dropdown-content z-[1] menu p-2 shadow bg-base-100 rounded-box w-52">
                            <li><a href="#" @click="filters.source = 'all'; applyFilters()">All Sources</a></li>
                            <li><a href="#" @click="filters.source = 'yotspot'; applyFilters()">Yotspot</a></li>
                            <li><a href="#" @click="filters.source = 'daywork123'; applyFilters()">Daywork123</a></li>
                        </ul>
                    </div>
                </div>
                
                <!-- Menu Dropdown -->
                <div class="dropdown dropdown-end">
                    <div tabindex="0" role="button" class="btn btn-ghost btn-circle btn-sm">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h7" /></svg>
                    </div>
                    <ul tabindex="0" class="menu menu-sm dropdown-content mt-3 z-[1] p-2 shadow bg-base-100 rounded-box w-52">
                        <li><a href="/">🏠 Home</a></li>
                        <li><a href="/dashboard">📊 Dashboard</a></li>
                        <li><a href="#" onclick="location.reload()">🔄 Refresh</a></li>
                    </ul>
                </div>
            </div>
        </nav>

        <!-- Yacht Image Banner (Mobile) -->
        <div class="md:hidden relative h-20 overflow-hidden">
            <img :src="currentYachtImage.url" 
                 :alt="currentYachtImage.alt"
                 class="w-full h-full object-cover opacity-20"
                 x-transition:enter="transition ease-out duration-500"
                 x-transition:enter-start="opacity-0"
                 x-transition:enter-end="opacity-20">
            <div class="absolute inset-0 bg-gradient-to-r from-base-100/80 to-transparent"></div>
            <div class="absolute bottom-2 left-4 text-xs text-base-content/60" x-text="currentYachtImage.alt"></div>
            <button @click="randomizeYachtImage()" 
                    class="absolute bottom-2 right-4 btn btn-ghost btn-xs opacity-70">
                🎲 New Image
            </button>
        </div>

        <!-- Integrated Search and Filters Section -->
        <div class="px-6 py-4 bg-gradient-to-r from-base-100 to-base-200/30">
            <!-- Filter Status and Actions Bar -->
            <div class="flex justify-between items-center mb-4" x-show="hasActiveFilters || searchResults.total > 0">
                <div class="flex items-center gap-4">
                    <div class="text-sm text-base-content/70">
                        <span class="font-semibold" x-text="searchResults.total || '0'"></span> jobs found
                        <span x-show="hasActiveFilters" class="text-primary ml-2">• <span x-text="activeFilterCount"></span> filters active</span>
                    </div>
                    
                    <!-- Active Filter Pills -->
                    <div class="flex flex-wrap gap-1" x-show="hasActiveFilters">
                        <span x-show="filters.search" 
                              class="badge badge-primary badge-sm cursor-pointer hover:badge-primary-focus"
                              @click="filters.search = ''; applyFilters()">
                            🔍 <span x-text="filters.search.slice(0, 10) + (filters.search.length > 10 ? '...' : '')"></span> ✕
                        </span>
                        <span x-show="filters.jobType" 
                              class="badge badge-secondary badge-sm cursor-pointer hover:badge-secondary-focus"
                              @click="filters.jobType = ''; applyFilters()">
                            💼 <span x-text="filters.jobType"></span> ✕
                        </span>
                        <span x-show="filters.location" 
                              class="badge badge-accent badge-sm cursor-pointer hover:badge-accent-focus"
                              @click="filters.location = ''; applyFilters()">
                            📍 <span x-text="filters.location"></span> ✕
                        </span>
                        <span x-show="filters.department" 
                              class="badge badge-info badge-sm cursor-pointer hover:badge-info-focus"
                              @click="filters.department = ''; applyFilters()">
                            👥 <span x-text="filters.department"></span> ✕
                        </span>
                    </div>
                </div>
                
                <button @click="clearAllFilters()" 
                        x-show="hasActiveFilters"
                        class="btn btn-ghost btn-sm">
                    Clear All
                </button>
            </div>
            
            <!-- Main Search and Filter Form -->
            <form hx-get="/htmx/jobs-table" 
                  hx-target="#jobs-container" 
                  hx-trigger="submit"
                  @submit="updateResults()">
                
                <!-- Primary Search Row -->
                <div class="grid grid-cols-1 md:grid-cols-3 lg:grid-cols-5 gap-3 mb-4">
                    <!-- Enhanced Search -->
                    <div class="form-control md:col-span-2">
                        <label class="label">
                            <span class="label-text font-medium">🔍 Search Jobs</span>
                        </label>
                        <div class="relative">
                            <input type="text" 
                                   name="search" 
                                   x-model="filters.search"
                                   @input.debounce.300ms="applyFilters()"
                                   placeholder="Job title, company, description..." 
                                   class="input input-bordered w-full pr-8" />
                            <button type="button"
                                    @click="filters.search = ''; applyFilters()"
                                    x-show="filters.search"
                                    class="absolute right-2 top-1/2 transform -translate-y-1/2 text-gray-400 hover:text-gray-600 transition-colors">
                                ✕
                            </button>
                        </div>
                    </div>

                    <!-- Job Type Filter -->
                    <div class="form-control">
                        <label class="label">
                            <span class="label-text font-medium">💼 Type</span>
                        </label>
                        <select name="job_type" 
                                x-model="filters.jobType"
                                @change="applyFilters()"
                                class="select select-bordered select-sm">
                            <option value="">All Types</option>
                            <option value="Permanent">Permanent</option>
                            <option value="Temporary">Temporary</option>
                            <option value="Rotational">Rotational</option>
                            <option value="Seasonal">Seasonal</option>
                        </select>
                    </div>

                    <!-- Location Filter -->
                    <div class="form-control">
                        <label class="label">
                            <span class="label-text font-medium">📍 Location</span>
                        </label>
                        <select name="location" 
                                x-model="filters.location"
                                @change="applyFilters()"
                                class="select select-bordered select-sm">
                            <option value="">All Locations</option>
                            <option value="Mediterranean">Mediterranean</option>
                            <option value="Caribbean">Caribbean</option>
                            <option value="United States">United States</option>
                            <option value="France">France</option>
                            <option value="Monaco">Monaco</option>
                        </select>
                    </div>

                    <!-- Sort Filter -->
                    <div class="form-control">
                        <label class="label">
                            <span class="label-text font-medium">📊 Sort</span>
                        </label>
                        <select name="sort" 
                                x-model="filters.sort"
                                @change="applyFilters()"
                                class="select select-bordered select-sm">
                            <option value="">Latest</option>
                            <option value="posted_at">Date Posted</option>
                            <option value="title">Job Title</option>
                            <option value="salary">Salary</option>
                        </select>
                    </div>
                </div>

                <!-- Secondary Filters Row -->
                <div class="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-3 mb-4">
                    <!-- Vessel Size Filter -->
                    <div class="form-control">
                        <label class="label">
                            <span class="label-text font-medium">🛥️ Vessel Size</span>
                        </label>
                        <select name="vessel_size" 
                                x-model="filters.vessel_size"
                                @change="applyFilters()"
                                class="select select-bordered select-sm">
                            <option value="">All Sizes</option>
                            <option value="0-30m">0-30m</option>
                            <option value="31-39m">31-39m</option>
                            <option value="40-49m">40-49m</option>
                            <option value="50-74m">50-74m</option>
                            <option value="75m+">75m+</option>
                        </select>
                    </div>

                    <!-- Department Filter -->
                    <div class="form-control">
                        <label class="label">
                            <span class="label-text font-medium">👥 Department</span>
                        </label>
                        <select name="department" 
                                x-model="filters.department"
                                @change="applyFilters()"
                                class="select select-bordered select-sm">
                            <option value="">All Departments</option>
                            <option value="Deck">Deck</option>
                            <option value="Interior">Interior</option>
                            <option value="Engineering">Engineering</option>
                            <option value="Galley">Galley</option>
                        </select>
                    </div>
                    
                    <!-- Source Filter -->
                    <div class="form-control">
                        <label class="label">
                            <span class="label-text font-medium">📊 Source</span>
                        </label>
                        <select name="source" 
                                x-model="filters.source"
                                @change="applyFilters()"
                                class="select select-bordered select-sm">
                            <option value="all">All Sources</option>
                            <option value="yotspot">Yotspot</option>
                            <option value="daywork123">Daywork123</option>
                        </select>
                    </div>

                    <!-- Vessel Type Filter -->
                    <div class="form-control">
                        <label class="label">
                            <span class="label-text font-medium">⛵ Vessel Type</span>
                        </label>
                        <select name="vessel_type" 
                                x-model="filters.vessel_type"
                                @change="applyFilters()"
                                class="select select-bordered select-sm">
                            <option value="">All Types</option>
                            <option value="Motor">Motor Yacht</option>
                            <option value="Sailing">Sailing Yacht</option>
                            <option value="Explorer">Explorer</option>
                            <option value="Catamaran">Catamaran</option>
                        </select>
                    </div>
                </div>

                <!-- Quick Filter Badges -->
                <div class="border-t border-base-300 pt-4">
                    <div class="flex items-center gap-2 mb-2">
                        <span class="text-sm font-medium text-base-content/70">Quick Filters:</span>
                    </div>
                    <div class="flex flex-wrap gap-2">
                        <!-- Department Quick Filters -->
                        <button type="button" @click="setQuickFilter('department', 'Deck')" 
                                :class="filters.department === 'Deck' ? 'badge-primary' : 'badge-outline hover:badge-primary'"
                                class="badge cursor-pointer transition-all duration-200">
                            ⚓ Deck
                        </button>
                        <button type="button" @click="setQuickFilter('department', 'Interior')" 
                                :class="filters.department === 'Interior' ? 'badge-primary' : 'badge-outline hover:badge-primary'"
                                class="badge cursor-pointer transition-all duration-200">
                            🏠 Interior
                        </button>
                        <button type="button" @click="setQuickFilter('department', 'Engineering')" 
                                :class="filters.department === 'Engineering' ? 'badge-primary' : 'badge-outline hover:badge-primary'"
                                class="badge cursor-pointer transition-all duration-200">
                            🔧 Engineering
                        </button>
                        <button type="button" @click="setQuickFilter('department', 'Galley')" 
                                :class="filters.department === 'Galley' ? 'badge-primary' : 'badge-outline hover:badge-primary'"
                                class="badge cursor-pointer transition-all duration-200">
                            👨‍🍳 Galley
                        </button>
                        
                        <!-- Vessel Type Quick Filters -->
                        <div class="divider divider-horizontal mx-1"></div>
                        <button type="button" @click="setQuickFilter('vessel_type', 'Motor')" 
                                :class="filters.vessel_type === 'Motor' ? 'badge-secondary' : 'badge-outline hover:badge-secondary'"
                                class="badge cursor-pointer transition-all duration-200">
                            🛥️ Motor
                        </button>
                        <button type="button" @click="setQuickFilter('vessel_type', 'Sailing')" 
                                :class="filters.vessel_type === 'Sailing' ? 'badge-secondary' : 'badge-outline hover:badge-secondary'"
                                class="badge cursor-pointer transition-all duration-200">
                            ⛵ Sailing
                        </button>
                        <button type="button" @click="setQuickFilter('vessel_type', 'Explorer')" 
                                :class="filters.vessel_type === 'Explorer' ? 'badge-secondary' : 'badge-outline hover:badge-secondary'"
                                class="badge cursor-pointer transition-all duration-200">
                            🧭 Explorer
                        </button>
                    </div>
                </div>

                <!-- Hidden submit button -->
                <input type="submit" style="display: none;" />
            </form>
        </div>
    </div>
    
    <!-- Status Messages -->
    <div id="refresh-status"></div>

    <!-- Jobs Container -->
    <div id="jobs-container" 
         hx-get="/htmx/jobs-table" 
         hx-trigger="load">
        <!-- Jobs will be loaded here -->
        <div class="flex justify-center py-8">
            <span class="loading loading-spinner loading-lg"></span>
        </div>
    </div>
</div>

<!-- Job Detail Modal -->
<dialog id="job_modal" class="modal">
    <div class="modal-box w-11/12 max-w-5xl">
        <form method="dialog">
            <button class="btn btn-sm btn-circle btn-ghost absolute right-2 top-2">✕</button>
        </form>
        <div id="job-modal-content">
            <!-- Job details will be loaded here via HTMX -->
        </div>
    </div>
    <form method="dialog" class="modal-backdrop">
        <button>close</button>
    </form>
</dialog>
{% endblock %}

{% block scripts %}
<script>
    // Yacht Gallery Component
    function yachtGallery() {
        return {
            currentYachtImage: {},
            yachtImages: [
                {
                    id: 1,
                    url: "https://images.unsplash.com/photo-1544551763-46a013bb70d5?w=400&h=400&fit=crop&crop=center",
                    alt: "Luxury Motor Yacht"
                },
                {
                    id: 2,
                    url: "https://images.unsplash.com/photo-1469474968028-56623f02e42e?w=400&h=400&fit=crop&crop=center",
                    alt: "Superyacht in Harbor"
                },
                {
                    id: 3,
                    url: "https://images.unsplash.com/photo-1582967788606-a171c1080cb0?w=400&h=400&fit=crop&crop=center",
                    alt: "Modern Sailing Yacht"
                },
                {
                    id: 4,
                    url: "https://images.unsplash.com/photo-1559827260-dc66d52bef19?w=400&h=400&fit=crop&crop=center",
                    alt: "Explorer Yacht"
                },
                {
                    id: 5,
                    url: "https://images.unsplash.com/photo-1578662996442-48f60103fc96?w=400&h=400&fit=crop&crop=center",
                    alt: "Catamaran Superyacht"
                },
                {
                    id: 6,
                    url: "https://images.unsplash.com/photo-1615781222256-cd404c71eb9e?w=400&h=400&fit=crop&crop=center",
                    alt: "Mega Yacht at Sunset"
                },
                {
                    id: 7,
                    url: "https://images.unsplash.com/photo-1566576912321-d58ddd7a6088?w=400&h=400&fit=crop&crop=center",
                    alt: "Classic Motor Yacht"
                },
                {
                    id: 8,
                    url: "https://images.unsplash.com/photo-1500917293891-ef795e70e1f6?w=400&h=400&fit=crop&crop=center",
                    alt: "Sailing Superyacht"
                },
                {
                    id: 9,
                    url: "https://images.unsplash.com/photo-1578662996442-48f60103fc96?w=400&h=400&fit=crop&crop=center",
                    alt: "Luxury Charter Yacht"
                },
                {
                    id: 10,
                    url: "https://images.unsplash.com/photo-1517101216274-0a676e44e4bd?w=400&h=400&fit=crop&crop=center",
                    alt: "Sport Fishing Yacht"
                },
                {
                    id: 11,
                    url: "https://images.unsplash.com/photo-1615781222256-cd404c71eb9e?w=400&h=400&fit=crop&crop=center",
                    alt: "Mediterranean Superyacht"
                },
                {
                    id: 12,
                    url: "https://images.unsplash.com/photo-1506905925346-21bda4d32df4?w=400&h=400&fit=crop&crop=center",
                    alt: "Ocean Explorer"
                },
                {
                    id: 13,
                    url: "https://images.unsplash.com/photo-1559827260-dc66d52bef19?w=400&h=400&fit=crop&crop=center",
                    alt: "Modern Expedition Yacht"
                },
                {
                    id: 14,
                    url: "https://images.unsplash.com/photo-1469474968028-56623f02e42e?w=400&h=400&fit=crop&crop=center",
                    alt: "Luxury Motor Yacht"
                },
                {
                    id: 15,
                    url: "https://images.unsplash.com/photo-1567899378494-47b22a2ae96a?w=400&h=400&fit=crop&crop=center",
                    alt: "Racing Yacht"
                },
                {
                    id: 16,
                    url: "https://images.unsplash.com/photo-1559827260-dc66d52bef19?w=400&h=400&fit=crop&crop=center",
                    alt: "Charter Superyacht"
                },
                {
                    id: 17,
                    url: "https://images.unsplash.com/photo-1578662996442-48f60103fc96?w=400&h=400&fit=crop&crop=center",
                    alt: "Twin Hull Catamaran"
                },
                {
                    id: 18,
                    url: "https://images.unsplash.com/photo-1615781222256-cd404c71eb9e?w=400&h=400&fit=crop&crop=center",
                    alt: "Sunset Cruiser"
                },
                {
                    id: 19,
                    url: "https://images.unsplash.com/photo-1507003211169-0a1dd7228f2d?w=400&h=400&fit=crop&crop=center",
                    alt: "Ocean Liner Style Yacht"
                },
                {
                    id: 20,
                    url: "https://images.unsplash.com/photo-1578662996442-48f60103fc96?w=400&h=400&fit=crop&crop=center",
                    alt: "High-Speed Motor Yacht"
                },
                {
                    id: 21,
                    url: "https://images.unsplash.com/photo-1544551763-46a013bb70d5?w=400&h=400&fit=crop&crop=center",
                    alt: "Flybridge Motor Yacht"
                },
                {
                    id: 22,
                    url: "https://images.unsplash.com/photo-1506905925346-21bda4d32df4?w=400&h=400&fit=crop&crop=center",
                    alt: "Explorer Superyacht"
                },
                {
                    id: 23,
                    url: "https://images.unsplash.com/photo-1517101216274-0a676e44e4bd?w=400&h=400&fit=crop&crop=center",
                    alt: "Sport Yacht"
                },
                {
                    id: 24,
                    url: "https://images.unsplash.com/photo-1559827260-dc66d52bef19?w=400&h=400&fit=crop&crop=center",
                    alt: "Mega Yacht"
                },
                {
                    id: 25,
                    url: "https://images.unsplash.com/photo-1615781222256-cd404c71eb9e?w=400&h=400&fit=crop&crop=center",
                    alt: "Luxury Charter Vessel"
                },
                {
                    id: 26,
                    url: "https://images.unsplash.com/photo-1578662996442-48f60103fc96?w=400&h=400&fit=crop&crop=center",
                    alt: "High-Performance Yacht"
                },
                {
                    id: 27,
                    url: "https://images.unsplash.com/photo-1500917293891-ef795e70e1f6?w=400&h=400&fit=crop&crop=center",
                    alt: "Classic Sailing Yacht"
                },
                {
                    id: 28,
                    url: "https://images.unsplash.com/photo-1567899378494-47b22a2ae96a?w=400&h=400&fit=crop&crop=center",
                    alt: "Performance Cruiser"
                },
                {
                    id: 29,
                    url: "https://images.unsplash.com/photo-1506905925346-21bda4d32df4?w=400&h=400&fit=crop&crop=center",
                    alt: "Long Range Explorer"
                },
                {
                    id: 30,
                    url: "https://images.unsplash.com/photo-1469474968028-56623f02e42e?w=400&h=400&fit=crop&crop=center",
                    alt: "Ultimate Superyacht"
                }
            ],
            
            init() {
                this.shuffleYachtImages();
                this.randomizeYachtImage();
                
                // Auto-rotate images every 30 seconds
                setInterval(() => {
                    this.randomizeYachtImage();
                }, 30000);
            },
            
            randomizeYachtImage() {
                const randomIndex = Math.floor(Math.random() * this.yachtImages.length);
                this.currentYachtImage = this.yachtImages[randomIndex];
            },
            
            shuffleYachtImages() {
                // Fisher-Yates shuffle algorithm
                const shuffled = [...this.yachtImages];
                for (let i = shuffled.length - 1; i > 0; i--) {
                    const j = Math.floor(Math.random() * (i + 1));
                    [shuffled[i], shuffled[j]] = [shuffled[j], shuffled[i]];
                }
                this.yachtImages = shuffled;
            },
            
            getRandomYachtImage() {
                const randomIndex = Math.floor(Math.random() * this.yachtImages.length);
                return this.yachtImages[randomIndex];
            }
        }
    }



    // Alpine.js Components
    function jobFilters() {
        return {
            filters: {
                search: '',
                jobType: '',
                location: '',
                vessel_size: '',
                vessel_type: '',
                department: '',
                source: 'all',
                sort: ''
            },
            searchResults: {
                total: 0,
                loading: false
            },
            
            get hasActiveFilters() {
                return Object.values(this.filters).some(value => value !== '');
            },
            
            get activeFilterCount() {
                return Object.values(this.filters).filter(value => value !== '').length;
            },
            
            setQuickFilter(type, value) {
                if (this.filters[type] === value) {
                    this.filters[type] = '';
                } else {
                    this.filters[type] = value;
                }
                this.applyFilters();
            },
            
            clearAllFilters() {
                this.filters = {
                    search: '',
                    jobType: '',
                    location: '',
                    vessel_size: '',
                    vessel_type: '',
                    department: '',
                    source: 'all',
                    sort: ''
                };
                this.applyFilters();
            },
            
            applyFilters() {
                this.searchResults.loading = true;
                
                // Build query parameters
                const params = new URLSearchParams();
                Object.entries(this.filters).forEach(([key, value]) => {
                    if (value) {
                        // Map frontend filter names to backend parameter names
                        const paramMap = {
                            'jobType': 'job_type',
                            'vessel_size': 'vessel_size',
                            'vessel_type': 'vessel_type'
                        };
                        const paramName = paramMap[key] || key;
                        params.append(paramName, value);
                    }
                });
                
                // Trigger HTMX request with filters
                htmx.ajax('GET', `/htmx/jobs-table?${params.toString()}`, {
                    target: '#jobs-container'
                }).then(() => {
                    this.searchResults.loading = false;
                });
            },
            
            updateResults() {
                // Called when form is submitted
                this.searchResults.loading = false;
            }
        }
    }
</script>
{% endblock %} 
</code>

RUN_GUIDE.md:
<code>
# 🚀 How to Run the Daywork123.com Scraper

## Quick Start (5 minutes)

### 1. Install Dependencies
```bash
# In your project directory
pip install -r requirements.txt
playwright install chromium
```

### 2. Test the Installation
```bash
# Run the test script to verify everything works
python test_daywork123_scraper.py
```

### 3. Run Individual Scrapers
```bash
# Scrape Daywork123.com (1 page)
python -c "from app.services.scraping_service import scrape_daywork123; import asyncio; print(asyncio.run(scrape_daywork123(max_pages=1)))"

# Scrape Yotspot.com (1 page)  
python -c "from app.services.scraping_service import scrape_yotspot; import asyncio; print(asyncio.run(scrape_yotspot(max_pages=1)))"

# Scrape all sources
python -c "from app.services.scraping_service import scrape_all_sources; import asyncio; print(asyncio.run(scrape_all_sources(max_pages=1)))"
```

## Detailed Usage Guide

### Method 1: Using the Test Script (Recommended for Testing)
```bash
# This runs comprehensive tests for all scrapers
python test_daywork123_scraper.py
```

### Method 2: Using the Scraping Service (Production)
```python
# Create a simple run script: run_scraper.py
import asyncio
from app.services.scraping_service import ScrapingService

async def main():
    service = ScrapingService()
    
    # Option A: Scrape Daywork123 only
    result = await service.scrape_source("daywork123", max_pages=3)
    print(f"Daywork123: Found {result['jobs_found']} jobs, {result['new_jobs']} new")
    
    # Option B: Scrape all sources
    results = await service.scrape_all_sources(max_pages=2)
    for result in results:
        print(f"{result['source']}: {result['jobs_found']} jobs found")

if __name__ == "__main__":
    asyncio.run(main())
```

Run it:
```bash
python run_scraper.py
```

### Method 3: Interactive Python
```bash
# Start Python interactive mode
python

# Then run these commands:
from app.services.scraping_service import ScrapingService
import asyncio

service = ScrapingService()
result = asyncio.run(service.scrape_source("daywork123", max_pages=2))
print(result)
```

## Common Usage Patterns

### Check What's Available
```bash
# List all registered scrapers
python -c "from app.scrapers.registry import ScraperRegistry; print('Available scrapers:', ScraperRegistry.list_scrapers())"

# Check health of all sources
python -c "from app.services.scraping_service import ScrapingService; import asyncio; service = ScrapingService(); print(asyncio.run(service.health_check_all()))"
```

### Run with Custom Settings
```bash
# Create custom_run.py
import asyncio
from app.services.scraping_service import ScrapingService

async def custom_scrape():
    service = ScrapingService()
    
    # Scrape with filters
    filters = {"location": "Caribbean", "job_type": "deckhand"}
    
    # Scrape 5 pages from Daywork123
    result = await service.scrape_source("daywork123", max_pages=5)
    
    print(f"Scraped {result['jobs_found']} jobs from {result['source']}")
    print(f"New jobs: {result['new_jobs']}, Updated: {result['updated_jobs']}")
    print(f"Duration: {result['duration']} seconds")

if __name__ == "__main__":
    asyncio.run(custom_scrape())
```

## Troubleshooting Commands

### If Playwright Fails
```bash
# Install Playwright browsers
playwright install chromium

# On Linux, install dependencies
sudo playwright install-deps chromium
```

### If Dependencies Missing
```bash
# Reinstall requirements
pip install -r requirements.txt --force-reinstall

# Install specific missing packages
pip install playwright aiohttp beautifulsoup4
```

### Test Database Connection
```bash
python -c "from app.database import SessionLocal; db = SessionLocal(); print('Database connected successfully')"
```

## Quick Debug Mode

### Check if Scrapers Are Registered
```bash
python -c "
from app.scrapers.registry import ScraperRegistry
scrapers = ScraperRegistry.list_scrapers()
print('Registered scrapers:', scrapers)
for name in scrapers:
    scraper = ScraperRegistry.get_scraper(name)
    print(f'{name}: {scraper.base_url}')
"
```

### Test Individual Components
```bash
# Test Daywork123 scraper directly
python -c "
from app.scrapers.daywork123 import Daywork123Scraper
import asyncio

async def test():
    scraper = Daywork123Scraper()
    is_working = await scraper.test_connection()
    print(f'Daywork123 accessible: {is_working}')

asyncio.run(test())
"
```

## Production Integration

### Add to Your Scheduler
```python
# In your scheduler.py or wherever you run scheduled tasks
from app.services.scraping_service import scrape_all_sources
import asyncio

async def scheduled_job():
    """Run this every 45 minutes"""
    results = await scrape_all_sources(max_pages=3)
    for result in results:
        print(f"{result['source']}: {result['jobs_found']} jobs")

# Run manually
asyncio.run(scheduled_job())
```

### API Endpoint Example
```python
# In your FastAPI routes
from fastapi import APIRouter
from app.services.scraping_service import ScrapingService

router = APIRouter()

@router.post("/api/scrape/daywork123")
async def trigger_daywork123_scraping(pages: int = 3):
    service = ScrapingService()
    return await service.scrape_source("daywork123", max_pages=pages)
```

## One-Line Commands Summary

```bash
# Install everything
pip install -r requirements.txt && playwright install chromium

# Test all scrapers
python test_daywork123_scraper.py

# Quick Daywork123 scrape
python -c "from app.services.scraping_service import scrape_daywork123; import asyncio; print(asyncio.run(scrape_daywork123(max_pages=1)))"

# Check system health
python -c "from app.services.scraping_service import ScrapingService; import asyncio; service = ScrapingService(); print(asyncio.run(service.health_check_all()))"
</code>

scrape_daywork123_to_db.py:
<code>
#!/usr/bin/env python3
"""
Use the existing daywork123_scraper.py to scrape jobs and save to database
Modified to only scrape jobs from the last week
"""
import asyncio
import logging
import sys
import os
from datetime import datetime, timedelta
import dateparser
import re

# Add the app directory to Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'app'))

# Import the existing scraper
from daywork123_scraper import Daywork123Scraper

# Import database components
from app.database import SessionLocal
from app.models import Job

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def parse_posted_date(date_str):
    """Parse posted date string to datetime object"""
    if not date_str or date_str.strip() == "":
        return None
    
    try:
        # Try to parse the date using dateparser
        parsed_date = dateparser.parse(date_str)
        if parsed_date:
            return parsed_date
        
        # Fallback: try common date formats
        date_formats = [
            "%Y/%m/%d",
            "%m/%d/%Y", 
            "%d/%m/%Y",
            "%Y-%m-%d",
            "%m-%d-%Y",
            "%d-%m-%Y"
        ]
        
        for fmt in date_formats:
            try:
                return datetime.strptime(date_str.strip(), fmt)
            except ValueError:
                continue
                
    except Exception as e:
        logger.warning(f"Could not parse date '{date_str}': {e}")
    
    return None

def is_job_from_last_week(scraped_job):
    """Check if job was posted in the last 7 days"""
    posted_date_str = scraped_job.get('posted_date', '')
    
    if not posted_date_str:
        logger.debug(f"No posted date for job {scraped_job.get('id', 'unknown')}")
        return False
    
    posted_date = parse_posted_date(posted_date_str)
    
    if not posted_date:
        logger.debug(f"Could not parse posted date '{posted_date_str}' for job {scraped_job.get('id', 'unknown')}")
        return False
    
    # Calculate if job is from last week
    one_week_ago = datetime.now() - timedelta(days=7)
    is_recent = posted_date >= one_week_ago
    
    if is_recent:
        logger.info(f"✅ Job from last week: {scraped_job.get('title', 'Unknown')} (posted: {posted_date_str})")
    else:
        logger.debug(f"⏰ Job too old: {scraped_job.get('title', 'Unknown')} (posted: {posted_date_str})")
    
    return is_recent

def convert_to_db_job(scraped_job):
    """Convert scraped job data to database Job model"""
    
    # Create a proper external_id
    external_id = f"dw123_{scraped_job.get('id', 'unknown')}"
    
    # Parse the posted date
    posted_date = parse_posted_date(scraped_job.get('posted_date', ''))
    
    # Map job data to database fields
    db_job = Job(
        external_id=external_id,
        title=scraped_job.get('title', 'Unknown Title'),
        company=scraped_job.get('company', 'Unknown Company'),
        location=scraped_job.get('location', 'Unknown Location'),
        description=scraped_job.get('title', 'No description available'),
        
        # Set daywork123 as source
        source='daywork123',
        source_url=scraped_job.get('source_url', ''),
        
        # Default values for new fields
        vessel_type=None,
        vessel_size=None,
        vessel_name=None,
        employment_type='daywork',  # Default for Daywork123
        job_type='daywork',
        department=None,
        position_level=None,
        
        # Salary fields (daywork123 doesn't provide salary info)
        salary_range=None,
        salary_currency=None,
        salary_period=None,
        
        # Content fields
        requirements=[],  # Empty array for now
        benefits=[],      # Empty array for now
        
        # Location fields
        country=None,
        region=None,
        
        # Metadata
        quality_score=scraped_job.get('quality_score', 0.5),
        raw_data=scraped_job,  # Store original scraped data
        
        # Timestamps
        posted_date=posted_date or datetime.utcnow(),
        scraped_at=datetime.utcnow(),
        created_at=datetime.utcnow(),
        updated_at=datetime.utcnow()
    )
    
    return db_job

def save_jobs_to_database(scraped_jobs):
    """Save scraped jobs to database"""
    if not scraped_jobs:
        logger.warning("No jobs to save")
        return 0
    
    logger.info(f"Saving {len(scraped_jobs)} jobs to database...")
    saved_count = 0
    updated_count = 0
    
    with SessionLocal() as db:
        for scraped_job in scraped_jobs:
            try:
                external_id = f"dw123_{scraped_job.get('id', 'unknown')}"
                
                # Check if job already exists
                existing_job = db.query(Job).filter(
                    Job.external_id == external_id,
                    Job.source == 'daywork123'
                ).first()
                
                if existing_job:
                    # Update existing job
                    existing_job.title = scraped_job.get('title', existing_job.title)
                    existing_job.company = scraped_job.get('company', existing_job.company)
                    existing_job.location = scraped_job.get('location', existing_job.location)
                    existing_job.source_url = scraped_job.get('source_url', existing_job.source_url)
                    existing_job.quality_score = scraped_job.get('quality_score', existing_job.quality_score)
                    existing_job.raw_data = scraped_job
                    existing_job.updated_at = datetime.utcnow()
                    updated_count += 1
                    logger.debug(f"Updated job: {existing_job.title}")
                else:
                    # Create new job
                    new_job = convert_to_db_job(scraped_job)
                    db.add(new_job)
                    saved_count += 1
                    logger.debug(f"Added new job: {new_job.title}")
                    
            except Exception as e:
                logger.error(f"Error processing job {scraped_job.get('id', 'unknown')}: {e}")
                continue
        
        try:
            db.commit()
            logger.info(f"✅ Database operation completed: {saved_count} new, {updated_count} updated")
        except Exception as e:
            logger.error(f"Error committing to database: {e}")
            db.rollback()
            return 0
    
    return saved_count + updated_count

async def main():
    """Main function to scrape and save jobs"""
    print("=" * 60)
    print("🔄 Daywork123 Scraper → Database Integration")
    print("=" * 60)
    
    # Configure the scraper
    BASE_URL = "https://www.daywork123.com/jobannouncementlist.aspx"
    MAX_PAGES = 2  # Scrape 2 pages for good sample
    
    try:
        # Initialize and run the existing scraper
        logger.info("🚀 Starting Daywork123 scraper...")
        scraper = Daywork123Scraper(base_url=BASE_URL)
        
        # Scrape jobs using the existing scraper
        await scraper.scrape_jobs(max_pages=MAX_PAGES)
        
        # Get the scraped jobs
        all_scraped_jobs = scraper.jobs
        
        logger.info(f"📊 Initial scraping completed:")
        logger.info(f"   Total jobs found: {len(all_scraped_jobs)}")
        
        if not all_scraped_jobs:
            logger.warning("No jobs were scraped. This might be due to:")
            logger.warning("- Website blocking requests")
            logger.warning("- Changes in website structure")
            logger.warning("- Network connectivity issues")
            return
        
        # Filter jobs from last week only
        logger.info("🔍 Filtering jobs from last week...")
        recent_jobs = [job for job in all_scraped_jobs if is_job_from_last_week(job)]
        
        logger.info(f"📊 Date filtering completed:")
        logger.info(f"   Total jobs scraped: {len(all_scraped_jobs)}")
        logger.info(f"   Jobs from last week: {len(recent_jobs)}")
        logger.info(f"   Filtered out: {len(all_scraped_jobs) - len(recent_jobs)} older jobs")
        
        if not recent_jobs:
            logger.warning("⚠️  No jobs from the last week found!")
            logger.info("This might mean:")
            logger.info("- No new jobs posted recently")
            logger.info("- Date parsing issues")
            logger.info("- Website date format changes")
            
            # Show sample dates for debugging
            logger.info("Sample posted dates from scraped jobs:")
            for i, job in enumerate(all_scraped_jobs[:5]):
                posted_date = job.get('posted_date', 'No date')
                logger.info(f"   {i+1}. {job.get('title', 'No title')} - Posted: {posted_date}")
            return
        
        # Use only recent jobs
        scraped_jobs = recent_jobs
        
        # Show sample of scraped jobs
        logger.info("📋 Sample scraped jobs:")
        for i, job in enumerate(scraped_jobs[:3]):
            logger.info(f"   {i+1}. {job.get('title', 'No title')} @ {job.get('company', 'No company')}")
        
        # Save to database
        logger.info("💾 Saving jobs to database...")
        total_saved = save_jobs_to_database(scraped_jobs)
        
        if total_saved > 0:
            # Verify database content
            with SessionLocal() as db:
                total_jobs = db.query(Job).count()
                daywork123_jobs = db.query(Job).filter(Job.source == 'daywork123').count()
                
                logger.info(f"📋 Database verification:")
                logger.info(f"   Total jobs in database: {total_jobs}")
                logger.info(f"   Daywork123 jobs: {daywork123_jobs}")
            
            print("\n🎉 SUCCESS!")
            print(f"✅ Scraped {len(scraped_jobs)} jobs from Daywork123.com")
            print(f"✅ Saved/updated {total_saved} jobs to database")
            print("✅ Ready to view jobs on the frontend")
            
            print("\n📋 Next steps:")
            print("1. Start the application: python main.py")
            print("2. Open browser: http://localhost:8000")
            print("3. Filter by 'Daywork123' to see scraped jobs")
            print("4. View JSON: curl 'http://localhost:8000/api/jobs?source=daywork123'")
        else:
            print("\n❌ Failed to save jobs to database")
            
    except Exception as e:
        logger.error(f"Unexpected error: {e}")
        print(f"\n❌ Error: {e}")

if __name__ == "__main__":
    asyncio.run(main())

</code>

debug_scrapers.py:
<code>
#!/usr/bin/env python3
"""Debug script to inspect website structure and fix scraper selectors"""
import asyncio
import logging
import sys
from pathlib import Path
from datetime import datetime

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent))

from app.scrapers.registry import ScraperRegistry

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def debug_daywork123():
    """Debug Daywork123.com structure"""
    print("🔍 Debugging Daywork123.com structure...")
    
    try:
        from playwright.async_api import async_playwright
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=False)  # Show browser for debugging
            context = await browser.new_context()
            page = await context.new_page()
            
            # Navigate to the jobs page
            url = "https://www.daywork123.com/jobs"
            print(f"Navigating to: {url}")
            
            await page.goto(url, wait_until='networkidle', timeout=30000)
            
            # Wait a bit for content to load
            await asyncio.sleep(5)
            
            # Take a screenshot for debugging
            await page.screenshot(path="daywork123_debug.png")
            print("📸 Screenshot saved as: daywork123_debug.png")
            
            # Try to find job listings with different selectors
            selectors_to_try = [
                '.job-listing',
                '.job-card',
                '.job-item',
                'article',
                '[class*="job"]',
                '[class*="listing"]',
                '.card',
                '.item'
            ]
            
            for selector in selectors_to_try:
                try:
                    elements = await page.query_selector_all(selector)
                    if elements:
                        print(f"✅ Found {len(elements)} elements with selector: {selector}")
                        
                        # Get some sample content
                        if len(elements) > 0:
                            sample_text = await elements[0].text_content()
                            print(f"   Sample content: {sample_text[:200]}...")
                    else:
                        print(f"❌ No elements found with selector: {selector}")
                except Exception as e:
                    print(f"❌ Error with selector {selector}: {e}")
            
            # Get page title and URL
            title = await page.title()
            current_url = page.url
            print(f"\n📄 Page title: {title}")
            print(f"🌐 Current URL: {current_url}")
            
            # Get page HTML for manual inspection
            html = await page.content()
            with open("daywork123_debug.html", "w", encoding="utf-8") as f:
                f.write(html)
            print("📄 HTML saved as: daywork123_debug.html")
            
            await browser.close()
            
    except Exception as e:
        print(f"❌ Error debugging Daywork123: {e}")

async def debug_yotspot():
    """Debug Yotspot.com structure"""
    print("\n🔍 Debugging Yotspot.com structure...")
    
    try:
        import aiohttp
        
        async with aiohttp.ClientSession() as session:
            url = "https://www.yotspot.com/jobs"
            print(f"Fetching: {url}")
            
            async with session.get(url) as response:
                print(f"Status: {response.status}")
                print(f"Headers: {dict(response.headers)}")
                
                if response.status == 200:
                    html = await response.text()
                    
                    # Save HTML for inspection
                    with open("yotspot_debug.html", "w", encoding="utf-8") as f:
                        f.write(html)
                    print("📄 HTML saved as: yotspot_debug.html")
                    
                    # Try to parse with BeautifulSoup
                    from bs4 import BeautifulSoup
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # Look for job-related elements
                    job_selectors = [
                        '[class*="job"]',
                        '[class*="listing"]',
                        'article',
                        '.card',
                        '.item'
                    ]
                    
                    for selector in job_selectors:
                        elements = soup.select(selector)
                        if elements:
                            print(f"✅ Found {len(elements)} elements with selector: {selector}")
                            if len(elements) > 0:
                                sample_text = elements[0].get_text(strip=True)[:200]
                                print(f"   Sample content: {sample_text}...")
                        else:
                            print(f"❌ No elements found with selector: {selector}")
                    
                    # Get page title
                    title = soup.title.string if soup.title else "No title"
                    print(f"\n📄 Page title: {title}")
                    
                else:
                    print(f"❌ Failed to fetch page: {response.status}")
                    
    except Exception as e:
        print(f"❌ Error debugging Yotspot: {e}")

async def test_alternative_urls():
    """Test alternative URLs for the job sites"""
    print("\n🔍 Testing alternative URLs...")
    
    # Test different possible job URLs
    daywork123_urls = [
        "https://www.daywork123.com/jobs",
        "https://www.daywork123.com/job-listings",
        "https://www.daywork123.com/careers",
        "https://www.daywork123.com/positions",
        "https://www.daywork123.com"
    ]
    
    yotspot_urls = [
        "https://www.yotspot.com/jobs",
        "https://www.yotspot.com/job-listings",
        "https://www.yotspot.com/careers",
        "https://www.yotspot.com/positions",
        "https://www.yotspot.com"
    ]
    
    try:
        import aiohttp
        
        async with aiohttp.ClientSession() as session:
            print("Testing Daywork123 URLs:")
            for url in daywork123_urls:
                try:
                    async with session.get(url, timeout=10) as response:
                        print(f"  {url}: {response.status}")
                except Exception as e:
                    print(f"  {url}: Error - {e}")
            
            print("\nTesting Yotspot URLs:")
            for url in yotspot_urls:
                try:
                    async with session.get(url, timeout=10) as response:
                        print(f"  {url}: {response.status}")
                except Exception as e:
                    print(f"  {url}: Error - {e}")
                    
    except Exception as e:
        print(f"❌ Error testing URLs: {e}")

async def main():
    """Main debug function"""
    print("🐛 Yacht Jobs Scraper Debug Tool")
    print("=" * 50)
    
    try:
        # Test alternative URLs first
        await test_alternative_urls()
        
        # Debug individual sites
        await debug_daywork123()
        await debug_yotspot()
        
        print("\n✅ Debug complete! Check the generated files:")
        print("  - daywork123_debug.png (screenshot)")
        print("  - daywork123_debug.html (page source)")
        print("  - yotspot_debug.html (page source)")
        
    except KeyboardInterrupt:
        print("\nDebug interrupted by user")
    except Exception as e:
        print(f"Debug failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main()) 
</code>

SETUP_GUIDE.md:
<code>
# 🛥️ Daywork123.com Scraper Setup Guide

This guide provides step-by-step instructions for setting up the Daywork123.com scraper in your yacht jobs platform.

## 📋 Prerequisites

- Python 3.11+ (recommended)
- Conda environment (recommended) or virtualenv
- Git

## 🚀 Quick Setup

### 1. Environment Setup

```bash
# Activate your conda environment
conda activate yachtjobs

# Install new dependencies
pip install -r requirements.txt

# Install Playwright browsers (for Daywork123.com)
playwright install chromium
```

### 2. Database Migration

The new scrapers use the existing database schema. No migration needed.

### 3. Test the Installation

```bash
# Test the scraper installation
python test_daywork123_scraper.py

# Test individual scrapers
python -c "from app.services.scraping_service import scrape_daywork123; import asyncio; print(asyncio.run(scrape_daywork123(max_pages=1)))"
```

## 🔧 Configuration

### Environment Variables

Add these to your `.env` file:

```bash
# Scraping Configuration
SCRAPER_INTERVAL_MINUTES=45
MAX_SCRAPING_PAGES=5
MIN_REQUEST_DELAY=2.0
MAX_REQUEST_DELAY=5.0

# Anti-detection settings
PLAYWRIGHT_HEADLESS=true
PLAYWRIGHT_TIMEOUT=30000
```

### Custom Filters

You can configure custom filters for each scraper:

```python
# Example: Filter by location and job type
filters = {
    "location": "Caribbean",
    "job_type": "daywork",
    "vessel_size": "50-100m"
}
```

## 📊 Usage Examples

### Basic Usage

```python
import asyncio
from app.services.scraping_service import ScrapingService

async def main():
    service = ScrapingService()
    
    # Scrape Daywork123.com
    result = await service.scrape_source("daywork123", max_pages=3)
    print(f"Found {result['jobs_found']} jobs")
    
    # Scrape all sources
    results = await service.scrape_all_sources(max_pages=2)
    for result in results:
        print(f"{result['source']}: {result['jobs_found']} jobs")

if __name__ == "__main__":
    asyncio.run(main())
```

### Advanced Usage

```python
from app.scrapers.registry import ScraperRegistry

# Get specific scraper
scraper = ScraperRegistry.get_scraper("daywork123")

# Test connection
is_working = await scraper.test_connection()

# Get supported filters
filters = scraper.get_supported_filters()
```

## 🔍 Monitoring and Debugging

### Health Checks

```bash
# Check all scrapers
python -c "from app.services.scraping_service import ScrapingService; import asyncio; service = ScrapingService(); print(asyncio.run(service.health_check_all()))"
```

### Log Files

Logs are written to the console by default. Configure logging in your application:

```python
import logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('scraper.log'),
        logging.StreamHandler()
    ]
)
```

## 🧪 Testing

### Run All Tests

```bash
python test_daywork123_scraper.py
```

### Test Individual Components

```bash
# Test scraper registry
python -c "from app.scrapers.registry import ScraperRegistry; print(ScraperRegistry.list_scrapers())"

# Test database integration
python -c "from app.services.scraping_service import ScrapingService; import asyncio; service = ScrapingService(); print(service.get_scraper_stats())"
```

## 🚨 Troubleshooting

### Common Issues

#### Playwright Not Found
```bash
pip install playwright
playwright install chromium
```

#### Permission Errors
```bash
# On Linux/macOS
sudo playwright install-deps chromium

# On Windows (run as Administrator)
playwright install chromium
```

#### Connection Timeouts
- Check internet connection
- Verify target websites are accessible
- Increase timeout values in configuration

#### Database Issues
- Ensure database file exists: `yacht_jobs.db`
- Check database permissions
- Verify SQLAlchemy models are up to date

### Debug Mode

Enable debug logging:

```python
import logging
logging.getLogger('app.scrapers').setLevel(logging.DEBUG)
```

## 🔄 Integration with Existing System

### Update Scheduler

Modify your scheduler to use the new scraping service:

```python
from app.services.scraping_service import scrape_all_sources

# In your scheduler
async def scheduled_scraping():
    results = await scrape_all_sources(max_pages=3)
    # Process results...
```

### API Endpoints

Update your FastAPI endpoints:

```python
from fastapi import APIRouter
from app.services.scraping_service import ScrapingService

router = APIRouter()

@router.post("/api/scrape/daywork123")
async def scrape_daywork123_endpoint(max_pages: int = 3):
    service = ScrapingService()
    return await service.scrape_source("daywork123", max_pages)
```

## 📈 Performance Optimization

### Rate Limiting

The scrapers include built-in rate limiting:
- Daywork123: 2.5s delay between pages
- Yotspot: 2.0s delay between pages
- 30s delay between different sources

### Concurrent Scraping

For production use, consider:

```python
import asyncio
from app.services.scraping_service import ScrapingService

async def concurrent_scraping():
    service = ScrapingService()
    
    # Run scrapers concurrently
    tasks = [
        service.scrape_source("daywork123", max_pages=2),
        service.scrape_source("yotspot", max_pages=2)
    ]
    
    results = await asyncio.gather(*tasks)
    return results
```

## 🔒 Security Considerations

### Anti-Detection Measures

The Daywork123 scraper includes:
- Randomized user agents
- Realistic browser fingerprints
- Human-like delays
- Stealth mode scripts

### Rate Limiting

Respectful scraping with:
- Configurable delays
- Page limits
- Connection pooling
- Error handling

## 📞 Support

For issues or questions:
1. Check the troubleshooting section above
2. Review logs for specific error messages
3. Test with the provided test script
4. Check website accessibility manually

## 🎯 Next Steps

1. Run the test script to verify installation
2. Configure your environment variables
3. Test individual scrapers
4. Integrate with your scheduler
5. Monitor logs for any issues
6. Scale up gradually

## 📝 Changelog

- **v1.0.0**: Initial Daywork123.com scraper implementation
- **v1.1.0**: Added pluggable architecture with Yotspot support
- **v1.2.0**: Enhanced anti-detection measures
- **v1.3.0**: Added comprehensive testing suite
</code>

daywork123_scraper.py:
<code>
import asyncio
import time
from datetime import datetime
from playwright.async_api import async_playwright, TimeoutError
from bs4 import BeautifulSoup
import logging
import re
import pprint

# --- Configuration ---
# In a real application, this would come from a config file or environment variables
# as specified in your documents (e.g., daywork123_scraper_spec.md).
BASE_URL = "https://www.daywork123.com/jobannouncementlist.aspx"
MAX_PAGES = 1  # Limit for this demonstration, your spec allows for more.
REQUEST_DELAY = 2.5  # Respectful delay between requests as per your spec.
HEADLESS_BROWSER = True # Set to False for debugging to see the browser UI.
OUTPUT_FILE_TIMESTAMP = datetime.now().strftime("%Y%m%d_%H%M%S")
OUTPUT_FILE_NAME = f"daywork123.{OUTPUT_FILE_TIMESTAMP}.md"

# --- Logging Setup ---
# As per your spec for monitoring and observability.
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('scraper.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class Daywork123Scraper:
    """
    A production-grade scraper for Daywork123.com, built according to the
    provided architectural specifications. It uses Playwright for robust,
    anti-detection browser automation.
    """

    def __init__(self, base_url: str):
        """Initializes the scraper with the target URL."""
        self.base_url = base_url
        self.jobs = []
        # Corrected job table selector based on provided HTML
        self.job_table_selector = '#ContentPlaceHolder1_RepJobAnnouncement'
        logger.info("Daywork123Scraper initialized.")

    async def scrape_jobs(self, max_pages: int):
        """
        Main method to orchestrate the scraping process.
        It launches a browser, handles pagination, and extracts job data.
        """
        logger.info(f"Starting scrape for up to {max_pages} pages.")
        async with async_playwright() as p:
            # Launch browser with anti-detection measures as per spec
            browser = await p.chromium.launch(headless=HEADLESS_BROWSER)
            context = await browser.new_context(
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Safari/537.36',
                viewport={'width': 1920, 'height': 1080},
                locale='en-US',
                timezone_id='America/New_York',
            )
            # Add stealth script to avoid detection
            await context.add_init_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            page = await context.new_page()

            try:
                for page_num in range(1, max_pages + 1):
                    logger.info(f"Scraping page {page_num}...")
                    page_loaded_successfully = await self._navigate_to_page(page, page_num)

                    if not page_loaded_successfully:
                        logger.error(f"Failed to load page {page_num} or find job table. Stopping.")
                        break
                    
                    content = await page.content()
                    new_jobs = self._parse_jobs(content)

                    if not new_jobs:
                        logger.warning(f"No jobs found on page {page_num} while parsing. Stopping pagination.")
                        break
                    
                    self.jobs.extend(new_jobs)
                    logger.info(f"Found {len(new_jobs)} jobs on page {page_num}. Total jobs: {len(self.jobs)}.")

                    # Respectful delay
                    await asyncio.sleep(REQUEST_DELAY)

            except Exception as e:
                logger.error(f"An error occurred during scraping: {e}", exc_info=True)
            finally:
                await browser.close()
                logger.info("Browser closed. Scraping finished.")

    async def _navigate_to_page(self, page, page_num: int) -> bool:
        """
        Navigates to a specific page number on the website and waits for the
        job table to be visible. Returns True on success, False on failure.
        """
        try:
            if page_num == 1:
                # Changed wait_until to 'domcontentloaded' for initial page load
                await page.goto(self.base_url, wait_until='domcontentloaded')
            else:
                # Find the link for the next page and click it
                pagination_link_selector = f'a[href*="Page${page_num}"]'
                await page.click(pagination_link_selector)
            
            # Increased timeout for waiting for the job table selector
            await page.wait_for_selector(self.job_table_selector, timeout=30000) # 30 second timeout
            return True
        except TimeoutError:
            logger.error(f"Timeout occurred waiting for job table or pagination link on page {page_num}.")
            return False
        except Exception as e:
            logger.error(f"An unexpected error occurred during navigation to page {page_num}: {e}")
            return False


    def _parse_jobs(self, html_content: str) -> list:
        """
        Parses the HTML content of a job list page to extract job details.
        Uses BeautifulSoup for robust HTML parsing.
        """
        soup = BeautifulSoup(html_content, 'html.parser')
        job_listings = []
        
        # Find the main table containing the jobs
        job_table = soup.select_one(self.job_table_selector)
        if not job_table:
            logger.warning("Could not find job table in the parsed HTML.")
            return []

        # Find all job rows, skipping the header row
        job_rows = job_table.find_all('tr')[1:]

        for row in job_rows:
            cells = row.find_all('td')
            if len(cells) < 6:
                continue

            try:
                # Extract data based on the table structure
                job_id = cells[0].text.strip() # Extract Job ID
                job_title_link = cells[0].find('a') # The link is associated with the ID cell
                
                # The "Work Type" column contains the job description/title
                job_description_or_title = cells[3].text.strip() 
                
                # The source URL is constructed from the base URL and the link's href
                job_url = f"https://www.daywork123.com/{job_title_link['href']}" if job_title_link and job_title_link.get('href') else "N/A"
                
                # Extracting other fields, adjusted based on the new HTML structure
                company = cells[2].text.strip()
                location = cells[4].text.strip()
                job_type = "" # No explicit 'job_type' column, 'Work Type' is description
                posted_date_str = cells[1].text.strip()
                
                # A simple quality score as per the spec
                quality_score = self._calculate_quality_score(
                    title=job_description_or_title, company=company, location=location
                )

                job_listings.append({
                    'id': job_id, # Added job ID
                    'title': job_description_or_title, # Using description as title for output
                    'company': company,
                    'location': location,
                    'job_type': job_type,
                    'posted_date': posted_date_str,
                    'source_url': job_url,
                    'quality_score': quality_score
                })
            except (AttributeError, IndexError) as e:
                logger.error(f"Error parsing a job row: {e}. Row content: {row}")

        return job_listings
        
    def _calculate_quality_score(self, **kwargs) -> float:
        """
        Calculates a data quality score based on field completeness.
        This is a simplified version of the algorithm in your spec.
        """
        score = 0.0
        required_fields = ['title', 'company', 'location']
        total_weight = 1.0
        weight_per_field = total_weight / len(required_fields)
        
        for field in required_fields:
            if kwargs.get(field) and kwargs[field] != "N/A":
                score += weight_per_field
                
        return round(score, 2)

    def print_jobs(self):
        """Prints the scraped job data to the console."""
        logger.info(f"Printing {len(self.jobs)} jobs to the console.")
        print("\n" + "="*60)
        print(f"Found {len(self.jobs)} jobs on Daywork123.com")
        print("="*60 + "\n")

        if not self.jobs:
            print("No jobs found in this run.")
            return

        for i, job in enumerate(self.jobs, 1):
            pprint.pprint(job)
            print("-" * 20)

        logger.info("Finished printing jobs.")


    def save_to_markdown(self, filename: str):
        """Saves the scraped job data to a Markdown file."""
        logger.info(f"Saving {len(self.jobs)} jobs to {filename}.")
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(f"# Daywork123.com Job Listings\n\n")
            f.write(f"*Scraped on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n")
            f.write(f"*Total Jobs Found: {len(self.jobs)}*\n\n")
            f.write("---\n\n")

            if not self.jobs:
                f.write("No jobs found in this run.\n")
                return

            for job in self.jobs:
                f.write(f"## {job['title']}\n\n")
                f.write(f"- **ID:** {job['id']}\n") # Added Job ID to Markdown output
                f.write(f"- **Company:** {job['company']}\n")
                f.write(f"- **Location:** {job['location']}\n")
                f.write(f"- **Job Type:** {job['job_type']}\n")
                f.write(f"- **Posted Date:** {job['posted_date']}\n")
                f.write(f"- **Quality Score:** {job['quality_score']}\n")
                f.write(f"- **Source URL:** [{job['source_url']}]({job['source_url']})\n\n")
                f.write("---\n\n")
        logger.info("Successfully saved jobs to Markdown file.")


async def main():
    """Main function to run the scraper."""
    scraper = Daywork123Scraper(base_url=BASE_URL)
    await scraper.scrape_jobs(max_pages=MAX_PAGES)
    scraper.print_jobs()
    scraper.save_to_markdown(OUTPUT_FILE_NAME) # Uncommented this line

if __name__ == "__main__":
    # Ensure you have installed the necessary dependencies:
    # pip install playwright beautifulsoup4
    # And install the browser binaries:
    # playwright install chromium
    
    # To run the script:
    # python your_script_name.py
    
    asyncio.run(main())
</code>

requirements.txt:
<code>
# FastAPI and web framework
fastapi==0.104.1
uvicorn[standard]==0.24.0
jinja2==3.1.2
python-multipart==0.0.6

# Database
sqlalchemy==2.0.23
alembic==1.13.1

# Web scraping
requests==2.31.0
httpx==0.25.2
beautifulsoup4==4.12.2
playwright==1.40.0
lxml==4.9.3

# Async and scheduling
aiohttp==3.9.1
asyncio-throttle==1.0.2
apscheduler==3.11.0

# Date parsing
python-dateutil==2.8.2
dateparser==1.2.0

# Data processing
pydantic==2.5.0

# Environment and configuration
python-dotenv==1.0.0

# Caching and performance
redis==5.0.1

# Development
pytest==7.4.3
black==23.11.0 
</code>

run.py:
<code>
#!/usr/bin/env python3
"""
Simple run script for YotCrew.app
"""

import uvicorn
import os

if __name__ == "__main__":
    # Load environment variables
    from dotenv import load_dotenv
    load_dotenv()
    
    # Get configuration from environment
    host = os.getenv("HOST", "0.0.0.0")
    port = int(os.getenv("PORT", 8000))
    debug = os.getenv("DEBUG", "True").lower() == "true"
    
    print("🛥️  Starting YotCrew.app...")
    print(f"📍 Server: http://{host}:{port}")
    print(f"🔧 Debug mode: {debug}")
    print("=" * 50)
    
    uvicorn.run(
        "main:app",
        host=host,
        port=port,
        reload=debug,
        log_level="info"
    ) 
</code>

README.md:
<code>
# 🛥️ YotCrew.app

A modern, interactive yacht job platform built with **FastAPI**, **Alpine.js**, **HTMX**, **Tailwind CSS**, and **DaisyUI**. Automatically scrapes and displays yacht crew positions from top industry platforms with enhanced user interactivity.

![FastAPI](https://img.shields.io/badge/FastAPI-009688?style=for-the-badge&logo=FastAPI&logoColor=white)
![Alpine.js](https://img.shields.io/badge/Alpine.js-8BC34A?style=for-the-badge&logo=Alpine.js&logoColor=white)
![HTMX](https://img.shields.io/badge/HTMX-334155?style=for-the-badge&logo=html5&logoColor=white)
![Tailwind CSS](https://img.shields.io/badge/Tailwind_CSS-06B6D4?style=for-the-badge&logo=tailwind-css&logoColor=white)

## ✨ Features

### 🚀 Interactive Frontend
- **🎯 Alpine.js Integration**: Reactive components for job cards, filters, and search
- **📱 Expandable Job Cards**: Click to expand descriptions with smooth animations
- **⭐ Save/Bookmark System**: Mark favorite jobs for later review
- **☑️ Multi-select Comparison**: Select multiple jobs to compare side-by-side
- **🔍 Real-time Filtering**: Instant search and filter without page reloads
- **🏷️ Quick Filter Tags**: One-click filtering by department and vessel type
- **📊 Dynamic Sorting**: Sort by title, salary, or posting date

### 🔧 Backend Capabilities
- **🔄 Automated Scraping**: Scrapes yacht jobs from Yotspot every 45 minutes
- **⚡ HTMX-Powered**: Seamless partial page updates
- **🎨 Modern Themes**: Beautiful DaisyUI themes (currently: Cupcake)
- **🛥️ Yacht-Specific Categories**: Deck, Interior, Engineering, and Galley positions
- **📱 Mobile Responsive**: Perfect experience across all devices
- **🌈 Theme Switching**: Easy theme customization

## 🚀 Quick Start

### Prerequisites

- **Python 3.11+**
- **Conda** (recommended) or pip

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/evgenii-codesmith/yotcrew.app.git
   cd yotcrew.app
   ```

2. **Set up environment**
   ```bash
   # Using Conda (recommended)
   conda create -n yachtjobs python=3.11
   conda activate yachtjobs
   
   # Install dependencies
   pip install -r requirements.txt
   ```

3. **Create sample data** (optional but recommended)
   ```bash
   python create_sample_data.py
   ```

4. **Start the application**
   ```bash
   python run.py
   ```

5. **Open your browser**
   ```
   http://localhost:8000
   ```

## 📁 Project Structure

```
yotcrew.app/
├── app/
│   ├── __init__.py
│   ├── database.py          # Database configuration
│   ├── models.py            # SQLAlchemy models (Job, ScrapingJob)
│   ├── scraper.py           # Yotspot scraper
│   └── scheduler.py         # Background job scheduler
├── templates/
│   ├── base.html           # Base template with Alpine.js/HTMX/Tailwind
│   ├── dashboard.html      # Simple dashboard overview
│   ├── jobs.html           # Main interactive jobs page
│   └── partials/           # HTMX partial templates
│       ├── jobs_table.html # Interactive job cards with Alpine.js
│       ├── job_card.html   # Individual job card
│       └── dashboard_stats.html
├── static/                 # Static assets
│   ├── logo.png           # YotCrew.app logo
│   ├── favicon.svg        # Site favicon
│   └── favicon.ico        # Alternative favicon
├── DesignSpecs/           # Project documentation
├── main.py                # FastAPI application
├── requirements.txt       # Python dependencies
├── run.py                # Application runner
└── yacht_jobs.db         # SQLite database
```

## 🛠️ Technology Stack

### Backend
- **FastAPI**: Modern Python web framework with async support
- **SQLAlchemy**: Database ORM with relationship management
- **SQLite**: Lightweight database (production-ready for small to medium loads)
- **APScheduler**: Background job scheduling for automated scraping
- **Beautiful Soup**: Web scraping and HTML parsing
- **Requests**: HTTP client with session management

### Frontend
- **Alpine.js**: Lightweight reactive framework for interactivity
- **HTMX**: Dynamic HTML updates without full page reloads
- **Tailwind CSS**: Utility-first CSS framework
- **DaisyUI**: Beautiful component library with theme support
- **Jinja2**: Server-side template engine

### Interactive Features (Alpine.js)
- **Job Cards**: Expandable descriptions, save/bookmark functionality
- **Multi-select**: Compare multiple jobs with floating panel
- **Real-time Filters**: Search, department, vessel type, location
- **Dynamic Sorting**: Client-side sorting with server validation
- **Quick Actions**: Copy job links, share functionality
- **Filter Status**: Visual feedback on active filters with clear options

## 🎯 Pages & Routes

### Main Routes
- **`/`** - Interactive jobs page (Alpine.js powered)
- **`/dashboard`** - Simple dashboard overview
- **`/health`** - Health check endpoint

### API Endpoints
- **`GET /api/jobs`** - Get jobs with filtering and sorting
- **`POST /api/scrape`** - Trigger manual scraping
- **`GET /api/scrape/status`** - Get scraping status

### HTMX Endpoints
- **`GET /htmx/jobs-table`** - Jobs table with interactive cards
- **`GET /htmx/dashboard-stats`** - Dashboard statistics

## 🎨 Current Theme: Cupcake

YotCrew.app uses the **Cupcake** DaisyUI theme featuring:
- Soft pastel colors (pinks and creams)
- Light, friendly background
- Professional yet approachable design
- Excellent readability and contrast

### Theme Customization
Easily change themes by modifying `templates/base.html`:
```html
<html lang="en" data-theme="cupcake">
```

Available themes: `cupcake`, `nord`, `abyss`, `coffee`, `dark`, `light`, and more.

## 🔧 Configuration

### Environment Variables

Copy `env.example` to `.env` and configure:

```env
# Database
DATABASE_URL=sqlite:///./yacht_jobs.db

# Application
HOST=0.0.0.0
PORT=8000
DEBUG=True

# Scraping
SCRAPER_INTERVAL_MINUTES=45
MAX_SCRAPING_PAGES=5
MIN_REQUEST_DELAY=2.0
MAX_REQUEST_DELAY=5.0
```

## 🔍 Job Categories & Filtering

### Department Categories
- **⚓ Deck**: Captain, First Mate, Bosun, Deckhand, Navigation
- **🏠 Interior**: Chief Stewardess, Stewardess, Butler, Housekeeping
- **🔧 Engineering**: Chief Engineer, Engineer, ETO, Mechanical
- **👨‍🍳 Galley**: Head Chef, Sous Chef, Cook, Galley Assistant

### Vessel Types
- Motor Yacht
- Sailing Yacht
- Explorer Yacht
- Catamaran
- Superyacht
- Expedition Vessel

### Advanced Filtering
- **Real-time Search**: Instant results as you type
- **Location Filtering**: By region, country, or city
- **Salary Range**: Filter by compensation levels
- **Job Type**: Permanent, Temporary, Rotational
- **Experience Level**: Entry, Junior, Senior, Executive

## 🎮 Interactive Features

### Alpine.js Components

#### Job Cards (`jobFilters` component)
```javascript
// Real-time filtering and search
x-data="jobFilters()"
x-model="filters.search"
@change="applyFilters()"
```

#### Expandable Content
```javascript
// Expandable job descriptions
x-data="{ expanded: false }"
x-show="expanded"
x-transition
```

#### Save/Bookmark System
```javascript
// Job bookmarking
x-data="{ saved: false }"
@click="toggleSave(jobId)"
```

#### Multi-select Comparison
```javascript
// Compare multiple jobs
x-model="selectedJobs"
x-show="selectedJobs.length > 0"
```

## 🔄 Scraping Details

### Source
- **Primary**: [Yotspot.com](https://www.yotspot.com/job-search.html)
- **Categories**: All yacht crew positions
- **Update Frequency**: Every 45 minutes
- **Data Extracted**: Title, company, location, salary, description, vessel type

### Ethical Scraping
- ✅ **Rate Limited**: 2-5 second delays between requests
- ✅ **Respectful**: Maximum 5 pages per session
- ✅ **User-Agent**: Proper browser identification
- ✅ **Public Data**: Only publicly available job listings
- ✅ **Error Handling**: Graceful failure management

## 🐳 Docker Deployment

### Using Docker Compose

```yaml
version: '3.8'
services:
  yotcrew:
    build: .
    ports:
      - "8000:8000"
    environment:
      - DEBUG=False
      - HOST=0.0.0.0
    volumes:
      - ./yacht_jobs.db:/app/yacht_jobs.db
```

### Build and Run
```bash
docker build -t yotcrew-app .
docker run -p 8000:8000 yotcrew-app
```

## 🧪 Testing & Development

### Test Scraper
```bash
python test_scraper.py
```

### Create Sample Data
```bash
python create_sample_data.py
```

### Development Mode
```bash
# With auto-reload
python run.py
# OR
uvicorn main:app --reload
```

## 🚀 Production Deployment

### Render.com (Recommended)
The project includes `render.yaml` for easy deployment to Render:

1. Connect your GitHub repository
2. Render will automatically detect the configuration
3. Environment variables are pre-configured
4. Automatic deployments on git push

### Manual Production Setup
1. Set `DEBUG=False`
2. Configure production database (PostgreSQL recommended)
3. Set up proper secrets management
4. Configure reverse proxy (nginx)
5. Enable HTTPS

## 🤝 Contributing

1. **Fork** the repository
2. **Create** a feature branch (`git checkout -b feature/amazing-feature`)
3. **Commit** your changes (`git commit -m 'Add amazing feature'`)
4. **Push** to the branch (`git push origin feature/amazing-feature`)
5. **Open** a Pull Request

## 📄 License

This project is licensed under the MIT License - see the LICENSE file for details.

## 🙏 Acknowledgments

- **[Yotspot](https://www.yotspot.com)** - Primary source for yacht job listings
- **[FastAPI](https://fastapi.tiangolo.com/)** - Modern Python web framework
- **[Alpine.js](https://alpinejs.dev/)** - Lightweight reactive framework
- **[HTMX](https://htmx.org/)** - Modern web interactions
- **[Tailwind CSS](https://tailwindcss.com/)** - Utility-first styling
- **[DaisyUI](https://daisyui.com/)** - Beautiful component library

---

**Made with ❤️ for yacht crew professionals worldwide** 🛥️

**Repository**: [https://github.com/evgenii-codesmith/yotcrew.app](https://github.com/evgenii-codesmith/yotcrew.app) 
</code>

main.py:
<code>
from fastapi import FastAPI, Request, Depends, HTTPException, BackgroundTasks
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from sqlalchemy.orm import Session
from sqlalchemy import func, Integer
from datetime import datetime, timedelta
import uvicorn
import os
from contextlib import asynccontextmanager

from app.database import engine, get_db, Base
from app.models import Job, ScrapingJob
from app.scraper import YotspotScraper
from app.scheduler import start_scheduler, stop_scheduler

# Create tables
Base.metadata.create_all(bind=engine)

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    start_scheduler()
    yield
    # Shutdown
    stop_scheduler()

app = FastAPI(
    title="YotCrew.app",
    description="Real-time yacht job monitoring with HTMX and Tailwind CSS",
    version="1.0.0",
    lifespan=lifespan
)

# Mount static files
os.makedirs("static", exist_ok=True)
app.mount("/static", StaticFiles(directory="static"), name="static")

# Templates
templates = Jinja2Templates(directory="templates")

# Initialize scrapers
scraper = YotspotScraper()

@app.get("/", response_class=HTMLResponse)
async def main_page(request: Request):
    """Main interactive jobs page with Alpine.js features"""
    return templates.TemplateResponse("jobs.html", {"request": request})

@app.get("/dashboard", response_class=HTMLResponse)
async def simple_dashboard(request: Request, db: Session = Depends(get_db)):
    """Simple dashboard page (legacy)"""
    # Get recent jobs
    recent_jobs = db.query(Job).order_by(Job.posted_at.desc()).limit(10).all()
    
    # Get stats
    total_jobs = db.query(Job).count()
    today_jobs = db.query(Job).filter(
        Job.created_at >= datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
    ).count()
    
    return templates.TemplateResponse("dashboard.html", {
        "request": request,
        "recent_jobs": recent_jobs,
        "total_jobs": total_jobs,
        "today_jobs": today_jobs
    })

@app.get("/api/jobs")
async def get_jobs(
    page: int = 1,
    limit: int = 20,
    job_type: str = None,
    location: str = None,
    vessel_size: str = None,
    vessel_type: str = None,
    department: str = None,
    search: str = None,
    source: str = None,
    db: Session = Depends(get_db)
):
    """Get jobs with filtering and pagination - includes all scraped sources"""
    query = db.query(Job)
    
    # Apply source filter
    if source and source != "all":
        query = query.filter(Job.source == source)
    
    # Apply filters (check both job_type and employment_type for compatibility)
    if job_type:
        query = query.filter(
            (Job.job_type.ilike(f"%{job_type}%")) | 
            (Job.employment_type.ilike(f"%{job_type}%"))
        )
    if location:
        query = query.filter(
            (Job.location.ilike(f"%{location}%")) |
            (Job.country.ilike(f"%{location}%")) |
            (Job.region.ilike(f"%{location}%"))
        )
    if vessel_size:
        query = query.filter(Job.vessel_size.ilike(f"%{vessel_size}%"))
    if vessel_type:
        query = query.filter(Job.vessel_type.ilike(f"%{vessel_type}%"))
    if department:
        query = query.filter(Job.department.ilike(f"%{department}%"))
    if search:
        query = query.filter(
            (Job.title.ilike(f"%{search}%")) | 
            (Job.description.ilike(f"%{search}%")) |
            (Job.company.ilike(f"%{search}%"))
        )
    
    # Get total count before pagination
    total = query.count()
    
    # Apply pagination and ordering
    offset = (page - 1) * limit
    
    # Special sorting for Daywork123 by their original website ID
    if source == "daywork123":
        # Extract numeric ID from external_id (e.g., "dw123_172381" -> 172381) and sort descending
        jobs = query.order_by(
            func.cast(func.substr(Job.external_id, 7), Integer).desc()  # Skip "dw123_" prefix
        ).offset(offset).limit(limit).all()
    else:
        # Default sorting for other sources
        jobs = query.order_by(
            Job.posted_date.desc().nullslast(),
            Job.posted_at.desc().nullslast(),
            Job.created_at.desc()
        ).offset(offset).limit(limit).all()
    
    return {
        "jobs": [job.to_dict() for job in jobs],
        "total": total,
        "page": page,
        "pages": (total + limit - 1) // limit
    }

@app.get("/api/jobs/{job_id}")
async def get_job(job_id: str, db: Session = Depends(get_db)):
    """Get job details"""
    job = db.query(Job).filter(Job.id == job_id).first()
    if not job:
        raise HTTPException(status_code=404, detail="Job not found")
    return job.to_dict()

@app.get("/api/jobs/stats")
async def get_job_stats(request: Request, db: Session = Depends(get_db)):
    """Get job statistics by source (HTMX-compatible)"""
    total_jobs = db.query(Job).count()
    
    # Count by source
    source_stats = db.query(Job.source, func.count(Job.id)).group_by(Job.source).all()
    source_counts = {source: count for source, count in source_stats}
    
    # Recent activity (last 7 days)
    week_ago = datetime.now() - timedelta(days=7)
    recent_jobs = db.query(Job).filter(Job.created_at >= week_ago).count()
    
    # Return HTMX template if requested from frontend
    if "HX-Request" in request.headers:
        return templates.TemplateResponse("partials/job_stats.html", {
            "request": request,
            "total": total_jobs,
            "sources": source_counts,
            "recent_week": recent_jobs
        })
    
    # Return JSON for API calls
    return {
        "total": total_jobs,
        "sources": source_counts,
        "recent_week": recent_jobs,
        "available_sources": list(source_counts.keys())
    }

@app.get("/htmx/jobs-table")
async def htmx_jobs_table(
    request: Request,
    page: int = 1,
    limit: int = 20,
    job_type: str = None,
    location: str = None,
    vessel_size: str = None,
    vessel_type: str = None,  # Motor Yacht, Sailing Yacht
    department: str = None,   # Deck, Interior, Engineering, Galley
    search: str = None,
    sort: str = None,  # "posted_at", "title", "salary"
    source: str = "all",  # "all", "yotspot", "daywork123", "facebook"
    db: Session = Depends(get_db)
):
    """HTMX endpoint for jobs table - supports all scraped job sources"""
    
    # Get jobs from database with enhanced filtering
    jobs_data = await get_jobs(page, limit, job_type, location, vessel_size, vessel_type, department, search, source, db)
    all_jobs = list(jobs_data["jobs"])
    total = jobs_data["total"]
    pages = jobs_data["pages"]
    
    # Apply client-side sorting if specified (database already handles basic ordering)
    if sort and sort != "posted_at":  # posted_at is already handled by database
        def get_sort_key(job, sort_field):
            if sort_field == "title":
                return (job.get('title') or '').lower()
            elif sort_field == "salary":
                # Extract numeric value from salary string for sorting
                salary = job.get('salary_range') or job.get('salary') or ''
                import re
                numbers = re.findall(r'[\d,]+', str(salary))
                if numbers:
                    try:
                        return int(numbers[0].replace(',', ''))
                    except:
                        return 0
                return 0
            elif sort_field == "quality":
                return job.get('quality_score', 0)
            return (job.get('title') or '').lower()
        
        reverse_sort = sort in ["salary", "quality"]  # These sort descending
        all_jobs.sort(key=lambda job: get_sort_key(job, sort), reverse=reverse_sort)
    
    return templates.TemplateResponse("partials/jobs_table.html", {
        "request": request,
        "jobs": all_jobs,
        "total": total,
        "page": page,
        "pages": pages
    })

@app.get("/htmx/job-card/{job_id}")
async def htmx_job_card(request: Request, job_id: str, db: Session = Depends(get_db)):
    """HTMX endpoint for job card details"""
    job = db.query(Job).filter(Job.id == job_id).first()
    if not job:
        raise HTTPException(status_code=404, detail="Job not found")
    
    return templates.TemplateResponse("partials/job_card.html", {
        "request": request,
        "job": job
    })

@app.get("/htmx/dashboard-stats")
async def htmx_dashboard_stats(request: Request, db: Session = Depends(get_db)):
    """HTMX endpoint for dashboard statistics"""
    total_jobs = db.query(Job).count()
    today_jobs = db.query(Job).filter(
        Job.created_at >= datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
    ).count()
    week_jobs = db.query(Job).filter(
        Job.created_at >= datetime.now() - timedelta(days=7)
    ).count()
    
    # Get latest scraping status
    latest_scrape = db.query(ScrapingJob).order_by(ScrapingJob.started_at.desc()).first()
    
    return templates.TemplateResponse("partials/dashboard_stats.html", {
        "request": request,
        "total_jobs": total_jobs,
        "today_jobs": today_jobs,
        "week_jobs": week_jobs,
        "latest_scrape": latest_scrape
    })

@app.post("/api/scrape")
async def trigger_scrape(
    background_tasks: BackgroundTasks, 
    db: Session = Depends(get_db),
    source: str = "all",
    max_pages: int = 5
):
    """Manually trigger job scraping"""
    # Create scraping job record
    scraping_job = ScrapingJob(
        status="started",
        started_at=datetime.now(),
        scraper_type=source
    )
    db.add(scraping_job)
    db.commit()
    
    # Add background task
    background_tasks.add_task(run_scrape_task, scraping_job.id, source, max_pages)
    
    return {"message": f"Scraping started for {source}", "job_id": scraping_job.id}

async def run_scrape_task(scraping_job_id: int, source: str = "all", max_pages: int = 5):
    """Background task for scraping using new scraping service"""
    from app.services.scraping_service import ScrapingService
    
    db = next(get_db())
    try:
        scraping_job = db.query(ScrapingJob).filter(ScrapingJob.id == scraping_job_id).first()
        service = ScrapingService()
        
        if source == "all":
            # Scrape all sources
            results = await service.scrape_all_sources(max_pages=max_pages)
            total_found = sum(r.get("jobs_found", 0) for r in results)
            total_new = sum(r.get("new_jobs", 0) for r in results)
        elif source == "daywork123":
            # Scrape only Daywork123
            from app.scrapers.daywork123 import Daywork123Scraper
            daywork_scraper = Daywork123Scraper()
            result = await daywork_scraper.scrape_and_save_jobs(max_pages=max_pages)
            total_found = result.get("jobs_found", 0)
            total_new = result.get("jobs_saved", 0)
        else:
            # Scrape specific source
            result = await service.scrape_source(source, max_pages=max_pages)
            total_found = result.get("jobs_found", 0)
            total_new = result.get("new_jobs", 0)
        
        # Update scraping job
        scraping_job.status = "completed"
        scraping_job.completed_at = datetime.now()
        scraping_job.jobs_found = total_found
        scraping_job.new_jobs = total_new
        db.commit()
        
    except Exception as e:
        # Update scraping job with error
        scraping_job = db.query(ScrapingJob).filter(ScrapingJob.id == scraping_job_id).first()
        if scraping_job:
            scraping_job.status = "failed"
            scraping_job.completed_at = datetime.now()
            scraping_job.error_message = str(e)
            db.commit()
    finally:
        db.close()

@app.get("/api/scrape/status")
async def scrape_status(db: Session = Depends(get_db)):
    """Get latest scraping status"""
    latest_scrape = db.query(ScrapingJob).order_by(ScrapingJob.started_at.desc()).first()
    if not latest_scrape:
        return {"status": "no_jobs"}
    
    return {
        "status": latest_scrape.status,
        "started_at": latest_scrape.started_at.isoformat(),
        "completed_at": latest_scrape.completed_at.isoformat() if latest_scrape.completed_at else None,
        "jobs_found": latest_scrape.jobs_found,
        "new_jobs": latest_scrape.new_jobs,
        "error_message": latest_scrape.error_message
    }

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "version": "1.0.0"
    }

if __name__ == "__main__":
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=int(os.getenv("PORT", 8000)),
        reload=True
    ) 
</code>

SCHEDULER_README.md:
<code>
# Daywork123 Scheduler Documentation

## Overview

The Daywork123 scheduler is an advanced scheduling system that automates the scraping of job postings from Daywork123 using time-based intervals. It runs more frequently during morning and evening hours when job posting activity is typically higher.

## Features

- **Time-based scheduling**: Different frequencies for morning, day, and evening periods
- **Configurable intervals**: Easy customization through environment variables
- **Persistent job storage**: Uses SQLAlchemy for job persistence across restarts
- **Async execution**: Non-blocking scheduler operation
- **CLI management**: Command-line interface for manual control
- **Comprehensive logging**: Detailed logging for monitoring and debugging
- **Job status tracking**: Database records for all scraping activities

## Default Schedule

With default configuration, the scraper runs **18 times per day**:

### Morning (High Activity): 8 runs
- **Hours**: 6 AM, 7 AM, 8 AM, 9 AM
- **Minutes**: :00, :30
- **Frequency**: Every 30 minutes during morning hours

### Day (Lower Activity): 2 runs  
- **Hours**: 12 PM, 3 PM
- **Minutes**: :00
- **Frequency**: Less frequent during business hours

### Evening (High Activity): 8 runs
- **Hours**: 6 PM, 7 PM, 8 PM, 9 PM  
- **Minutes**: :00, :30
- **Frequency**: Every 30 minutes during evening hours

## Quick Start

### 1. Configuration

Copy the example environment file and customize:
```bash
cp scheduler.env.example .env
```

Key settings:
```env
# Database
SCHEDULER_DB_URL=sqlite:///./yachtjobs.db

# Scraping
DAYWORK123_MAX_PAGES=5

# Morning schedule (6-9 AM, every 30 min)
DAYWORK123_MORNING_HOURS=6,7,8,9
DAYWORK123_MORNING_MINUTES=0,30

# Day schedule (12 PM, 3 PM)
DAYWORK123_DAY_HOURS=12,15
DAYWORK123_DAY_MINUTES=0

# Evening schedule (6-9 PM, every 30 min)
DAYWORK123_EVENING_HOURS=18,19,20,21
DAYWORK123_EVENING_MINUTES=0,30
```

### 2. Test the Implementation

```bash
python test_scheduler.py
```

### 3. CLI Usage

Check scheduler status:
```bash
python -m app.cli status
```

Run scraper immediately:
```bash
python -m app.cli run-now
```

Update morning schedule:
```bash
python -m app.cli update-morning --hours "7,8,9,10" --minutes "0,15,30,45"
```

List scheduled jobs:
```bash
python -m app.cli list-jobs
```

Show next runs:
```bash
python -m app.cli next-runs --limit 10
```

## Architecture

### Core Components

1. **`app/config.py`**: Configuration management with environment variable support
2. **`app/daywork_scheduler.py`**: Core scheduler implementation with APScheduler
3. **`app/services/scheduler_service.py`**: High-level service interface
4. **`app/cli.py`**: Command-line interface for management

### Key Classes

- **`SchedulerConfig`**: Manages all configuration parameters
- **`ScrapingScheduler`**: Core scheduler with job management
- **`SchedulerService`**: Service layer for common operations

## Integration

### Adding to Your Application

To integrate the scheduler with your application lifecycle:

```python
import asyncio
from app.services.scheduler_service import SchedulerService

# Global scheduler instance
scheduler_service = None

async def startup():
    """Application startup"""
    global scheduler_service
    scheduler_service = SchedulerService()
    await scheduler_service.start()
    print("Scheduler started")

async def shutdown():
    """Application shutdown"""
    global scheduler_service
    if scheduler_service:
        await scheduler_service.stop()
    print("Scheduler stopped")

# For FastAPI
from contextlib import asynccontextmanager

@asynccontextmanager
async def lifespan(app):
    await startup()
    yield
    await shutdown()

app = FastAPI(lifespan=lifespan)
```

### Manual Operations

```python
from app.services.scheduler_service import SchedulerService

# Create service
service = SchedulerService()

# Get status
status = service.get_scheduler_status()

# Run immediately
result = await service.run_daywork123_now()

# Update schedule
result = await service.update_morning_schedule([7,8,9], [0,30])
```

## Configuration Options

### Schedule Customization

**High Frequency (24 runs/day)**:
```env
DAYWORK123_MORNING_HOURS=6,7,8,9
DAYWORK123_MORNING_MINUTES=0,30
DAYWORK123_DAY_HOURS=10,12,14,16
DAYWORK123_DAY_MINUTES=0,30
DAYWORK123_EVENING_HOURS=18,19,20,21
DAYWORK123_EVENING_MINUTES=0,30
```

**Medium Frequency (12 runs/day)**:
```env
DAYWORK123_MORNING_HOURS=7,8,9
DAYWORK123_MORNING_MINUTES=0
DAYWORK123_DAY_HOURS=12,15
DAYWORK123_DAY_MINUTES=0
DAYWORK123_EVENING_HOURS=18,19,20,21
DAYWORK123_EVENING_MINUTES=0
```

**Low Frequency (6 runs/day)**:
```env
DAYWORK123_MORNING_HOURS=8
DAYWORK123_MORNING_MINUTES=0
DAYWORK123_DAY_HOURS=12,15
DAYWORK123_DAY_MINUTES=0
DAYWORK123_EVENING_HOURS=19,21
DAYWORK123_EVENING_MINUTES=0
```

### Advanced Options

```env
# Job behavior
SCHEDULER_COALESCE=true              # Skip overlapping jobs
SCHEDULER_MAX_INSTANCES=1            # Max concurrent instances
SCHEDULER_MISFIRE_GRACE_TIME=300     # 5 minute grace period

# Performance
DAYWORK123_MAX_PAGES=5               # Pages per scraping run

# Logging
LOG_LEVEL=INFO                       # DEBUG, INFO, WARNING, ERROR
```

## Monitoring

### Database Tables

The scheduler creates these database tables:
- `apscheduler_jobs`: Scheduled job definitions
- `scraping_jobs`: Execution history and results

### Log Messages

Key log patterns to monitor:
```
INFO - Scheduler started successfully
INFO - Scheduled 18 Daywork123 scraping jobs per day  
INFO - Starting Daywork123 scraping - morning (07:00)
INFO - Daywork123 scraping completed - morning (07:00) - Found 15 jobs in 12.34s
ERROR - Error in Daywork123 scraping - morning (07:00): Connection timeout
```

### Status Monitoring

```python
service = SchedulerService()
status = service.get_scheduler_status()

print(f"Running: {status['running']}")
print(f"Total jobs: {status['total_jobs']}")
print(f"Daily runs: {status['config']['total_daily_runs']}")
```

## Troubleshooting

### Common Issues

**Scheduler not starting**:
- Check database connectivity
- Verify configuration validity
- Check file permissions

**Jobs not running**:
- Verify scheduler is running: `python -m app.cli status`
- Check next run times: `python -m app.cli next-runs`
- Review logs for errors

**High memory usage**:
- Reduce `DAYWORK123_MAX_PAGES`
- Check for database growth
- Monitor log file sizes

**Performance issues**:
- Reduce scraping frequency
- Optimize database queries
- Check network connectivity

### Debug Mode

Enable debug logging:
```env
LOG_LEVEL=DEBUG
```

This provides detailed information about:
- Job scheduling and execution
- Database operations
- Scraper activity
- Configuration loading

## Best Practices

### Production Deployment

1. **Use external database**: PostgreSQL or MySQL instead of SQLite
2. **Monitor resources**: Set up alerts for CPU, memory, disk usage
3. **Log rotation**: Configure log rotation to prevent disk issues
4. **Backup strategy**: Regular backups of job data and configuration
5. **Health checks**: Implement health check endpoints

### Security

1. **Environment variables**: Never commit sensitive data to git
2. **Database access**: Use restricted database users
3. **Network security**: Limit outbound connections if possible
4. **Log sanitization**: Avoid logging sensitive information

### Performance

1. **Reasonable frequency**: Don't over-scrape target sites
2. **Error handling**: Graceful degradation on failures
3. **Resource limits**: Set appropriate memory and CPU limits
4. **Database maintenance**: Regular cleanup of old job records

## Support

For issues or questions:
1. Check logs for error messages
2. Run `python test_scheduler.py` to verify setup
3. Use `python -m app.cli status` to check current state
4. Review configuration against examples in `scheduler.env.example`

## Future Enhancements

Potential improvements:
- Web dashboard for schedule management
- Email/webhook notifications for failures
- Metrics collection and monitoring
- Multi-scraper support
- Dynamic schedule adjustment based on results
- Rate limiting and respect for robots.txt

</code>

test_scheduler.py:
<code>
"""
Test script for the Daywork123 scheduler functionality.

This script tests the main scheduler components to ensure they work correctly
and integrate properly with the existing codebase.
"""
import asyncio
import logging
import sys
import os
from datetime import datetime

# Add the project root to Python path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from app.config import SchedulerConfig
from app.daywork_scheduler import ScrapingScheduler
from app.services.scheduler_service import SchedulerService


# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


async def test_config():
    """Test the scheduler configuration."""
    print("=== Testing Scheduler Configuration ===")
    
    try:
        # Test default configuration
        config = SchedulerConfig()
        
        # Validate configuration
        is_valid = config.validate_config()
        print(f"Configuration valid: {is_valid}")
        
        # Print configuration summary
        config.print_schedule_summary()
        
        # Test schedule string generation
        schedules = config.get_all_schedules()
        print("\nCron Schedules:")
        for period, schedule in schedules.items():
            print(f"  {period}: {schedule}")
        
        print(f"✅ Configuration test passed")
        return True
        
    except Exception as e:
        print(f"❌ Configuration test failed: {e}")
        return False


async def test_scheduler_creation():
    """Test creating and configuring the scheduler."""
    print("\n=== Testing Scheduler Creation ===")
    
    try:
        config = SchedulerConfig()
        scheduler = ScrapingScheduler(config)
        
        print(f"Scheduler created: {scheduler is not None}")
        print(f"Initial running state: {scheduler.is_running()}")
        
        print(f"✅ Scheduler creation test passed")
        return True
        
    except Exception as e:
        print(f"❌ Scheduler creation test failed: {e}")
        return False


async def test_scheduler_start_stop():
    """Test starting and stopping the scheduler."""
    print("\n=== Testing Scheduler Start/Stop ===")
    
    try:
        config = SchedulerConfig()
        scheduler = ScrapingScheduler(config)
        
        # Test start
        await scheduler.start()
        print(f"Scheduler running after start: {scheduler.is_running()}")
        
        # Test job scheduling
        jobs_status = scheduler.get_all_jobs_status()
        print(f"Number of jobs scheduled: {len(jobs_status)}")
        
        # Test scheduler status
        status = scheduler.get_scheduler_status()
        print(f"Scheduler status: {status.get('running', False)}")
        print(f"Total daily runs configured: {status.get('config', {}).get('total_daily_runs', 0)}")
        
        # Test stop
        await scheduler.stop()
        print(f"Scheduler running after stop: {scheduler.is_running()}")
        
        print(f"✅ Scheduler start/stop test passed")
        return True
        
    except Exception as e:
        print(f"❌ Scheduler start/stop test failed: {e}")
        return False


async def test_scheduler_service():
    """Test the scheduler service layer."""
    print("\n=== Testing Scheduler Service ===")
    
    try:
        service = SchedulerService()
        
        # Test service creation
        print(f"Service created: {service is not None}")
        print(f"Initial service state: {service.is_running()}")
        
        # Test getting status without starting
        status = service.get_scheduler_status()
        print(f"Status before start: {status.get('service_running', False)}")
        
        # Note: We won't actually start the service in tests to avoid conflicts
        # In a real test environment, you would start/stop the service here
        
        print(f"✅ Scheduler service test passed")
        return True
        
    except Exception as e:
        print(f"❌ Scheduler service test failed: {e}")
        return False


async def test_job_management():
    """Test job management functionality."""
    print("\n=== Testing Job Management ===")
    
    try:
        config = SchedulerConfig()
        scheduler = ScrapingScheduler(config)
        
        # Start scheduler to create jobs
        await scheduler.start()
        
        # Get all jobs
        jobs = scheduler.get_all_jobs_status()
        print(f"Total jobs created: {len(jobs)}")
        
        if jobs:
            first_job = jobs[0]
            job_id = first_job['id']
            print(f"First job ID: {job_id}")
            
            # Test getting specific job status
            job_status = scheduler.get_job_status(job_id)
            print(f"Job status retrieved: {job_status is not None}")
            
            # Test pause/resume (be careful in real environment)
            # scheduler.pause_job(job_id)
            # scheduler.resume_job(job_id)
        
        # Test removing all Daywork123 jobs
        await scheduler.remove_daywork123_jobs()
        
        jobs_after_removal = scheduler.get_all_jobs_status()
        print(f"Jobs after removal: {len(jobs_after_removal)}")
        
        await scheduler.stop()
        
        print(f"✅ Job management test passed")
        return True
        
    except Exception as e:
        print(f"❌ Job management test failed: {e}")
        return False


async def test_configuration_updates():
    """Test configuration updates."""
    print("\n=== Testing Configuration Updates ===")
    
    try:
        service = SchedulerService()
        
        # Test updating schedules (without starting the service)
        original_morning_hours = service.config.MORNING_HOURS.copy()
        original_morning_minutes = service.config.MORNING_MINUTES.copy()
        
        # Update morning schedule
        new_hours = [7, 8, 9]
        new_minutes = [0, 15, 30, 45]
        
        # Note: In a full test, you would start the service first
        # result = await service.update_morning_schedule(new_hours, new_minutes)
        # print(f"Schedule update result: {result.get('success', False)}")
        
        print(f"Original morning hours: {original_morning_hours}")
        print(f"New morning hours would be: {new_hours}")
        
        print(f"✅ Configuration updates test passed")
        return True
        
    except Exception as e:
        print(f"❌ Configuration updates test failed: {e}")
        return False


async def test_time_calculations():
    """Test time-based calculations."""
    print("\n=== Testing Time Calculations ===")
    
    try:
        config = SchedulerConfig()
        
        # Test total daily runs calculation
        total_runs = config.get_total_daily_runs()
        print(f"Total daily runs: {total_runs}")
        
        # Test individual period calculations
        morning_runs = len(config.MORNING_HOURS) * len(config.MORNING_MINUTES)
        day_runs = len(config.DAY_HOURS) * len(config.DAY_MINUTES)
        evening_runs = len(config.EVENING_HOURS) * len(config.EVENING_MINUTES)
        
        print(f"Morning runs: {morning_runs}")
        print(f"Day runs: {day_runs}")
        print(f"Evening runs: {evening_runs}")
        print(f"Total: {morning_runs + day_runs + evening_runs}")
        
        # Verify calculation
        assert total_runs == morning_runs + day_runs + evening_runs
        
        print(f"✅ Time calculations test passed")
        return True
        
    except Exception as e:
        print(f"❌ Time calculations test failed: {e}")
        return False


async def run_all_tests():
    """Run all scheduler tests."""
    print("🚀 Starting Scheduler Tests")
    print("=" * 50)
    
    test_results = []
    
    # Run individual tests
    test_results.append(await test_config())
    test_results.append(await test_scheduler_creation())
    test_results.append(await test_scheduler_start_stop())
    test_results.append(await test_scheduler_service())
    test_results.append(await test_job_management())
    test_results.append(await test_configuration_updates())
    test_results.append(await test_time_calculations())
    
    # Summary
    print("\n" + "=" * 50)
    print("🏁 Test Results Summary")
    print("=" * 50)
    
    passed = sum(test_results)
    total = len(test_results)
    
    print(f"Tests passed: {passed}/{total}")
    
    if passed == total:
        print("🎉 All tests passed!")
        return True
    else:
        print("❌ Some tests failed!")
        return False


async def main():
    """Main test entry point."""
    try:
        success = await run_all_tests()
        
        if success:
            print("\n✅ Scheduler implementation ready for use!")
            print("\nNext steps:")
            print("1. Update your main.py to integrate the scheduler")
            print("2. Set up environment variables from scheduler.env.example")
            print("3. Test the CLI commands")
            print("4. Monitor the logs during operation")
        else:
            print("\n❌ Please fix the failing tests before proceeding.")
            
    except KeyboardInterrupt:
        print("\n⏹️  Tests interrupted by user")
    except Exception as e:
        print(f"\n💥 Unexpected error during testing: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    asyncio.run(main())

</code>

scrape_and_save.py:
<code>
#!/usr/bin/env python3
"""
Scrape jobs from Daywork123 and save to database
"""
import asyncio
import logging
import sys
import os

# Add the app directory to Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'app'))

from app.scrapers.daywork123 import Daywork123Scraper
from app.database import SessionLocal
from app.models import Job

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

async def scrape_and_save_jobs():
    """Scrape Daywork123 and save jobs to database"""
    logger.info("🚀 Starting Daywork123 scraping...")
    
    scraper = Daywork123Scraper()
    
    # Test connection first
    logger.info("Testing connection to Daywork123.com...")
    if not await scraper.test_connection():
        logger.warning("⚠️  Cannot connect to Daywork123.com")
        logger.info("This might be due to:")
        logger.info("- Network connectivity issues")
        logger.info("- Website blocking requests")
        logger.info("- Site temporarily unavailable")
        return False
    
    logger.info("✅ Connection successful!")
    
    # Scrape and save jobs
    logger.info("Scraping jobs from Daywork123.com (2 pages)...")
    result = await scraper.scrape_and_save_jobs(max_pages=2)
    
    if result['success']:
        logger.info(f"✅ Scraping completed successfully!")
        logger.info(f"   📊 Jobs found: {result['jobs_found']}")
        logger.info(f"   💾 Jobs saved: {result['jobs_saved']}")
        logger.info(f"   ⏱️  Duration: {result['duration']:.2f} seconds")
        
        # Verify database content
        with SessionLocal() as db:
            total_jobs = db.query(Job).count()
            daywork123_jobs = db.query(Job).filter(Job.source == 'daywork123').count()
            
            logger.info(f"📋 Database verification:")
            logger.info(f"   Total jobs: {total_jobs}")
            logger.info(f"   Daywork123 jobs: {daywork123_jobs}")
            
            # Show sample jobs
            sample_jobs = db.query(Job).limit(3).all()
            if sample_jobs:
                logger.info("📝 Sample scraped jobs:")
                for job in sample_jobs:
                    quality_icon = "⭐" if job.quality_score >= 0.8 else "✨" if job.quality_score >= 0.6 else "💫"
                    logger.info(f"   {quality_icon} {job.title} @ {job.company} ({job.location})")
        
        return True
    else:
        logger.error(f"❌ Scraping failed!")
        for error in result.get('errors', []):
            logger.error(f"   Error: {error}")
        return False

async def main():
    """Main function"""
    print("=" * 60)
    print("🔄 YotCrew.app - Daywork123 Scraper")
    print("=" * 60)
    
    try:
        success = await scrape_and_save_jobs()
        
        if success:
            print("\n🎉 Scraping completed successfully!")
            print("✅ Real jobs from Daywork123.com are now in the database")
            print("✅ Ready to start the application and view jobs")
            print("\nNext steps:")
            print("1. Start the application: python main.py")
            print("2. Open browser: http://localhost:8000")
            print("3. See the scraped jobs rendered on the frontend!")
        else:
            print("\n❌ Scraping failed - check the logs above")
            
    except Exception as e:
        logger.error(f"Unexpected error: {e}")
        print(f"\n❌ Error: {e}")

if __name__ == "__main__":
    asyncio.run(main())


</code>

test_scraper.py:
<code>
#!/usr/bin/env python3
"""
Test script for the Yotspot scraper
"""

import asyncio
import sys
import os

# Add the current directory to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from app.scraper import YotspotScraper

async def test_scraper():
    """Test the Yotspot scraper"""
    scraper = YotspotScraper()
    
    print("🧪 Testing Yotspot scraper...")
    print("=" * 50)
    
    # Test basic connection
    print("1. Testing website connection...")
    test_result = scraper.test_scraping()
    print(f"   Status: {test_result.get('status', 'unknown')}")
    if test_result.get('title'):
        print(f"   Page title: {test_result['title']}")
    if test_result.get('view_job_links'):
        print(f"   Found {test_result['view_job_links']} 'View Job' links")
    
    if test_result.get('status') == 'error':
        print(f"   Error: {test_result.get('error')}")
        print("\n❌ Connection test failed. Check your internet connection.")
        return
    
    print("   ✅ Connection successful!")
    
    # Test scraping a small number of jobs
    print("\n2. Testing job scraping (1 page only)...")
    try:
        jobs = await scraper.scrape_jobs(max_pages=1)
        
        if jobs:
            print(f"   ✅ Successfully scraped {len(jobs)} jobs!")
            print("\n📋 Sample jobs found:")
            
            for i, job in enumerate(jobs[:3], 1):  # Show first 3 jobs
                print(f"\n   Job {i}:")
                print(f"   Title: {job.get('title', 'N/A')}")
                print(f"   Company: {job.get('company', 'N/A')}")
                print(f"   Location: {job.get('location', 'N/A')}")
                print(f"   Vessel: {job.get('vessel_type', 'N/A')} - {job.get('vessel_size', 'N/A')}")
                print(f"   Salary: {job.get('salary_range', 'N/A')}")
                print(f"   Type: {job.get('job_type', 'N/A')}")
                
                if i == 3 and len(jobs) > 3:
                    print(f"\n   ... and {len(jobs) - 3} more jobs")
        else:
            print("   ⚠️  No jobs found. This might be normal if:")
            print("      - The website structure has changed")
            print("      - There are no current job postings")
            print("      - The selectors need updating")
            
    except Exception as e:
        print(f"   ❌ Scraping failed: {e}")
        return
    
    print("\n" + "=" * 50)
    print("🎉 Scraper test completed!")
    
    if jobs:
        print(f"✅ Found {len(jobs)} jobs successfully")
        print("\n💡 Tips:")
        print("   - Run 'python create_sample_data.py' to add sample data")
        print("   - Start the app with 'uvicorn main:app --reload'")
        print("   - Visit http://localhost:8000 to see the dashboard")
    else:
        print("⚠️  No jobs found, but you can still test with sample data")
        print("   - Run 'python create_sample_data.py' to add sample data")

if __name__ == "__main__":
    asyncio.run(test_scraper()) 
</code>

search_bot_api.py:
<code>
from fastapi import FastAPI, Request, HTTPException
from pydantic import BaseModel
from typing import Optional, List, Dict, Any
import requests
import json
import logging
from datetime import datetime
import re
import asyncio
import aiohttp
import facebook
from googlesearch import search
import spacy
from spacy.matcher import Matcher
import uvicorn
from contextlib import asynccontextmanager

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Global variables for models
nlp = None
job_matcher = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Lifespan events for loading models on startup"""
    global nlp, job_matcher
    
    # Load spaCy model on startup
    try:
        nlp = spacy.load("en_core_web_sm")
        job_matcher = Matcher(nlp.vocab)
        logger.info("spaCy model loaded successfully")
    except OSError:
        logger.warning("spaCy model not found. Job detection will be limited.")
        nlp = None
    
    yield
    
    # Cleanup (if needed)
    logger.info("Shutting down...")

app = FastAPI(
    title="Search Bot API",
    description="A search bot that provides web search, job search, and Facebook search functionality",
    version="1.0.0",
    lifespan=lifespan
)

# Configuration
# Facebook Graph API for group posts
FB_ACCESS_TOKEN = "YOUR_FB_ACCESS_TOKEN"
FB_GROUP_ID = "YOUR_FB_GROUP_ID"

# Pydantic models
class SearchRequest(BaseModel):
    query: str
    max_results: int = 5

class JobSearchRequest(BaseModel):
    location: str = "remote"
    max_results: int = 5

class FacebookSearchRequest(BaseModel):
    query: str
    max_results: int = 10

class SearchBot:
    def __init__(self):
        self.setup_job_patterns()
    
    def setup_job_patterns(self):
        """Setup spaCy patterns for job detection"""
        if not nlp or not job_matcher:
            return
        
        hiring_patterns = [
            [{"LOWER": {"IN": ["hiring", "recruiting", "seeking"]}}],
            [{"LOWER": {"IN": ["job", "position", "role"]}}, 
             {"LOWER": {"IN": ["opening", "available", "vacancy"]}}],
            [{"LOWER": {"IN": ["apply", "send"]}}, 
             {"LOWER": {"IN": ["resume", "cv"]}}],
            [{"LOWER": {"IN": ["full", "part"]}}, {"LOWER": "time"}],
            [{"LOWER": "remote"}, {"LOWER": {"IN": ["work", "job"]}}],
            [{"LOWER": {"IN": ["salary", "wage"]}}, {"IS_DIGIT": True}],
            [{"LOWER": {"IN": ["we", "company"]}}, {"LOWER": {"IN": ["are", "is"]}}, {"LOWER": "hiring"}]
        ]
        
        try:
            for i, pattern in enumerate(hiring_patterns):
                job_matcher.add(f"JOB_PATTERN_{i}", [pattern])
            logger.info("Job patterns loaded successfully")
        except Exception as e:
            logger.error(f"Error setting up job patterns: {str(e)}")
    
    def is_job_post(self, text: str) -> bool:
        """Check if text is a job post using spaCy"""
        if not nlp or not job_matcher or not text:
            return False
        
        try:
            doc = nlp(text)
            matches = job_matcher(doc)
            
            job_keywords = {
                'hire', 'hiring', 'job', 'position', 'role', 'career', 'opportunity',
                'apply', 'resume', 'cv', 'salary', 'wage', 'remote', 'fulltime', 'parttime',
                'interview', 'candidate', 'employee', 'team', 'join', 'work', 'experience'
            }
            
            keyword_count = sum(1 for token in doc if token.lemma_.lower() in job_keywords)
            
            # Additional pattern checks
            salary_pattern = r'\$\d+(?:,\d+)*(?:\.\d+)?'
            has_salary = bool(re.search(salary_pattern, text))
            
            return len(matches) > 0 or keyword_count >= 2 or has_salary
            
        except Exception as e:
            logger.error(f"Error in job detection: {str(e)}")
            return False
    
    async def search_web(self, query: str, max_results: int = 5) -> List[str]:
        """Perform web search and return results"""
        try:
            # Using Google Search in a separate thread to avoid blocking
            loop = asyncio.get_event_loop()
            search_results = await loop.run_in_executor(
                None, 
                lambda: list(search(query, num_results=max_results))
            )
            return search_results
        except Exception as e:
            logger.error(f"Search error: {str(e)}")
            return []
    
    async def search_jobs(self, location: str = "remote", max_results: int = 5) -> List[str]:
        """Search for job postings"""
        try:
            # Search for jobs using web search
            job_query = f"jobs hiring {location} site:indeed.com OR site:linkedin.com OR site:glassdoor.com"
            
            loop = asyncio.get_event_loop()
            job_results = await loop.run_in_executor(
                None, 
                lambda: list(search(job_query, num_results=max_results))
            )
            return job_results
        except Exception as e:
            logger.error(f"Jobs search error: {str(e)}")
            return []
    
    async def search_facebook(self, search_term: str, max_results: int = 10) -> Dict[str, Any]:
        """Search Facebook group posts"""
        try:
            # Run Facebook API call in executor to avoid blocking
            loop = asyncio.get_event_loop()
            posts_data = await loop.run_in_executor(
                None, 
                self._search_facebook_posts, 
                search_term
            )
            return posts_data
        except Exception as e:
            logger.error(f"Facebook search error: {str(e)}")
            return {"job_posts": [], "regular_posts": []}
    
    def _search_facebook_posts(self, search_term: str):
        """Search Facebook posts synchronously"""
        try:
            graph = facebook.GraphAPI(access_token=FB_ACCESS_TOKEN)
            feed = graph.get_connections(FB_GROUP_ID, "feed", limit=50)
            
            job_posts = []
            regular_posts = []
            
            for post in feed['data']:
                if 'message' in post and search_term.lower() in post['message'].lower():
                    post_data = {
                        'id': post['id'],
                        'message': post['message'][:150] + "..." if len(post['message']) > 150 else post['message'],
                        'created_time': post.get('created_time', 'Unknown')[:10],  # Just date
                        'author': post.get('from', {}).get('name', 'Unknown')
                    }
                    
                    if self.is_job_post(post['message']):
                        job_posts.append(post_data)
                    else:
                        regular_posts.append(post_data)
            
            return {"job_posts": job_posts, "regular_posts": regular_posts}
            
        except Exception as e:
            logger.error(f"Facebook API error: {str(e)}")
            return {"job_posts": [], "regular_posts": []}
    
    async def search_news(self, topic: str = "technology", max_results: int = 5) -> List[str]:
        """Get latest news"""
        try:
            # Search for news
            news_query = f"{topic} news today site:reuters.com OR site:bbc.com OR site:cnn.com OR site:techcrunch.com"
            
            loop = asyncio.get_event_loop()
            news_results = await loop.run_in_executor(
                None, 
                lambda: list(search(news_query, num_results=max_results))
            )
            return news_results
        except Exception as e:
            logger.error(f"News search error: {str(e)}")
            return []

# Initialize bot
bot = SearchBot()

# Routes
@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "message": "Search Bot API",
        "version": "1.0.0",
        "status": "online",
        "timestamp": datetime.now().isoformat()
    }

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "spacy_loaded": nlp is not None,
        "timestamp": datetime.now().isoformat()
    }

# API endpoints
@app.post("/api/search")
async def api_search(request: SearchRequest):
    """Direct search API endpoint"""
    try:
        results = await bot.search_web(request.query, request.max_results)
        return {
            "query": request.query,
            "results": results,
            "count": len(results)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/jobs")
async def api_jobs(request: JobSearchRequest):
    """Direct job search API endpoint"""
    try:
        results = await bot.search_jobs(request.location, request.max_results)
        return {
            "location": request.location,
            "results": results,
            "count": len(results)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/facebook")
async def api_facebook(request: FacebookSearchRequest):
    """Direct Facebook search API endpoint"""
    try:
        posts_data = await bot.search_facebook(request.query, request.max_results)
        return {
            "query": request.query,
            "job_posts": posts_data["job_posts"],
            "regular_posts": posts_data["regular_posts"],
            "total_found": len(posts_data["job_posts"]) + len(posts_data["regular_posts"])
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/news")
async def api_news(query: str = "technology", max_results: int = 5):
    """Direct news search API endpoint"""
    try:
        results = await bot.search_news(query, max_results)
        return {
            "topic": query,
            "results": results,
            "count": len(results)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/status")
async def api_status():
    """Check bot status"""
    return {
        "system": "online",
        "ai_model": "spaCy Loaded" if nlp else "Not Available",
        "facebook_api": "Connected" if FB_ACCESS_TOKEN else "Not Configured",
        "search": "available",
        "uptime": datetime.now().isoformat()
    }

if __name__ == "__main__":
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )
</code>

=2.8.0:
<code>
Collecting pydantic
  Using cached pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)
Collecting annotated-types>=0.6.0 (from pydantic)
  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)
Collecting pydantic-core==2.33.2 (from pydantic)
  Using cached pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)
Collecting typing-extensions>=4.12.2 (from pydantic)
  Using cached typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)
Collecting typing-inspection>=0.4.0 (from pydantic)
  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)
Using cached pydantic-2.11.7-py3-none-any.whl (444 kB)
Using cached pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)
Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)
Using cached typing_extensions-4.14.1-py3-none-any.whl (43 kB)
Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)
Installing collected packages: typing-extensions, annotated-types, typing-inspection, pydantic-core, pydantic

Successfully installed annotated-types-0.7.0 pydantic-2.11.7 pydantic-core-2.33.2 typing-extensions-4.14.1 typing-inspection-0.4.1

</code>

test_daywork123_db.py:
<code>
#!/usr/bin/env python3
"""
Test script for Daywork123Scraper database saving functionality
"""
import asyncio
import logging
import sys
import os
from datetime import datetime

# Add the app directory to Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'app'))

from app.database import engine, Base, SessionLocal
from app.models import Job
from app.scrapers.daywork123 import Daywork123Scraper
from app.scrapers.base import UniversalJob, JobSource, EmploymentType, Department, VesselType

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def setup_database():
    """Create database tables if they don't exist"""
    try:
        Base.metadata.create_all(bind=engine)
        logger.info("Database tables created/verified")
    except Exception as e:
        logger.error(f"Error setting up database: {e}")
        raise

def test_job_saving():
    """Test saving UniversalJob objects to database"""
    logger.info("Testing job saving functionality...")
    
    # Create test jobs
    test_jobs = [
        UniversalJob(
            external_id="test_dw123_001",
            title="Test Captain Position",
            company="Test Yacht Company",
            source=JobSource.DAYWORK123,
            source_url="https://www.daywork123.com/test",
            location="Monaco",
            description="Test captain position for testing database saving",
            employment_type=EmploymentType.PERMANENT,
            department=Department.DECK,
            vessel_type=VesselType.MOTOR_YACHT,
            vessel_size="50-74m",
            salary_range="€8,000 - €12,000/month",
            salary_currency="EUR",
            posted_date=datetime.utcnow(),
            requirements=["Valid certificates", "5+ years experience"],
            benefits=["Competitive salary", "Travel opportunities"],
            quality_score=0.85,
            raw_data={"test": True, "source": "unit_test"}
        ),
        UniversalJob(
            external_id="test_dw123_002", 
            title="Test Chief Engineer",
            company="Test Marine Services",
            source=JobSource.DAYWORK123,
            source_url="https://www.daywork123.com/test2",
            location="Fort Lauderdale",
            description="Test chief engineer position for testing",
            employment_type=EmploymentType.ROTATIONAL,
            department=Department.ENGINEERING,
            vessel_type=VesselType.SUPER_YACHT,
            vessel_size="75m+",
            salary_range="$7,000 - $10,000/month",
            salary_currency="USD",
            posted_date=datetime.utcnow(),
            requirements=["Engineering degree", "Marine experience"],
            benefits=["Health insurance", "Rotation schedule"],
            quality_score=0.90,
            raw_data={"test": True, "source": "unit_test"}
        )
    ]
    
    # Test saving jobs
    scraper = Daywork123Scraper()
    
    async def run_test():
        saved_count = await scraper.save_jobs_to_db(test_jobs)
        logger.info(f"Saved {saved_count} test jobs to database")
        return saved_count
    
    return asyncio.run(run_test())

def verify_database_content():
    """Verify that jobs were saved correctly"""
    logger.info("Verifying database content...")
    
    with SessionLocal() as db:
        # Count total jobs
        total_jobs = db.query(Job).count()
        logger.info(f"Total jobs in database: {total_jobs}")
        
        # Count Daywork123 jobs
        dw123_jobs = db.query(Job).filter(Job.source == JobSource.DAYWORK123).count()
        logger.info(f"Daywork123 jobs in database: {dw123_jobs}")
        
        # Get test jobs
        test_jobs = db.query(Job).filter(Job.external_id.like("test_dw123_%")).all()
        logger.info(f"Test jobs found: {len(test_jobs)}")
        
        for job in test_jobs:
            logger.info(f"  - {job.title} ({job.external_id}) - Quality: {job.quality_score}")
        
        return len(test_jobs)

def test_duplicate_handling():
    """Test that duplicate jobs are handled correctly"""
    logger.info("Testing duplicate job handling...")
    
    # Create a duplicate job
    duplicate_job = UniversalJob(
        external_id="test_dw123_001",  # Same as first test job
        title="Updated Test Captain Position",  # Updated title
        company="Updated Test Yacht Company",  # Updated company
        source=JobSource.DAYWORK123,
        source_url="https://www.daywork123.com/test_updated",
        location="Monte Carlo",  # Updated location
        description="Updated test captain position description",
        employment_type=EmploymentType.PERMANENT,
        department=Department.DECK,
        vessel_type=VesselType.MOTOR_YACHT,
        vessel_size="50-74m",
        salary_range="€9,000 - €13,000/month",  # Updated salary
        salary_currency="EUR",
        posted_date=datetime.utcnow(),
        requirements=["Valid certificates", "7+ years experience"],  # Updated requirements
        benefits=["Competitive salary", "Travel opportunities", "Health insurance"],
        quality_score=0.90,  # Updated score
        raw_data={"test": True, "source": "duplicate_test", "updated": True}
    )
    
    scraper = Daywork123Scraper()
    
    async def run_duplicate_test():
        saved_count = await scraper.save_jobs_to_db([duplicate_job])
        logger.info(f"Processed {saved_count} duplicate job")
        return saved_count
    
    return asyncio.run(run_duplicate_test())

def test_real_scraping(max_pages=1):
    """Test real scraping and database saving (optional)"""
    logger.info(f"Testing real scraping with {max_pages} page(s)...")
    
    scraper = Daywork123Scraper()
    
    async def run_real_test():
        try:
            # Test connection first
            if not await scraper.test_connection():
                logger.warning("Cannot connect to Daywork123.com - skipping real scraping test")
                return {"success": False, "reason": "connection_failed"}
            
            # Run scraping with database saving
            result = await scraper.scrape_and_save_jobs(max_pages=max_pages)
            logger.info(f"Real scraping result: {result}")
            return result
            
        except Exception as e:
            logger.error(f"Error in real scraping test: {e}")
            return {"success": False, "error": str(e)}
    
    return asyncio.run(run_real_test())

def cleanup_test_data():
    """Clean up test data from database"""
    logger.info("Cleaning up test data...")
    
    with SessionLocal() as db:
        # Delete test jobs
        deleted_count = db.query(Job).filter(Job.external_id.like("test_dw123_%")).delete()
        db.commit()
        logger.info(f"Deleted {deleted_count} test jobs")
        return deleted_count

def main():
    """Main test function"""
    logger.info("=== Daywork123Scraper Database Saving Tests ===")
    
    try:
        # Setup
        logger.info("1. Setting up database...")
        setup_database()
        
        # Test 1: Basic job saving
        logger.info("\n2. Testing job saving...")
        saved_count = test_job_saving()
        assert saved_count == 2, f"Expected 2 jobs saved, got {saved_count}"
        
        # Test 2: Verify database content
        logger.info("\n3. Verifying database content...")
        test_jobs_count = verify_database_content()
        assert test_jobs_count >= 2, f"Expected at least 2 test jobs, found {test_jobs_count}"
        
        # Test 3: Duplicate handling
        logger.info("\n4. Testing duplicate job handling...")
        duplicate_result = test_duplicate_handling()
        assert duplicate_result == 1, f"Expected 1 duplicate processed, got {duplicate_result}"
        
        # Verify duplicate was updated, not added
        with SessionLocal() as db:
            updated_job = db.query(Job).filter(Job.external_id == "test_dw123_001").first()
            assert updated_job is not None, "Updated job not found"
            assert "Updated" in updated_job.title, "Job was not updated properly"
            assert updated_job.salary_range == "€9,000 - €13,000/month", "Salary was not updated"
            logger.info("✓ Duplicate handling works correctly")
        
        # Test 4: Real scraping (optional)
        user_input = input("\nDo you want to test real scraping from Daywork123.com? (y/N): ").strip().lower()
        if user_input in ['y', 'yes']:
            logger.info("\n5. Testing real scraping...")
            real_result = test_real_scraping(max_pages=1)
            if real_result.get("success"):
                logger.info(f"✓ Real scraping successful: {real_result['jobs_found']} jobs found, {real_result['jobs_saved']} saved")
            else:
                logger.warning(f"Real scraping failed or skipped: {real_result}")
        
        # Final verification
        logger.info("\n6. Final database verification...")
        final_count = verify_database_content()
        
        logger.info(f"\n=== All Tests Completed Successfully ===")
        logger.info(f"Database contains {final_count} test jobs")
        
        # Cleanup
        cleanup_choice = input("\nDo you want to clean up test data? (Y/n): ").strip().lower()
        if cleanup_choice not in ['n', 'no']:
            cleanup_test_data()
            logger.info("Test data cleaned up")
        
    except Exception as e:
        logger.error(f"Test failed: {e}")
        raise
    except AssertionError as e:
        logger.error(f"Assertion failed: {e}")
        raise

if __name__ == "__main__":
    main()


</code>

scheduler.env.example:
<code>
# Environment Configuration for Yacht Jobs Scheduler
# Copy this file to .env and customize the values for your environment

# =============================================================================
# Database Configuration
# =============================================================================

# Database URL for the scheduler job store and application data
# Examples:
#   SQLite: sqlite:///./yachtjobs.db
#   PostgreSQL: postgresql://user:password@localhost:5432/yachtjobs
#   MySQL: mysql://user:password@localhost:3306/yachtjobs
SCHEDULER_DB_URL=sqlite:///./yachtjobs.db

# =============================================================================
# Daywork123 Scraper Configuration
# =============================================================================

# Maximum number of pages to scrape per run
# Higher values = more jobs discovered but longer execution time
# Recommended: 3-10 pages depending on your needs
DAYWORK123_MAX_PAGES=5

# =============================================================================
# Time-Based Scheduling Configuration
# =============================================================================

# Morning scraping period - high activity hours
# Format: comma-separated hours (0-23) and minutes (0-59)
# Default: 6 AM - 9:30 AM with runs every 30 minutes
DAYWORK123_MORNING_HOURS=6,7,8,9
DAYWORK123_MORNING_MINUTES=0,30

# Daytime scraping period - lower activity hours  
# Format: comma-separated hours (0-23) and minutes (0-59)
# Default: 12 PM and 3 PM (less frequent during business hours)
DAYWORK123_DAY_HOURS=12,15
DAYWORK123_DAY_MINUTES=0

# Evening scraping period - high activity hours
# Format: comma-separated hours (0-23) and minutes (0-59)
# Default: 6 PM - 9:30 PM with runs every 30 minutes
DAYWORK123_EVENING_HOURS=18,19,20,21
DAYWORK123_EVENING_MINUTES=0,30

# =============================================================================
# Scheduler Job Configuration
# =============================================================================

# Job coalescing - run only the latest instance if jobs overlap
# Recommended: true (prevents job buildup)
SCHEDULER_COALESCE=true

# Maximum instances of the same job running simultaneously
# Recommended: 1 (prevents conflicts and resource issues)
SCHEDULER_MAX_INSTANCES=1

# Misfire grace time in seconds
# Jobs that miss their scheduled time will run if within this window
# Recommended: 300 (5 minutes)
SCHEDULER_MISFIRE_GRACE_TIME=300

# Database table name for storing job information
# Leave as default unless you have conflicts
SCHEDULER_JOBSTORE_TABLE=apscheduler_jobs

# =============================================================================
# Logging Configuration
# =============================================================================

# Logging level for the application
# Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
# Recommended: INFO for production, DEBUG for development
LOG_LEVEL=INFO

# =============================================================================
# Example Schedule Configurations
# =============================================================================

# High Frequency Configuration (24 runs per day)
# Scrapes every 30 minutes during active hours
# DAYWORK123_MORNING_HOURS=6,7,8,9
# DAYWORK123_MORNING_MINUTES=0,30
# DAYWORK123_DAY_HOURS=10,12,14,16
# DAYWORK123_DAY_MINUTES=0,30
# DAYWORK123_EVENING_HOURS=18,19,20,21
# DAYWORK123_EVENING_MINUTES=0,30

# Medium Frequency Configuration (12 runs per day)
# Scrapes hourly during active hours
# DAYWORK123_MORNING_HOURS=7,8,9
# DAYWORK123_MORNING_MINUTES=0
# DAYWORK123_DAY_HOURS=12,15
# DAYWORK123_DAY_MINUTES=0
# DAYWORK123_EVENING_HOURS=18,19,20,21
# DAYWORK123_EVENING_MINUTES=0

# Low Frequency Configuration (6 runs per day)
# Scrapes only at key times
# DAYWORK123_MORNING_HOURS=8
# DAYWORK123_MORNING_MINUTES=0
# DAYWORK123_DAY_HOURS=12,15
# DAYWORK123_DAY_MINUTES=0
# DAYWORK123_EVENING_HOURS=19,21
# DAYWORK123_EVENING_MINUTES=0

</code>

example_usage.py:
<code>
#!/usr/bin/env python3
"""
Example usage of Daywork123Scraper with database saving functionality

This script demonstrates how to use the enhanced Daywork123Scraper
to scrape jobs and save them to the database.
"""
import asyncio
import logging
import sys
import os
from datetime import datetime

# Add the app directory to Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'app'))

from app.database import engine, Base, SessionLocal
from app.models import Job
from app.scrapers.daywork123 import Daywork123Scraper
from app.services.scraping_service import ScrapingService

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

async def example_basic_scraping():
    """Example 1: Basic scraping with database saving"""
    logger.info("=== Example 1: Basic Scraping ===")
    
    # Create scraper instance
    scraper = Daywork123Scraper()
    
    # Test connection first
    if not await scraper.test_connection():
        logger.error("Cannot connect to Daywork123.com")
        return
    
    # Scrape and save jobs (1 page for demo)
    result = await scraper.scrape_and_save_jobs(max_pages=1)
    
    logger.info(f"Scraping completed:")
    logger.info(f"  - Jobs found: {result['jobs_found']}")
    logger.info(f"  - Jobs saved: {result['jobs_saved']}")
    logger.info(f"  - Duration: {result['duration']:.2f} seconds")
    logger.info(f"  - Success: {result['success']}")
    
    return result

async def example_manual_save():
    """Example 2: Manual scraping and saving"""
    logger.info("\n=== Example 2: Manual Scraping and Saving ===")
    
    scraper = Daywork123Scraper()
    
    # Collect jobs manually
    jobs = []
    async for job in scraper.scrape_jobs(max_pages=1):
        jobs.append(job)
        logger.info(f"Found job: {job.title} at {job.company}")
    
    logger.info(f"Collected {len(jobs)} jobs")
    
    # Save jobs to database
    if jobs:
        saved_count = await scraper.save_jobs_to_db(jobs)
        logger.info(f"Saved {saved_count} jobs to database")
        return saved_count
    
    return 0

async def example_using_service():
    """Example 3: Using the ScrapingService"""
    logger.info("\n=== Example 3: Using ScrapingService ===")
    
    service = ScrapingService()
    
    # Scrape specific source
    result = await service.scrape_source("daywork123", max_pages=1)
    
    logger.info(f"Service scraping result:")
    logger.info(f"  - Source: {result['source']}")
    logger.info(f"  - Jobs found: {result['jobs_found']}")
    logger.info(f"  - New jobs: {result['new_jobs']}")
    logger.info(f"  - Updated jobs: {result['updated_jobs']}")
    logger.info(f"  - Duration: {result['duration']:.2f} seconds")
    
    return result

def check_database_status():
    """Check current database status"""
    logger.info("\n=== Database Status ===")
    
    # Ensure tables exist
    Base.metadata.create_all(bind=engine)
    
    with SessionLocal() as db:
        # Count total jobs
        total_jobs = db.query(Job).count()
        logger.info(f"Total jobs in database: {total_jobs}")
        
        # Count by source
        dw123_jobs = db.query(Job).filter(Job.source == "daywork123").count()
        yotspot_jobs = db.query(Job).filter(Job.source == "yotspot").count()
        
        logger.info(f"  - Daywork123 jobs: {dw123_jobs}")
        logger.info(f"  - Yotspot jobs: {yotspot_jobs}")
        
        # Recent jobs
        recent_jobs = db.query(Job).order_by(Job.created_at.desc()).limit(5).all()
        
        if recent_jobs:
            logger.info("Recent jobs:")
            for job in recent_jobs:
                logger.info(f"  - {job.title} ({job.source}) - {job.created_at.strftime('%Y-%m-%d %H:%M')}")
        
        return {
            "total": total_jobs,
            "daywork123": dw123_jobs,
            "yotspot": yotspot_jobs,
            "recent": len(recent_jobs)
        }

async def example_health_check():
    """Example 4: Health check for scrapers"""
    logger.info("\n=== Example 4: Health Check ===")
    
    service = ScrapingService()
    
    # Health check all scrapers
    health_status = await service.health_check_all()
    
    for scraper_name, status in health_status.items():
        logger.info(f"{scraper_name}:")
        logger.info(f"  - Accessible: {status.get('accessible', False)}")
        logger.info(f"  - Status: {status.get('status', 'unknown')}")
        if status.get('error'):
            logger.info(f"  - Error: {status['error']}")

async def main():
    """Main example function"""
    logger.info("YotCrew.app Daywork123 Scraper Examples")
    logger.info("=" * 50)
    
    try:
        # Check initial database status
        db_status = check_database_status()
        
        # Example 1: Basic scraping
        await example_basic_scraping()
        
        # Example 2: Manual scraping
        await example_manual_save()
        
        # Example 3: Using service
        await example_using_service()
        
        # Example 4: Health check
        await example_health_check()
        
        # Final database status
        logger.info("\n=== Final Status ===")
        final_status = check_database_status()
        
        # Show changes
        new_jobs = final_status["total"] - db_status["total"]
        if new_jobs > 0:
            logger.info(f"Added {new_jobs} new jobs during examples")
        
    except Exception as e:
        logger.error(f"Error in examples: {e}")
        raise

if __name__ == "__main__":
    # Run examples
    asyncio.run(main())


</code>

meridian_scraper.py:
<code>
#!/usr/bin/env python3
"""
Meridian Go Job Board Scraper
Scrapes job listings from https://www.meridiango.com/jobs with pagination support
"""

import requests
from bs4 import BeautifulSoup
import json
import csv
import time
from typing import List, Dict, Optional
import logging
from urllib.parse import urljoin, urlparse
import re

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class MeridianScraper:
    def __init__(self, base_url: str = "https://www.meridiango.com"):
        self.base_url = base_url
        self.jobs_url = f"{base_url}/jobs"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
    def get_total_pages(self) -> int:
        """Get the total number of pages available"""
        try:
            response = self.session.get(self.jobs_url)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Look for pagination elements
            pagination = soup.find('nav', {'aria-label': 'pagination'}) or soup.find('ul', class_=re.compile('pagination'))
            if pagination:
                page_links = pagination.find_all('a', href=re.compile(r'page=\d+'))
                if page_links:
                    # Extract page numbers from href attributes
                    page_numbers = []
                    for link in page_links:
                        href = link.get('href', '')
                        match = re.search(r'page=(\d+)', href)
                        if match:
                            page_numbers.append(int(match.group(1)))
                    return max(page_numbers) if page_numbers else 1
            
            # Alternative: look for "Last" button or total pages indicator
            last_link = soup.find('a', text=re.compile(r'Last|»|>>'))
            if last_link and last_link.get('href'):
                href = last_link['href']
                match = re.search(r'page=(\d+)', href)
                if match:
                    return int(match.group(1))
                    
            return 1  # Default to 1 page if no pagination found
            
        except Exception as e:
            logger.error(f"Error getting total pages: {e}")
            return 1
    
    def scrape_job_listings(self, page: int = 1) -> List[Dict[str, str]]:
        """Scrape job listings from a specific page"""
        jobs = []
        
        try:
            url = f"{self.jobs_url}?page={page}" if page > 1 else self.jobs_url
            logger.info(f"Scraping page {page}: {url}")
            
            response = self.session.get(url)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Find job listings - adjust selectors based on actual HTML structure
            job_cards = soup.find_all('div', class_=re.compile('job|listing|position|card'))
            
            if not job_cards:
                # Try alternative selectors
                job_cards = soup.find_all('article', class_=re.compile('job|listing'))
                
            if not job_cards:
                # Try finding by data attributes
                job_cards = soup.find_all('div', {'data-job-id': True})
                
            logger.info(f"Found {len(job_cards)} job cards on page {page}")
            
            for card in job_cards:
                job_data = self.extract_job_data(card)
                if job_data:
                    jobs.append(job_data)
                    
            return jobs
            
        except Exception as e:
            logger.error(f"Error scraping page {page}: {e}")
            return []
    
    def extract_job_data(self, card) -> Optional[Dict[str, str]]:
        """Extract job data from a single job card element"""
        try:
            job_data = {}
            
            # Job title
            title_elem = card.find('h2') or card.find('h3') or card.find('a', class_=re.compile('title|job-title'))
            if not title_elem:
                title_elem = card.find('a', href=re.compile(r'/jobs/\d+'))
            job_data['title'] = title_elem.get_text(strip=True) if title_elem else 'N/A'
            
            # Job URL
            link_elem = title_elem.find('a') if title_elem else card.find('a', href=re.compile(r'/jobs/\d+'))
            if link_elem and link_elem.get('href'):
                job_data['url'] = urljoin(self.base_url, link_elem['href'])
            else:
                job_data['url'] = 'N/A'
            
            # Company name
            company_elem = card.find(class_=re.compile('company|employer')) or card.find('span', class_=re.compile('company'))
            job_data['company'] = company_elem.get_text(strip=True) if company_elem else 'Meridian Go'
            
            # Location
            location_elem = card.find(class_=re.compile('location|place')) or card.find('span', class_=re.compile('location'))
            job_data['location'] = location_elem.get_text(strip=True) if location_elem else 'N/A'
            
            # Job type (Full-time, Part-time, etc.)
            type_elem = card.find(class_=re.compile('type|employment')) or card.find('span', class_=re.compile('type'))
            job_data['job_type'] = type_elem.get_text(strip=True) if type_elem else 'N/A'
            
            # Posted date
            date_elem = card.find(class_=re.compile('date|posted')) or card.find('time')
            job_data['posted_date'] = date_elem.get_text(strip=True) if date_elem else 'N/A'
            
            # Salary (if available)
            salary_elem = card.find(class_=re.compile('salary|pay|compensation'))
            job_data['salary'] = salary_elem.get_text(strip=True) if salary_elem else 'N/A'
            
            # Description snippet
            desc_elem = card.find(class_=re.compile('description|summary')) or card.find('p')
            job_data['description'] = desc_elem.get_text(strip=True)[:200] + '...' if desc_elem else 'N/A'
            
            return job_data
            
        except Exception as e:
            logger.error(f"Error extracting job data: {e}")
            return None
    
    def scrape_all_jobs(self, max_pages: Optional[int] = None) -> List[Dict[str, str]]:
        """Scrape all job listings across all pages"""
        all_jobs = []
        
        total_pages = self.get_total_pages()
        if max_pages:
            total_pages = min(total_pages, max_pages)
            
        logger.info(f"Total pages to scrape: {total_pages}")
        
        for page in range(1, total_pages + 1):
            page_jobs = self.scrape_job_listings(page)
            all_jobs.extend(page_jobs)
            
            # Add delay to be respectful
            if page < total_pages:
                time.sleep(1)
        
        logger.info(f"Total jobs scraped: {len(all_jobs)}")
        return all_jobs
    
    def save_to_json(self, jobs: List[Dict[str, str]], filename: str = "meridian_jobs.json"):
        """Save jobs to JSON file"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(jobs, f, indent=2, ensure_ascii=False)
        logger.info(f"Saved {len(jobs)} jobs to {filename}")
    
    def save_to_csv(self, jobs: List[Dict[str, str]], filename: str = "meridian_jobs.csv"):
        """Save jobs to CSV file"""
        if not jobs:
            logger.warning("No jobs to save")
            return
            
        with open(filename, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=jobs[0].keys())
            writer.writeheader()
            writer.writerows(jobs)
        logger.info(f"Saved {len(jobs)} jobs to {filename}")

def main():
    """Main execution function"""
    scraper = MeridianScraper()
    
    # Scrape all jobs
    jobs = scraper.scrape_all_jobs()
    
    # Save results
    if jobs:
        scraper.save_to_json(jobs)
        scraper.save_to_csv(jobs)
        
        # Print summary
        print(f"\nScraping completed successfully!")
        print(f"Total jobs found: {len(jobs)}")
        print(f"First 5 jobs:")
        for i, job in enumerate(jobs[:5], 1):
            print(f"{i}. {job['title']} at {job['company']} - {job['location']}")
    else:
        print("No jobs found. The website structure might have changed.")

if __name__ == "__main__":
    main()
</code>


yacht_jobs.db binary file skipped..
yachtjobs.db binary file skipped..
